{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Problem 3: NumPy CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from scipy import signal\n",
    "from imageio import imread\n",
    "from random import shuffle\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_images\n",
    "    # Read in images and makes a list for each set in the form: [images, labels]\n",
    "    # images: np array with dims [N x img_height x img width x num_channels]\n",
    "    # labels: np array with dims [N x 1]. elephant = 0, lionfish = 1\n",
    "    #\n",
    "    # Returns:  train_set: The list [train_images, train_labels]\n",
    "    #           val_set: The list [val_images, val_labels] \n",
    "\n",
    "def load_images():\n",
    "    \n",
    "    sets = ['train', 'val']\n",
    "    \n",
    "    data_sets = []\n",
    "    for dset in sets:\n",
    "        img_path = './bin_dataset/' + dset + '/ele'\n",
    "        ele_list = [imread(os.path.join(img_path, img)) for img in os.listdir(img_path)]\n",
    "\n",
    "        img_path = './bin_dataset/' + dset + '/lio'\n",
    "        lio_list = [imread(os.path.join(img_path, img)) for img in os.listdir(img_path)]\n",
    "\n",
    "        set_images = np.stack(ele_list + lio_list)\n",
    "        N = set_images.shape[0]\n",
    "        labels = np.ones((N,1))\n",
    "        labels[0:int(N/2)] = 0\n",
    "        data_sets.append([set_images, labels])\n",
    "\n",
    "    train_set, val_set = data_sets\n",
    "\n",
    "    print(\"Loaded\", len(train_set[0]), \"training images\")\n",
    "    print(\"Loaded\", len(val_set[0]), \"validation images\")\n",
    "    \n",
    "    return train_set, val_set\n",
    "\n",
    "\n",
    "# batchify\n",
    "    # Inputs:    train_set: List containing images and labels\n",
    "    #            batch size: The desired size of each batch\n",
    "    #\n",
    "    # Returns:   image_batches: A list of shuffled training image batches, each with size batch_size\n",
    "    #            label_batches: A list of shuffled training label batches, each with size batch_size \n",
    "\n",
    "def batchify(train_set, batch_size):\n",
    "    num_batches = int(len(train_set[0])/batch_size)\n",
    "    image_batches = np.empty((num_batches,batch_size,100,100,3))\n",
    "    label_batches = np.empty((num_batches,batch_size))\n",
    "    seeds = np.arange(len(train_set[0]))\n",
    "    np.random.shuffle(seeds)\n",
    "    for i in range(num_batches):\n",
    "        for j in range(batch_size):\n",
    "            idx = seeds[i*batch_size + j]\n",
    "            image_batches[i][j] = train_set[0][idx]\n",
    "            label_batches[i][j] = train_set[1][idx]\n",
    "    return image_batches,label_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu\n",
    "    # Inputs:   x: Multi-dimensional array with size N along the first axis\n",
    "    # \n",
    "    # Returns:  out: Multi-dimensional array with same size of x \n",
    "\n",
    "def relu(x):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    N,W,H,D = len(x),len(x[0]),len(x[0][0]),len(x[0][0][0])\n",
    "    out = np.zeros((N,W,H,D))\n",
    "    for i in range(N):\n",
    "        for j in range(W):\n",
    "            for k in range(H):\n",
    "                for l in range(D):\n",
    "                    out[i][j][k][l] = max(0,x[i][j][k][l])\n",
    "    return out\n",
    "\n",
    "def relu2(x):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    N,F = len(x),len(x[0])\n",
    "    out = np.zeros((N,F))\n",
    "    for i in range(N):\n",
    "        for j in range(F):\n",
    "            out[i][j] = max(0,x[i][j])\n",
    "    return out\n",
    "\n",
    "# sigmoid\n",
    "    # Inputs:    x: Multi-dimensional array with size N along the first axis\n",
    "    # \n",
    "    # Returns:   out: Multi-dimensional array with same size of x \n",
    "\n",
    "def sigmoid(x):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    N = len(x)\n",
    "    out = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        out[i] = sig_help(x[i])\n",
    "    return out\n",
    "\n",
    "def sig_help(x):\n",
    "    return 1/(1+ np.exp(-x))\n",
    "\n",
    "# unit_step\n",
    "    # Inputs:    x: Multi-dimensional array with size N along the first axis \n",
    "    # \n",
    "    # Returns:   out: Multi-dimensional array with same size of x \n",
    "\n",
    "def unit_step(x):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "#     N,W = len(x),len(x[0])\n",
    "#     out = np.zeros((N,W))\n",
    "#     for i in range(N):\n",
    "#         for j in range(W):\n",
    "#             if(x[i][j] > 0):\n",
    "#                 out[i][j] = 1\n",
    "    \n",
    "    out = np.heaviside(x, 0)\n",
    "\n",
    "    return out "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolve2D\n",
    "    # Inputs:    X: [N x height x width x num_channels]\n",
    "    #            filters: [num_filters x filter_height x filter_width x num_input_channels]\n",
    "    # \n",
    "    # Returns:   Xc: output array by convoling X and filters. [N x output_height x output_width x num_filters]\n",
    "\n",
    "def convolve2D(X0, filters):\n",
    "   \n",
    "    N, X0_len, _, num_ch = X0.shape\n",
    "    num_out_ch, filter_len, _, _ = filters.shape\n",
    "    F0_side = X0_len - filter_len + 1\n",
    "\n",
    "    F0 = []\n",
    "    for n in range(N):\n",
    "        for o_ch in range(num_out_ch):\n",
    "            conv = 0\n",
    "            for ch in range(num_ch):\n",
    "                # YOUR CODE HERE\n",
    "                f = filters[o_ch,:,:,ch]\n",
    "                sample = X0[n]\n",
    "                # print('sample size: ',sample)\n",
    "                c = signal.convolve2d(sample[:,:,ch],f, mode='valid')\n",
    "                conv += c\n",
    "            if o_ch == 0:\n",
    "                conv_stack = conv\n",
    "            else:\n",
    "                conv_stack = np.dstack((conv_stack, conv))\n",
    "        conv_stack = conv_stack.tolist()\n",
    "        F0.append(conv_stack)\n",
    "    return F0\n",
    "\n",
    "\n",
    "# maxPool\n",
    "    # Inputs:    R0: [N x height x width x num_channels]\n",
    "    #            mp_len: size of max pool window, also the stride for this MP\n",
    "    # \n",
    "    # Returns:   p_out: output of pooling R0. [N x output_height x output_width x num_channels]\n",
    "    #            R0_mask: A binary mask with the same size as R0. Indicates which index was chosen to be the max\n",
    "    #            for each max pool window. This will be used for backpropagation.\n",
    "\n",
    "def maxPool(R0, mp_len):\n",
    "\n",
    "    N, R0_len, _, num_ch = R0.shape\n",
    "    p_out_len = int((R0_len-mp_len)/mp_len + 1)\n",
    "\n",
    "    R0_mask = np.zeros(R0.shape)\n",
    "    p_out = np.zeros((N, p_out_len, p_out_len, num_ch))\n",
    "    \n",
    "    for n in range(N):\n",
    "        for ch in range(num_ch):\n",
    "            for row in range(p_out_len): \n",
    "                for col in range(p_out_len):\n",
    "                    # YOUR CODE HERE\n",
    "                    cells = []\n",
    "                    for i in range(mp_len):\n",
    "                        for j in range(mp_len):\n",
    "                            if((row * mp_len + i) < R0_len  and (col * mp_len + j) < R0_len ):\n",
    "                                cells.append(R0[n][row * mp_len + i][col * mp_len + j][ch])\n",
    "                            else:\n",
    "                                cells.append(0)\n",
    "                    p_out[n][row][col][ch] = np.max(cells)\n",
    "                    pool_index = np.argmax(cells)\n",
    "#                     print(len(cells))\n",
    "#                     for i in range(mp_len):\n",
    "#                         for j in range(mp_len):\n",
    "#                             if((row * mp_len + i) < R0_len  and (col * mp_len + j) < R0_len ):\n",
    "#                                 R0_mask[n][row * mp_len + i][col * mp_len + j][ch] = pool_index                    \n",
    "                    \n",
    "                    \n",
    "#                     window = R0[n,row*mp_len:(row+1)*mp_len-1,col*mp_len:(col+1)*mp_len-1,ch]\n",
    "#                     window = np.array(window)\n",
    "#                     max_val = window.max()\n",
    "#                     p_out[n][row][col][ch] = max_val\n",
    "                    i,j = np.unravel_index(pool_index, (mp_len,mp_len))\n",
    "                    R0_mask[n][i+row*mp_len][j+col*mp_len][ch] = 1\n",
    "#                     if (n == 2) and (ch == 2) and (row == 2) and (col == 2):\n",
    "#                         print(window)\n",
    "#                         print(R0_mask[n,row*mp_len:(row+1)*mp_len-1,col*mp_len:(col+1)*mp_len-1,ch])\n",
    "\n",
    "    return p_out, R0_mask\n",
    "\n",
    "# fc\n",
    "    # Inputs:    X: [N x num_input_features]\n",
    "    #            W: [num_input_features x num_fc_nodes]\n",
    "    # \n",
    "    # Returns:   out: Linear combination of X and W. [N x num_fc_nodes]\n",
    "\n",
    "def fc(X, W):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    out = np.dot(X,W)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn_fwd\n",
    "    # Inputs:    X0: batch of images. [N x img_height x img_width x num_channels]\n",
    "    #            W0, W1, W2: Parameters of the CNN\n",
    "    #            mp_len: the length of one side of the max pool window\n",
    "    # \n",
    "    # Returns:   sig: vector containing the output for each sample. [N x 1]\n",
    "    #            cache: a dict containing the relevant output layer calculations that will be\n",
    "    #            used in backpropagation\n",
    "    \n",
    "def cnn_fwd(X0, W0, W1, W2, mp_len):\n",
    "    \n",
    "    # F0 \n",
    "    # YOUR CODE HERE\n",
    "    F0 = convolve2D(X0, W0)\n",
    "    R0 = relu(F0)\n",
    "    # X1p \n",
    "    # YOUR CODE HERE\n",
    "    X1p, R0_mask = maxPool(R0, mp_len)\n",
    "    # X1 (flatten)\n",
    "    # YOUR CODE HERE\n",
    "    X1 = X1p.reshape(len(X0),-1)\n",
    "    # FC Layers\n",
    "    # YOUR CODE HERE\n",
    "    F1 = fc(X1,W1)\n",
    "    X2 = relu2(F1)\n",
    "    F2 = fc(X2,W2)\n",
    "    # Output\n",
    "    # YOUR CODE HERE\n",
    "    sig = sigmoid(F2)\n",
    "    # Save outputs of functions for backward pass\n",
    "    cache = {\n",
    "        \"F0\":F0,\n",
    "        \"R0\":R0,\n",
    "        \"X1p\":X1p,\n",
    "        \"R0m\":R0_mask,\n",
    "        \"X1\":X1,\n",
    "        \"F1\":F1,\n",
    "        \"X2\":X2,\n",
    "        \"F2\":F2      \n",
    "    }\n",
    "#     print(\"F0\",F0.shape)\n",
    "#     print(\"R0\",R0.shape)\n",
    "#     print(\"X1p\",X1p.shape)\n",
    "#     print(\"R0m\",R0_mask.shape)\n",
    "#     print(\"X1\",X1.shape)\n",
    "#     print(\"F1\",F1.shape)\n",
    "#     print(\"X2\",X2.shape)\n",
    "#     print(\"F2\",F2.shape)\n",
    "    \n",
    "    return sig, cache\n",
    "\n",
    "\n",
    "# loss\n",
    "    # Inputs:    sig: vector containing the CNN output for each sample. [N x 1]\n",
    "    #            Y: vector containing the ground truth label for each sample. [N x 1]\n",
    "    # \n",
    "    # Returns:   L: Loss/error criterion for the model. \n",
    "\n",
    "def loss(sig, Y):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    N = len(sig)\n",
    "    L = 0\n",
    "    for i in range(N):\n",
    "        L += -Y[i]*np.log(sig[i]) - (1- Y[i])*np.log(1-sig[i])\n",
    "    L = L/N\n",
    "    return L\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backprop Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolve2DBwd\n",
    "    # Inputs:    X0: batch of images. [N x height x width x num_channels]\n",
    "    #            dL_dF0: Gradient at the output of the conv layer. \n",
    "    # \n",
    "    # Returns:   dL_dW0. gradient of loss L wrt W0. Same size as W0\n",
    "\n",
    "def convolve2DBwd(X0, dL_dF0):\n",
    "    \n",
    "    N, X0_len, _, num_ch = X0.shape\n",
    "    _, dL_dF0_len, _, num_out_ch  = dL_dF0.shape\n",
    "    filter_len = X0_len - dL_dF0_len + 1\n",
    "    \n",
    "    dL_dW0 = np.zeros((num_out_ch, filter_len, filter_len, num_ch))\n",
    "    \n",
    "    for n in range(N):\n",
    "        for o_ch in range(num_out_ch):\n",
    "            for ch in range(num_ch):\n",
    "                # YOUR CODE HERE \n",
    "                A = X0[n,:,:,ch]\n",
    "                B = dL_dF0[n,:,:,o_ch]\n",
    "\n",
    "                #reverse A\n",
    "                A_ = np.fliplr( np.flip(A,axis=0)) \n",
    "                B_ = np.fliplr(np.flip(B,axis=0))\n",
    "\n",
    "#                 if (n == 0) and (o_ch == 0) and (ch == 0):\n",
    "#                     print(A)\n",
    "#                     print(A_)\n",
    "                corr = signal.convolve2d(A, B_, mode='valid')    \n",
    "#                 print(\"corr\",corr)\n",
    "                dL_dW0[o_ch,:,:,ch] += corr\n",
    "    \n",
    "    return dL_dW0\n",
    "\n",
    "\n",
    "# maxPoolBwd\n",
    "    # Inputs:    dL_dX1p: Gradient at the output of the MaxPool layer\n",
    "    #            R0_mask: A binary mask with the same size as R0. Defined in maxPool\n",
    "    #            mp_len: the length of one side of the max pool window\n",
    "    # \n",
    "    # Returns:   dL_dR0: Gradient at the output of ReLu\n",
    "    \n",
    "def maxPoolBwd(dL_dX1p, R0_mask,  mp_len):\n",
    "    \n",
    "    N, H, W, C = R0_mask.shape\n",
    "    N, dH, dW, C = dL_dX1p.shape\n",
    "    \n",
    "    dL_dR0 = np.zeros(R0_mask.shape)\n",
    "    \n",
    "    for n in range(N):\n",
    "        for ch in range(C):\n",
    "            for row in range(dH):\n",
    "                for col in range(dW):\n",
    "                    # YOUR CODE HERE\n",
    "                    window = R0_mask[n,row*mp_len:(row+1)*mp_len,col*mp_len:(col+1)*mp_len,ch]\n",
    "                    i,j = np.unravel_index(window.argmax(), window.shape)                    \n",
    "                    dL_dR0[n][i+row*mp_len][j+col*mp_len][ch] = dL_dX1p[n][row][col][ch]\n",
    "#                     if (n == 0) and (ch == 0) and (row == 0) and (col == 0):\n",
    "#                         print(window)\n",
    "#                         print(dL_dR0[n,row*mp_len:(row+1)*mp_len-1,col*mp_len:(col+1)*mp_len-1,ch])\n",
    "#                         print(i,j)\n",
    "                    \n",
    "    return dL_dR0\n",
    "\n",
    "def dL_dF2(Y, cache):\n",
    "    N = len(Y)\n",
    "    F2 = cache['F2']\n",
    "    dL_dF2 = np.zeros((N,1))\n",
    "    for i in range(N):\n",
    "        dL_dF2[i] = (-1*Y[i]*(1-sigmoid(F2[i])) + (1-Y[i])*sigmoid(F2[i]))\n",
    "    dL_dF2 /= N\n",
    "    return dL_dF2\n",
    "\n",
    "# dL_dW2\n",
    "    # Inputs:    Y: vector containing the ground truth label for each sample. [N x 1]\n",
    "    #            cache: a dict containing the relevant output layer calculations \n",
    "    # \n",
    "    # Returns:   dL_dW2: Gradient of the Loss wrt W2\n",
    "    \n",
    "def dL_dW2(Y, cache):\n",
    "   \n",
    "    # YOUR CODE HERE\n",
    "#     N = len(Y)\n",
    "#     f2 = cache[\"F2\"]\n",
    "#     x2 = cache[\"X2\"]\n",
    "#     sig = sigmoid(f2)\n",
    "#     dL_dW2 = 0\n",
    "#     for i in range(N):\n",
    "#         dL_dW2 += np.outer(sig[i]-Y[i],x2[i])\n",
    "#     dL_dW2 = dL_dW2/N\n",
    "#     dL_dW2 = dL_dW2.T\n",
    "\n",
    "    dLdF2 = dL_dF2(Y, cache)\n",
    "    X2 = cache['X2']\n",
    "    dF2_dW2 = X2\n",
    "    dL_dW2 = np.matmul(dF2_dW2.T, dLdF2)\n",
    "    \n",
    "    \n",
    "    return dL_dW2    \n",
    "\n",
    "# dL_dW1\n",
    "    # Inputs:    Y: vector containing the ground truth label for each sample. [N x 1]\n",
    "    #            W2: Weight matrix for the second FC layer\n",
    "    #            cache: a dict containing the relevant output layer calculations \n",
    "    # \n",
    "    # Returns:   dL_dW1: Gradient of the Loss wrt W1\n",
    "    \n",
    "def dL_dW1(Y, W2, cache):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "#     N = len(Y)\n",
    "#     f2 = cache[\"F2\"]\n",
    "#     f1 = cache[\"F1\"]\n",
    "#     x1 = cache[\"X1\"]\n",
    "#     sig = sigmoid(f2)\n",
    "#     unit = unit_step(f1) \n",
    "#     dL_dW1 = 0\n",
    "#     for i in range(N):\n",
    "#         dL_dW1 += np.outer( np.dot(sig[i]-Y[i], np.transpose(W2)) *unit[i]  ,x1[i])   \n",
    "#     dL_dW1 = dL_dW1/N\n",
    "#     dL_dW1 = dL_dW1.T\n",
    "\n",
    "    F1 = cache['F1']\n",
    "    X1 = cache['X1']\n",
    "    dLdF2 = dL_dF2(Y, cache)\n",
    "    dF2dX2 = W2\n",
    "    dX2dF1 = unit_step(F1)\n",
    "    dF1dW1 = X1\n",
    "    A = np.matmul(dLdF2, dF2dX2.T)\n",
    "    B = A * dX2dF1\n",
    "    dL_dW1 = np.matmul(dF1dW1.T, B)\n",
    "    return dL_dW1\n",
    "    \n",
    "#     return dL_dW1\n",
    "\n",
    "\n",
    "# dL_dW0\n",
    "    # Inputs:    X0: batch of images. [N x height x width x num_channels]\n",
    "    #            Y: vector containing the ground truth label for each sample. [N x 1]\n",
    "    #            W1: Weight matrix for the first FC layer\n",
    "    #            W2: Weight matrix for the second FC layer\n",
    "    #            mp_len: the length of one side of the max pool window\n",
    "    #            cache: a dict containing the relevant output layer calculations \n",
    "    # \n",
    "    # Returns:   dL_dW0: Gradient of the Loss wrt W0\n",
    "\n",
    "def dL_dW0(X0, Y, W1, W2, mp_len, cache):\n",
    "    \n",
    "    N, X1p_len, _, no_out_ch  = cache['X1p'].shape\n",
    "    F2 = cache['F2']\n",
    "    F1 = cache['F1']\n",
    "    R0m = cache['R0m']\n",
    "    F0 = cache['F0']\n",
    "    \n",
    "    #dL_dF2\n",
    "    # YOUR CODE HERE\n",
    "    dLdF2 = dL_dF2(Y, cache) \n",
    "#     print(\"dldF2\", dLdF2.shape)\n",
    "#     print(\"dldF2\", dLdF2)\n",
    "\n",
    "    #dL_dF1\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    dF2_dX2 = W2\n",
    "    dX2_dF1 = unit_step(F1)\n",
    "    dL_dF1 = np.dot(dLdF2, dF2_dX2.T) * dX2_dF1  \n",
    "#     print(np.matmul(dLdF2, dF2_dX2.T).shape)\n",
    "#     print(\"dldf1\",dL_dF1.shape)\n",
    "#     print(\"dldf1\",dL_dF1)\n",
    "\n",
    "    #dL_dX1\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    dF1_dX1 = W1\n",
    "    dL_dX1 = np.dot(dL_dF1, dF1_dX1.T)\n",
    "#     print(\"dldx1\", dL_dX1.shape)\n",
    "#     print(\"dldx1\", dL_dX1)\n",
    "\n",
    "    # dL_dX1p (unflatten)\n",
    "    # YOUR CODE HERE\n",
    "    dL_dX1p = dL_dX1.reshape(cache['X1p'].shape) \n",
    "#     print(\"dldx1p\", dL_dX1p.shape)\n",
    "#     print(\"dldx1p\", dL_dX1p)\n",
    "\n",
    "    # dL_dR0 (unpool)\n",
    "    # YOUR CODE HERE\n",
    "    dL_dR0 = maxPoolBwd(dL_dX1p, R0m, mp_len)\n",
    "#     print(\"dL_dR0\", dL_dR0.shape)\n",
    "#     print(\"dL_dR0\", dL_dR0)\n",
    "\n",
    "    # dL_dF0 (relu_bwd)\n",
    "    # YOUR CODE HERE\n",
    "    dR0_dF0 = unit_step(F0)\n",
    "#     dR0_dF0 = unit_step(dL_dR0)\n",
    "#     print(np.sum(dR0_dF0))\n",
    "#     print(\"len\",16*96*96*3)\n",
    "    dL_dF0 = dL_dR0 *  dR0_dF0\n",
    "#     print(\"dL_dF0\",dL_dF0.shape)\n",
    "#     print(\"dL_dF0\",dL_dF0)\n",
    "\n",
    "    # dL_dW0\n",
    "    # YOUR CODE HERE\n",
    "    dL_dW0 = convolve2DBwd(X0, dL_dF0)\n",
    "#     print(\"dldw0\", dL_dW0.shape)\n",
    "#     print(\"dldw0\", dL_dW0)\n",
    "    \n",
    "    return dL_dW0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2]\n",
      " [3 4 5]\n",
      " [6 7 8]]\n",
      "[[8 7 6]\n",
      " [5 4 3]\n",
      " [2 1 0]]\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(9).reshape(3,3)\n",
    "a_ = np.fliplr( np.flip(a,axis=0)) \n",
    "print(a)\n",
    "print(a_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2000 training images\n",
      "Loaded 800 validation images\n"
     ]
    }
   ],
   "source": [
    "# Load images and scale them\n",
    "# YOUR CODE HERE\n",
    "ts, vs = load_images()\n",
    "#normalize\n",
    "train_set = []\n",
    "train_set.append(ts[0].astype(float))\n",
    "train_set.append(ts[1])\n",
    "\n",
    "val_set = []\n",
    "val_set.append(vs[0].astype(float))\n",
    "val_set.append(vs[1])\n",
    "\n",
    "for n in range(len(train_set[0])):\n",
    "    for i in range(len(train_set[0][0])):\n",
    "        for j in range(len(train_set[0][0][0])):\n",
    "            for ch in range(len(train_set[0][0][0][0])):\n",
    "                train_set[0][n][i][j][ch] = train_set[0][n][i][j][ch] /255\n",
    "\n",
    "for n in range(len(val_set[0])):\n",
    "    for i in range(len(val_set[0][0])):\n",
    "        for j in range(len(val_set[0][0][0])):\n",
    "            for ch in range(len(val_set[0][0][0][0])):\n",
    "                val_set[0][n][i][j][ch] = val_set[0][n][i][j][ch] /255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "epochs = 20\n",
    "lr = 0.1\n",
    "batch_size = 16\n",
    "filter_len = 5\n",
    "num_out_ch = 3\n",
    "mp_len = 12\n",
    "fc_nodes = 2\n",
    "\n",
    "# Declare weights\n",
    "myw0 = 0.05*np.random.randn(3,5,5,3)\n",
    "myw1 = 0.05*np.random.randn(192,2)\n",
    "myw2 = 0.05*np.random.randn(2,1)\n",
    "print(epochs)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Batch: 1 / 125\n",
      "Batch: 2 / 125\n",
      "Batch: 3 / 125\n",
      "Batch: 4 / 125\n",
      "Batch: 5 / 125\n",
      "Batch: 6 / 125\n",
      "Batch: 7 / 125\n",
      "Batch: 8 / 125\n",
      "Batch: 9 / 125\n",
      "Batch: 10 / 125\n",
      "Batch: 11 / 125\n",
      "Batch: 12 / 125\n",
      "Batch: 13 / 125\n",
      "Batch: 14 / 125\n",
      "Batch: 15 / 125\n",
      "Batch: 16 / 125\n",
      "Batch: 17 / 125\n",
      "Batch: 18 / 125\n",
      "Batch: 19 / 125\n",
      "Batch: 20 / 125\n",
      "Batch: 21 / 125\n",
      "Batch: 22 / 125\n",
      "Batch: 23 / 125\n",
      "Batch: 24 / 125\n",
      "Batch: 25 / 125\n",
      "Batch: 26 / 125\n",
      "Batch: 27 / 125\n",
      "Batch: 28 / 125\n",
      "Batch: 29 / 125\n",
      "Batch: 30 / 125\n",
      "Batch: 31 / 125\n",
      "Batch: 32 / 125\n",
      "Batch: 33 / 125\n",
      "Batch: 34 / 125\n",
      "Batch: 35 / 125\n",
      "Batch: 36 / 125\n",
      "Batch: 37 / 125\n",
      "Batch: 38 / 125\n",
      "Batch: 39 / 125\n",
      "Batch: 40 / 125\n",
      "Batch: 41 / 125\n",
      "Batch: 42 / 125\n",
      "Batch: 43 / 125\n",
      "Batch: 44 / 125\n",
      "Batch: 45 / 125\n",
      "Batch: 46 / 125\n",
      "Batch: 47 / 125\n",
      "Batch: 48 / 125\n",
      "Batch: 49 / 125\n",
      "Batch: 50 / 125\n",
      "Batch: 51 / 125\n",
      "Batch: 52 / 125\n",
      "Batch: 53 / 125\n",
      "Batch: 54 / 125\n",
      "Batch: 55 / 125\n",
      "Batch: 56 / 125\n",
      "Batch: 57 / 125\n",
      "Batch: 58 / 125\n",
      "Batch: 59 / 125\n",
      "Batch: 60 / 125\n",
      "Batch: 61 / 125\n",
      "Batch: 62 / 125\n",
      "Batch: 63 / 125\n",
      "Batch: 64 / 125\n",
      "Batch: 65 / 125\n",
      "Batch: 66 / 125\n",
      "Batch: 67 / 125\n",
      "Batch: 68 / 125\n",
      "Batch: 69 / 125\n",
      "Batch: 70 / 125\n",
      "Batch: 71 / 125\n",
      "Batch: 72 / 125\n",
      "Batch: 73 / 125\n",
      "Batch: 74 / 125\n",
      "Batch: 75 / 125\n",
      "Batch: 76 / 125\n",
      "Batch: 77 / 125\n",
      "Batch: 78 / 125\n",
      "Batch: 79 / 125\n",
      "Batch: 80 / 125\n",
      "Batch: 81 / 125\n",
      "Batch: 82 / 125\n",
      "Batch: 83 / 125\n",
      "Batch: 84 / 125\n",
      "Batch: 85 / 125\n",
      "Batch: 86 / 125\n",
      "Batch: 87 / 125\n",
      "Batch: 88 / 125\n",
      "Batch: 89 / 125\n",
      "Batch: 90 / 125\n",
      "Batch: 91 / 125\n",
      "Batch: 92 / 125\n",
      "Batch: 93 / 125\n",
      "Batch: 94 / 125\n",
      "Batch: 95 / 125\n",
      "Batch: 96 / 125\n",
      "Batch: 97 / 125\n",
      "Batch: 98 / 125\n",
      "Batch: 99 / 125\n",
      "Batch: 100 / 125\n",
      "Batch: 101 / 125\n",
      "Batch: 102 / 125\n",
      "Batch: 103 / 125\n",
      "Batch: 104 / 125\n",
      "Batch: 105 / 125\n",
      "Batch: 106 / 125\n",
      "Batch: 107 / 125\n",
      "Batch: 108 / 125\n",
      "Batch: 109 / 125\n",
      "Batch: 110 / 125\n",
      "Batch: 111 / 125\n",
      "Batch: 112 / 125\n",
      "Batch: 113 / 125\n",
      "Batch: 114 / 125\n",
      "Batch: 115 / 125\n",
      "Batch: 116 / 125\n",
      "Batch: 117 / 125\n",
      "Batch: 118 / 125\n",
      "Batch: 119 / 125\n",
      "Batch: 120 / 125\n",
      "Batch: 121 / 125\n",
      "Batch: 122 / 125\n",
      "Batch: 123 / 125\n",
      "Batch: 124 / 125\n",
      "Batch: 125 / 125\n",
      "Training accuracy for epoch  0 : 0.8175\n",
      "1\n",
      "Batch: 1 / 125\n",
      "Batch: 2 / 125\n",
      "Batch: 3 / 125\n",
      "Batch: 4 / 125\n",
      "Batch: 5 / 125\n",
      "Batch: 6 / 125\n",
      "Batch: 7 / 125\n",
      "Batch: 8 / 125\n",
      "Batch: 9 / 125\n",
      "Batch: 10 / 125\n",
      "Batch: 11 / 125\n",
      "Batch: 12 / 125\n",
      "Batch: 13 / 125\n",
      "Batch: 14 / 125\n",
      "Batch: 15 / 125\n",
      "Batch: 16 / 125\n",
      "Batch: 17 / 125\n",
      "Batch: 18 / 125\n",
      "Batch: 19 / 125\n",
      "Batch: 20 / 125\n",
      "Batch: 21 / 125\n",
      "Batch: 22 / 125\n",
      "Batch: 23 / 125\n",
      "Batch: 24 / 125\n",
      "Batch: 25 / 125\n",
      "Batch: 26 / 125\n",
      "Batch: 27 / 125\n",
      "Batch: 28 / 125\n",
      "Batch: 29 / 125\n",
      "Batch: 30 / 125\n",
      "Batch: 31 / 125\n",
      "Batch: 32 / 125\n",
      "Batch: 33 / 125\n",
      "Batch: 34 / 125\n",
      "Batch: 35 / 125\n",
      "Batch: 36 / 125\n",
      "Batch: 37 / 125\n",
      "Batch: 38 / 125\n",
      "Batch: 39 / 125\n",
      "Batch: 40 / 125\n",
      "Batch: 41 / 125\n",
      "Batch: 42 / 125\n",
      "Batch: 43 / 125\n",
      "Batch: 44 / 125\n",
      "Batch: 45 / 125\n",
      "Batch: 46 / 125\n",
      "Batch: 47 / 125\n",
      "Batch: 48 / 125\n",
      "Batch: 49 / 125\n",
      "Batch: 50 / 125\n",
      "Batch: 51 / 125\n",
      "Batch: 52 / 125\n",
      "Batch: 53 / 125\n",
      "Batch: 54 / 125\n",
      "Batch: 55 / 125\n",
      "Batch: 56 / 125\n",
      "Batch: 57 / 125\n",
      "Batch: 58 / 125\n",
      "Batch: 59 / 125\n",
      "Batch: 60 / 125\n",
      "Batch: 61 / 125\n",
      "Batch: 62 / 125\n",
      "Batch: 63 / 125\n",
      "Batch: 64 / 125\n",
      "Batch: 65 / 125\n",
      "Batch: 66 / 125\n",
      "Batch: 67 / 125\n",
      "Batch: 68 / 125\n",
      "Batch: 69 / 125\n",
      "Batch: 70 / 125\n",
      "Batch: 71 / 125\n",
      "Batch: 72 / 125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-9a358983058c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mmysig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmycache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_fwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmyw0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmyw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmyw2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmp_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Calculate gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-75f876ecebf7>\u001b[0m in \u001b[0;36mcnn_fwd\u001b[0;34m(X0, W0, W1, W2, mp_len)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mF0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvolve2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mR0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;31m# X1p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-76587eb2ff74>\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epochnum in range(epochs):\n",
    "    print(epochnum)\n",
    "    # make set of batches\n",
    "    img_batches,label_batches = batchify(train_set,batch_size)\n",
    "    num_batches = len(img_batches)\n",
    "    for b_idx in range(num_batches):\n",
    "        print(\"Batch:\",b_idx+1,\"/\",num_batches)\n",
    "        X = np.array(img_batches[b_idx])\n",
    "        Y = np.array(label_batches[b_idx])\n",
    "        \n",
    "        # Forward pass\n",
    "        mysig,mycache = cnn_fwd(X, myw0, myw1, myw2, mp_len)\n",
    "        \n",
    "        # Calculate gradients\n",
    "        w0grad = dL_dW0(X, Y, myw1, myw2, mp_len, mycache)        \n",
    "        w1grad = dL_dW1(Y, myw2, mycache)\n",
    "        w2grad = dL_dW2(Y, mycache)\n",
    "        \n",
    "        # Update gradients\n",
    "        myw2 = myw2 - lr* w2grad\n",
    "        myw1 = myw1 - lr* w1grad\n",
    "        myw0 = myw0 - lr* w0grad\n",
    "    pickle.dump( (myw0,myw1,myw2), open( \"weights.p\", \"wb\" ) )\n",
    "    sig, _ = cnn_fwd(val_set[0], myw0, myw1, myw2, mp_len)\n",
    "    train_acc = 0\n",
    "    for sigi in range(len(sig)):\n",
    "        if(np.round(sig[sigi])==val_set[1][sigi]):\n",
    "            train_acc +=1\n",
    "    train_acc = (train_acc/len(sig))\n",
    "    print(\"Training accuracy for epoch \",epochnum,\":\",train_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Correctness of Forward and Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0 dim (3, 5, 5, 3)\n",
      "W1 dim (192, 2)\n",
      "W2 dim (2, 1)\n",
      "F0 (800, 96, 96, 3)\n",
      "R0 (800, 96, 96, 3)\n",
      "X1p (800, 8, 8, 3)\n",
      "R0m (800, 96, 96, 3)\n",
      "X1 (800, 192)\n",
      "F1 (800, 2)\n",
      "X2 (800, 2)\n",
      "F2 (800, 1)\n",
      "train_loss: [ 0.23271396] train_acc: 0.9175\n"
     ]
    }
   ],
   "source": [
    "weights = np.load('weights.npz')\n",
    "W0 = weights['W0']\n",
    "W1 = weights['W1']\n",
    "W2 = weights['W2']\n",
    "print(\"W0 dim\", W0.shape)\n",
    "print(\"W1 dim\", W1.shape)\n",
    "print(\"W2 dim\", W2.shape)\n",
    "sig, _ = cnn_fwd(val_set[0], W0, W1, W2, mp_len)\n",
    "train_acc = 0\n",
    "for i in range(len(sig)):\n",
    "    if(np.round(sig[i])==val_set[1][i]):\n",
    "        train_acc +=1\n",
    "train_acc = (train_acc/len(sig))\n",
    "\n",
    "print(\"train_loss:\", loss(sig, val_set[1]), \"train_acc:\", train_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F0 (16, 96, 96, 3)\n",
      "R0 (16, 96, 96, 3)\n",
      "X1p (16, 8, 8, 3)\n",
      "R0m (16, 96, 96, 3)\n",
      "X1 (16, 192)\n",
      "F1 (16, 2)\n",
      "X2 (16, 2)\n",
      "F2 (16, 1)\n",
      "dldF2 (16, 1)\n",
      "dldf1 (16, 2)\n",
      "dldx1 (16, 192)\n",
      "dldx1p (16, 8, 8, 3)\n",
      "dL_dR0 (16, 96, 96, 3)\n",
      "dL_dF0 (16, 96, 96, 3)\n",
      "dldw0 (3, 5, 5, 3)\n",
      "W2 value: -834.631617647\n",
      "W1 value: -452.631617647\n",
      "W0 value: -611.631617647\n"
     ]
    }
   ],
   "source": [
    "# Make backprop testing batch\n",
    "X_bp = np.vstack([train_set[0][0:8,:,:,:], train_set[0][-9:-1,:,:,:]])\n",
    "Y_bp = np.vstack([train_set[1][0:8], train_set[1][-9:-1]])\n",
    "\n",
    "# Initialize weights to all ones\n",
    "# YOUR CODE HERE\n",
    "W0 = np.ones((3,5,5,3))\n",
    "W1 = np.ones((192,2))\n",
    "W2 = np.ones((2,1))\n",
    "# Update weights once\n",
    "# YOUR CODE HERE\n",
    "sig, cache = cnn_fwd(X_bp,W0,W1,W2, mp_len)\n",
    "W2_grad = (dL_dW2(Y_bp,cache))\n",
    "W1_grad = dL_dW1(Y_bp,W2,cache)\n",
    "W0_grad = dL_dW0(X_bp, Y_bp, W1, W2, mp_len, cache)\n",
    "\n",
    "W2 = W2 - lr* W2_grad\n",
    "W1 = W1 - lr* W1_grad\n",
    "W0 = W0 - lr* W0_grad\n",
    "\n",
    "# print(dL_dW0(X_bp, Y_bp, W1, W2, mp_len, cache))\n",
    "print(\"W2 value:\", np.sum(W2))\n",
    "print(\"W1 value:\", np.sum(W1))\n",
    "print(\"W0 value:\", np.sum(W0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
