{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction\n",
    "train will have the training samples and test will have the test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.41283743e+00 -1.55428945e+01  2.92278843e+00  1.49472286e-01\n",
      "  6.35381972e-01  2.16369842e-01  1.42493539e-01  5.27392847e-02\n",
      "  2.55955474e-01  5.94233038e-02  1.82504452e-01  4.46193938e-03\n",
      "  4.64175756e-02 -1.72107996e-02]\n"
     ]
    }
   ],
   "source": [
    "#speaker dependent partitions\n",
    "#sdeptrain dimensions sdeptrain[speaker][word][utterance idx] with idxs given alphabetically\n",
    "sdeptrain = [[[[] for i in range(4)] for j in range(5)] for k in range(4)]\n",
    "#sdeptest dimensions sdeptrain[speaker][word] with idxs given alphabetically\n",
    "sdeptest = [[[] for i in range(5)] for j in range(4)]\n",
    "p = './feature'\n",
    "speakerList = ['dg', 'ls', 'mh', 'yx']\n",
    "wordList = ['asr','cnn','dnn','hmm','tts']\n",
    "snum = 0\n",
    "for s in speakerList:\n",
    "    speakerpath = p+'/'+s\n",
    "    wnum = 0\n",
    "    for w in wordList:\n",
    "        wordpath = speakerpath +'/'+s+'_'+w\n",
    "        #set up train\n",
    "        for utt in range(1,5):\n",
    "            fpath = wordpath + str(utt)+'.fea'\n",
    "            f = open(fpath,'r')\n",
    "            data = f.readlines()\n",
    "            data = [i.split(',') for i in data]\n",
    "            for sample in range(len(data)):\n",
    "                data[sample] = [float(i) for i in data[sample]]\n",
    "            for d in data:\n",
    "                sdeptrain[snum][wnum][utt-1].append(d)\n",
    "        #set up test\n",
    "        fpath2 = wordpath + str(5)+'.fea'\n",
    "        f2 = open(fpath2,'r')\n",
    "        data = f2.readlines()\n",
    "        data = [i.split(',') for i in data]\n",
    "        for d in data:\n",
    "            sdeptest[snum][wnum].append(d)\n",
    "        wnum += 1\n",
    "    snum += 1\n",
    "sdeptest = np.array(sdeptest)\n",
    "sdeptrain = np.array(sdeptrain)\n",
    "\n",
    "\n",
    "bigasr = []\n",
    "bigcnn = []\n",
    "bigdnn = []\n",
    "bighmm = []\n",
    "bigtts = []\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        bigasr.extend(sdeptrain[i][0][j])\n",
    "        bigcnn.extend(sdeptrain[i][1][j])\n",
    "        bigdnn.extend(sdeptrain[i][2][j])\n",
    "        bighmm.extend(sdeptrain[i][3][j])\n",
    "        bigtts.extend(sdeptrain[i][4][j])\n",
    "sdepasr_cov = np.diag(np.diag(np.cov(np.array(bigasr).T)))\n",
    "sdepcnn_cov = np.diag(np.diag(np.cov(np.array(bigcnn).T)))\n",
    "sdepdnn_cov = np.diag(np.diag(np.cov(np.array(bigdnn).T)))\n",
    "sdephmm_cov = np.diag(np.diag(np.cov(np.array(bighmm).T)))\n",
    "sdeptts_cov = np.diag(np.diag(np.cov(np.array(bigtts).T)))\n",
    "sdepasr_mn = np.mean(bigasr,axis=0)\n",
    "sdepcnn_mn = np.mean(bigcnn,axis=0)\n",
    "sdepdnn_mn = np.mean(bigdnn,axis=0)\n",
    "sdephmm_mn = np.mean(bighmm,axis=0)\n",
    "sdeptts_mn = np.mean(bigtts,axis=0)\n",
    "print(sdepasr_mn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#speaker independent partitions\n",
    "#sindeptrain dimensions sdeptrain[speaker][word][utterance idx] with idxs given alphabetically\n",
    "sindeptrain = [[[[] for i in range(5)] for j in range(5)] for k in range(3)]\n",
    "#sindeptest dimensions sdeptest[word][utterance idx] with idxs given alphabetically\n",
    "sindeptest = [[[] for i in range(5)] for j in range(5)]\n",
    "p = './feature'\n",
    "speakerList2 = ['dg', 'ls', 'mh']#'yx'\n",
    "wordList = ['asr','cnn','dnn','hmm','tts']\n",
    "snum = 0\n",
    "for s in speakerList2:\n",
    "    speakerpathi = p+'/'+s\n",
    "    wnum = 0\n",
    "    for w in wordList:\n",
    "        wordpathi = speakerpathi +'/'+s+'_'+w\n",
    "        #set up train\n",
    "        for utt in range(1,6):\n",
    "            fpathi = wordpathi + str(utt)+'.fea'\n",
    "            fi = open(fpathi,'r')\n",
    "            datai = fi.readlines()\n",
    "            datai = [i.split(',') for i in datai]\n",
    "            for sample in range(len(datai)):\n",
    "                datai[sample] = [float(i) for i in datai[sample]]\n",
    "            for d in datai:\n",
    "                sindeptrain[snum][wnum][utt-1].append(d)\n",
    "        wnum += 1\n",
    "    snum += 1\n",
    "    \n",
    "        #set up test\n",
    "s2 = 'yx'\n",
    "speakertestpath = p+'/'+s2\n",
    "wnum = 0\n",
    "for w in wordList:\n",
    "    wordtestpath = speakertestpath +'/'+s2+'_'+w\n",
    "    for utt in range(1,6):\n",
    "        ftestpath = wordtestpath + str(utt)+'.fea'\n",
    "        fitest = open(ftestpath,'r')\n",
    "        dataitest = fitest.readlines()\n",
    "        dataitest = [i.split(',') for i in dataitest]\n",
    "        for sample in range(len(dataitest)):\n",
    "            dataitest[sample] = [float(i) for i in dataitest[sample]]\n",
    "        for d in dataitest:\n",
    "            sindeptest[wnum][utt-1].append(d)\n",
    "    wnum += 1\n",
    "\n",
    "sindeptrain = np.array(sindeptrain)\n",
    "sindeptest = np.array(sindeptest)\n",
    "\n",
    "bigasri = []\n",
    "bigcnni = []\n",
    "bigdnni = []\n",
    "bighmmi = []\n",
    "bigttsi = []\n",
    "for i in range(3):\n",
    "    for j in range(5):\n",
    "        bigasri.extend(sindeptrain[i][0][j])\n",
    "        bigcnni.extend(sindeptrain[i][1][j])\n",
    "        bigdnni.extend(sindeptrain[i][2][j])\n",
    "        bighmmi.extend(sindeptrain[i][3][j])\n",
    "        bigttsi.extend(sindeptrain[i][4][j])\n",
    "sdepasri_cov = np.diag(np.diag(np.cov(np.array(bigasri).T)))\n",
    "sdepcnni_cov = np.diag(np.diag(np.cov(np.array(bigcnni).T)))\n",
    "sdepdnni_cov = np.diag(np.diag(np.cov(np.array(bigdnni).T)))\n",
    "sdephmmi_cov = np.diag(np.diag(np.cov(np.array(bighmmi).T)))\n",
    "sdepttsi_cov = np.diag(np.diag(np.cov(np.array(bigttsi).T)))\n",
    "sdepasri_mn = np.mean(bigasri,axis=0)\n",
    "sdepcnni_mn = np.mean(bigcnni,axis=0)\n",
    "sdepdnni_mn = np.mean(bigdnni,axis=0)\n",
    "sdephmmi_mn = np.mean(bighmmi,axis=0)\n",
    "sdepttsi_mn = np.mean(bigttsi,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### HELPER FUNCTIONSSSSS\n",
    "#implemented from https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm\n",
    "\n",
    "#Forward Pass\n",
    "\n",
    "def calculateBMatrix(X, mu, sigma,N):\n",
    "    #X is the data in a file\n",
    "    #mu is the average across all states \n",
    "    #sigma is the covariance matrix across all states \n",
    "    #N is the number of states\n",
    "    #BMatrix = [T,N]\n",
    "    #print(sigma[0])\n",
    "    T = len(X)\n",
    "    bMatrix = np.zeros((N,T))\n",
    "    for frame in range(T):\n",
    "        temp_frame = []\n",
    "        for state in range(N):\n",
    "#             print(\"MUSTATE\",mu[state])\n",
    "#             print(\"COVSTATE\",sigma[state])\n",
    "#             print(\"FRAME\",X[frame])\n",
    "#             print(stats.multivariate_normal(mean=(mu[state]),cov=sigma[state]).pdf(X[frame]))\n",
    "            bMatrix[state,frame] = stats.multivariate_normal(mean=(mu[state]),cov=sigma[state]).pdf(X[frame])\n",
    "    return bMatrix\n",
    "\n",
    "def calculateAlphaMatrix(transMatrix, BMatrix,priors):\n",
    "    #transMatrix is A \n",
    "    #BMatrix is B\n",
    "    #priors is pi\n",
    "    #populate our alpha matrix\n",
    "    T = len(BMatrix[0])\n",
    "    N = len(BMatrix)\n",
    "    Amat = np.array(transMatrix)\n",
    "    alpha = np.zeros((N,T))\n",
    "    #alphaMatrix = [T,N]\n",
    "    for i in range(N):\n",
    "        alpha[i,0]=priors[i]*BMatrix[i,0]\n",
    "    for t in range(1,T):\n",
    "        for i in range(0,N):\n",
    "            alpha[i,t]=BMatrix[i,t]*np.inner(alpha[:,t-1],Amat[:,i])       \n",
    "    return alpha\n",
    "\n",
    "def calculateAlphaBetaTildeG(transMatrix,BMatrix,priors):\n",
    "    T = len(BMatrix[0])\n",
    "    N = len(BMatrix)\n",
    "    baralpha = np.zeros((N,T))\n",
    "    tildealpha = np.zeros((N,T))\n",
    "    tildebeta = np.zeros((N,T))\n",
    "    log_g = np.zeros((T))\n",
    "    for i in range(0,N):\n",
    "#         print(\"PI\",priors[i])\n",
    "#         print(\"B\",BMatrix[i,0])\n",
    "        baralpha[i,0]=priors[i]*BMatrix[i,0]\n",
    "#     print(baralpha[:,0])\n",
    "    log_g[0] = np.log(np.sum(baralpha[:,0]))\n",
    "    tildealpha[:,0]=baralpha[:,0]/np.exp(log_g[0])\n",
    "\n",
    "    for t in range(1,T):\n",
    "        for i in range(0,N):\n",
    "            baralpha[i,t]=BMatrix[i,t]*np.inner(tildealpha[:,t-1],transMatrix[:,i])\n",
    "        log_g[t] = np.log(np.sum(baralpha[:,t]))\n",
    "        tildealpha[:,t]=baralpha[:,t]/np.exp(log_g[t])\n",
    "    for i in range(0,N):\n",
    "        tildebeta[i,T-1] = 1/np.exp(log_g[T-1])\n",
    "\n",
    "    for t in range(T-2,-1,-1):\n",
    "        for i in range(0,N):\n",
    "            tildebeta[i,t]=np.inner(transMatrix[i,0:N],tildebeta[:,t+1]*BMatrix[:,t+1])/np.exp(log_g[t+1])\n",
    "    \n",
    "    return tildealpha,tildebeta,log_g\n",
    "#Backward Pass\n",
    "    \n",
    "def calculateBetaMatrix(transMatrix, bMatrix):\n",
    "    #populate the Beta matrix\n",
    "    T = len(bMatrix[0])\n",
    "    N = len(bMatrix)\n",
    "    beta = np.zeros((N,T))\n",
    "    for i in range(N):\n",
    "        beta[i,T-1]=1\n",
    "    for t in range(T-2,-1,-1):\n",
    "        for i in range(0,N):\n",
    "            beta[i,t]=np.inner(transMatrix[i,0:N],beta[:,t+1]*bMatrix[:,t+1])\n",
    "    return beta\n",
    "#Update step helper functions\n",
    "\n",
    "def calculateGammaMatrix(alphaMatrix, betaMatrix):\n",
    "    T = len(betaMatrix[0])\n",
    "    N = len(betaMatrix)\n",
    "    gamma = np.zeros((N,T))\n",
    "    for t in range(T):\n",
    "       # print(\"ALPHA\",alphaMatrix[:,t])\n",
    "       # print(\"BETA\",betaMatrix[:,t])\n",
    "        gamma[:,t]=alphaMatrix[:,t]*betaMatrix[:,t]\n",
    "        gamma[:,t]=gamma[:,t]/np.sum(gamma[:,t])\n",
    "#     sumList = np.zeros((T))\n",
    "#     for f in range(T):\n",
    "#         for n in range(N):\n",
    "#             sumList[f] += alphaMatrix[f][n]*betaMatrix[f][n]\n",
    "#     #sumList = [sum([alphaMatrix[f][n] * betaMatrix[f][n] for n in range(N)]) for f in range(T)]\n",
    "#     gammaMatrix = []\n",
    "#     for frame in range(T):\n",
    "#         temp_frame = []\n",
    "#         for state in range(N):\n",
    "#             temp_frame.append(alphaMatrix[frame][state] / sumList[frame])\n",
    "#         gammaMatrix.append(temp_frame)\n",
    "    return gamma\n",
    "\n",
    "def calculateXiMatrix( transMatrix, alphaMatrix, betaMatrix, bMatrix):\n",
    "    #caluculate xi \n",
    "    #probability of going from state n to state j at times t and t+1 respectively\n",
    "    T = len(bMatrix[0])\n",
    "    N = len(bMatrix)\n",
    "    xi = np.zeros((2*N,T))\n",
    "    for t in range(0,T):\n",
    "        for i in range(0,N):\n",
    "            for j in range(i,i+2):\n",
    "                if j>= N:\n",
    "                    xi[i+j,t] = 0\n",
    "                else:\n",
    "                    xi[i+j,t]=alphaMatrix[i,t]*transMatrix[i,j]\n",
    "                if (t<T-1):\n",
    "                    if j==N:\n",
    "                        xi[i+j,t]=0\n",
    "                    else:\n",
    "                        xi[i+j,t] = xi[i+j,t]*bMatrix[j,t+1]*betaMatrix[j,t+1]\n",
    "        xi[:,t]=xi[:,t]/np.sum(xi[:,t])\n",
    "    \n",
    "#     for frame in range(T):\n",
    "#         for state in range(N):\n",
    "#             for state_ in range(state, state + 2):\n",
    "#                 xiMatrix[frame, state + state_] = alphaMatrix[frame, state] * transMatrix[state][state_]\n",
    "#                 if frame < (T-1):\n",
    "#                     if state_ == N:\n",
    "#                         xiMatrix[frame, state + state_] = 0\n",
    "#                     else:\n",
    "#                         xiMatrix[frame, state + state_] = xiMatrix[frame, state + state_] * bMatrix[frame + 1, state_] * betaMatrix[frame+1, state_]\n",
    "#         xi[:,t]=xi[:,t]/np.sum(xi[:,t])\n",
    "#         xiMatrix[frame,:] = xiMatrix[frame,:]/np.sum(xiMatrix[frame, :]) \n",
    "    \n",
    "    return xi \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#speaker independent hmm\n",
    "initialProbabilities = [0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "\n",
    "initialTransition = [[0.8,0.2,  0,  0,   0],\n",
    "                     [  0,0.8,0.2,  0,   0],\n",
    "                     [  0,  0,0.8,0.2,   0],\n",
    "                     [  0,  0,  0,0.8, 0.2],\n",
    "                     [  0,  0,  0,  0,   1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments\n",
    "\n",
    "There are N = 5 possible states and the number of observations is equal to the number of rows or time steps for each utterance. At each time step or observation, we are in one of the states, so a number 1-5 i think. So T = how many ever rows there are present. Still not sure exactly how we should chunk the data. Not sure if we should separate it by utterance or if we should group all the utterances of the same word together. Update parameters using baum welch forward/backward propogation equations. According to Piazza, update alpha and beta after each individual utterance and update the transition matrix after all the files for that word? Not how to implement this though. \n",
    "\n",
    "We train one hmm per word, so 5 in total. To predict a test sample, we run the utterance through each of the 5 models and assign it the label of the hmm that gives the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#SDEP general\n",
    "def SINDEPTRAIN(initialTransition,initialProbabilities,initialmeans,initialcov,wordidx,numiterations,numcoeff):\n",
    "    A = np.array(initialTransition)\n",
    "    mn = initialmeans\n",
    "    cov = initialcov\n",
    "    logprobs = np.zeros((15,numiterations))\n",
    "    for iteration in range(numiterations):\n",
    "        # calc params\n",
    "        alphas = []\n",
    "        betas = []\n",
    "        gammas = []\n",
    "        xis = []\n",
    "        for s in range(3):#for each speaker\n",
    "            for u in range(5): # for each utterance\n",
    "                B = calculateBMatrix(sindeptrain[s][wordidx][u],mn,cov,5)\n",
    "                Alpha,Beta,logg = calculateAlphaBetaTildeG(A,B,initialProbabilities)\n",
    "                Gamma = calculateGammaMatrix(Alpha,Beta)\n",
    "                xi = calculateXiMatrix(A,Alpha,Beta,B)\n",
    "                alphas.append(Alpha)\n",
    "                betas.append(Beta)\n",
    "                gammas.append(Gamma)\n",
    "                xis.append(xi)\n",
    "                logprobs[:,iteration] = np.sum(logg)\n",
    "\n",
    "        #upd8 \n",
    "        num_files = len(alphas)\n",
    "        N = len(gammas[0])\n",
    "        A_new = np.zeros((N,N))\n",
    "        mu_new = np.zeros((N,numcoeff))\n",
    "        sigma_new = np.zeros((N,numcoeff, numcoeff))\n",
    "        # find aij\n",
    "        denomsum = np.zeros((N))\n",
    "        for i in range(0,N):\n",
    "\n",
    "            for f in range(num_files):\n",
    "                for t in range(len(gammas[f][i])):\n",
    "                    denomsum[i] += gammas[f][i][t]\n",
    "\n",
    "            for j in range(i,i+2):\n",
    "                if j<N:\n",
    "                    numsum = 0\n",
    "                    for f in range(num_files):\n",
    "                        for t in range(len(gammas[f][i])):\n",
    "                            numsum += xis[f][i+j][t]\n",
    "                    A_new[i][j] = numsum/denomsum[i]\n",
    "        #find mui\n",
    "        for i in range(0,N):\n",
    "            numsum = 0\n",
    "            for f in range(num_files):\n",
    "                for t in range(len(sindeptrain[f//5][wordidx][f%5])):\n",
    "                    numsum += (np.array(gammas[f][i][t]) * np.array(sindeptrain[f//5][wordidx][f%5][t]))\n",
    "            mu_new[i] = numsum/denomsum[i]\n",
    "\n",
    "        #find covdi\n",
    "        for i in range(0,N):\n",
    "            numsum = 0\n",
    "            for f in range(num_files):\n",
    "                for t in range(len(sindeptrain[f//5][wordidx][f%5])):\n",
    "                    numsum += (np.array(gammas[f][i][t]) * np.array(np.outer((sindeptrain[f//5][wordidx][f%5][t]-mn[i]),(sindeptrain[f//5][wordidx][f%5][t]-mn[i]))))\n",
    "            sigma_new[i] = numsum/denomsum[i]\n",
    "\n",
    "        A = A_new\n",
    "        mn = mu_new\n",
    "        cov = sigma_new\n",
    "    print(A_new)\n",
    "    print(\"indep hmm completed for word\",wordidx)\n",
    "    return A,mn,cov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SDEP general\n",
    "def SDEPTRAIN(initialTransition,initialProbabilities,initialmeans,initialcov,wordidx,numiterations,numcoeff):\n",
    "    A = np.array(initialTransition)\n",
    "    mn = initialmeans\n",
    "    cov = initialcov\n",
    "    logprobs = np.zeros((16,numiterations))\n",
    "    for iteration in range(numiterations):\n",
    "        # calc params\n",
    "        alphas = []\n",
    "        betas = []\n",
    "        gammas = []\n",
    "        xis = []\n",
    "        for s in range(4):#for each speaker\n",
    "            for u in range(4): # for each utterance\n",
    "                B = calculateBMatrix(sdeptrain[s][wordidx][u],mn,cov,5)\n",
    "                Alpha,Beta,logg = calculateAlphaBetaTildeG(A,B,initialProbabilities)\n",
    "                Gamma = calculateGammaMatrix(Alpha,Beta)\n",
    "                xi = calculateXiMatrix(A,Alpha,Beta,B)\n",
    "                alphas.append(Alpha)\n",
    "                betas.append(Beta)\n",
    "                gammas.append(Gamma)\n",
    "                xis.append(xi)\n",
    "                logprobs[:,iteration] = np.sum(logg)\n",
    "\n",
    "        #upd8 \n",
    "        num_files = len(alphas)\n",
    "        N = len(gammas[0])\n",
    "        A_new = np.zeros((N,N))\n",
    "        mu_new = np.zeros((N,numcoeff))\n",
    "        sigma_new = np.zeros((N,numcoeff, numcoeff))\n",
    "        # find aij\n",
    "        denomsum = np.zeros((N))\n",
    "        for i in range(0,N):\n",
    "\n",
    "            for f in range(num_files):\n",
    "                for t in range(len(gammas[f][i])):\n",
    "                    denomsum[i] += gammas[f][i][t]\n",
    "\n",
    "            for j in range(i,i+2):\n",
    "                if j<N:\n",
    "                    numsum = 0\n",
    "                    for f in range(num_files):\n",
    "                        for t in range(len(gammas[f][i])):\n",
    "                            numsum += xis[f][i+j][t]\n",
    "                    A_new[i][j] = numsum/denomsum[i]\n",
    "        #find mui\n",
    "        for i in range(0,N):\n",
    "\n",
    "            numsum = 0\n",
    "            for f in range(num_files):\n",
    "                for t in range(len(gammas[f][i])):\n",
    "                    numsum += (np.array(gammas[f][i][t]) * np.array(sdeptrain[f//4][wordidx][f%4][t]))\n",
    "            mu_new[i] = numsum/denomsum[i]\n",
    "\n",
    "        #find covdi\n",
    "        for i in range(0,N):\n",
    "\n",
    "            numsum = 0\n",
    "            for f in range(num_files):\n",
    "                for t in range(len(gammas[f][i])):\n",
    "                    numsum += (np.array(gammas[f][i][t]) * np.array(np.outer((sdeptrain[f//4][wordidx][f%4][t]-mn[i]),(sdeptrain[f//4][wordidx][f%4][t]-mn[i]))))\n",
    "            sigma_new[i] = numsum/denomsum[i]\n",
    "\n",
    "        A = A_new\n",
    "        mn = mu_new\n",
    "        cov = sigma_new\n",
    "    print(A_new)\n",
    "    print(\"dep hmm completed for word\",wordidx)\n",
    "    return A,mn,cov\n",
    "\n",
    "def testlogProb(testfile,Atest,meanstest,covtest,initProbs):\n",
    "    At = np.array(Atest)\n",
    "    Btest = calculateBMatrix(testfile,meanstest,covtest,5)\n",
    "    logg = np.array(calculateAlphaBetaTildeG(At,Btest,initProbs)[2])\n",
    "    return np.sum(logg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.88004837 0.11995163 0.         0.         0.        ]\n",
      " [0.         0.91842459 0.08157541 0.         0.        ]\n",
      " [0.         0.         0.89646572 0.10353428 0.        ]\n",
      " [0.         0.         0.         0.92693646 0.07306354]\n",
      " [0.         0.         0.         0.         1.        ]]\n",
      "dep hmm completed for word 0\n",
      "[[0.93941349 0.06058651 0.         0.         0.        ]\n",
      " [0.         0.93207714 0.06792286 0.         0.        ]\n",
      " [0.         0.         0.85274211 0.14725789 0.        ]\n",
      " [0.         0.         0.         0.94678115 0.05321885]\n",
      " [0.         0.         0.         0.         1.        ]]\n",
      "dep hmm completed for word 1\n",
      "[[0.85035388 0.14964612 0.         0.         0.        ]\n",
      " [0.         0.94331581 0.05668419 0.         0.        ]\n",
      " [0.         0.         0.89479096 0.10520904 0.        ]\n",
      " [0.         0.         0.         0.96392057 0.03607943]\n",
      " [0.         0.         0.         0.         1.        ]]\n",
      "dep hmm completed for word 2\n",
      "[[0.82018617 0.17981383 0.         0.         0.        ]\n",
      " [0.         0.94225915 0.05774085 0.         0.        ]\n",
      " [0.         0.         0.77478542 0.22521458 0.        ]\n",
      " [0.         0.         0.         0.84477418 0.15522582]\n",
      " [0.         0.         0.         0.         1.        ]]\n",
      "dep hmm completed for word 3\n",
      "[[0.90084898 0.09915102 0.         0.         0.        ]\n",
      " [0.         0.92296079 0.07703921 0.         0.        ]\n",
      " [0.         0.         0.91058641 0.08941359 0.        ]\n",
      " [0.         0.         0.         0.96567364 0.03432636]\n",
      " [0.         0.         0.         0.         1.        ]]\n",
      "dep hmm completed for word 4\n"
     ]
    }
   ],
   "source": [
    "#TRAINING FOR SPEAKER DEPENDENT\n",
    "asrmean = [sdepasr_mn,sdepasr_mn,sdepasr_mn,sdepasr_mn,sdepasr_mn]\n",
    "asrcova = [sdepasr_cov,sdepasr_cov,sdepasr_cov,sdepasr_cov,sdepasr_cov]\n",
    "cnnmean = [sdepcnn_mn,sdepcnn_mn,sdepcnn_mn,sdepcnn_mn,sdepcnn_mn]\n",
    "cnncova = [sdepcnn_cov,sdepcnn_cov,sdepcnn_cov,sdepcnn_cov,sdepcnn_cov]\n",
    "dnnmean = [sdepdnn_mn,sdepdnn_mn,sdepdnn_mn,sdepdnn_mn,sdepdnn_mn]\n",
    "dnncova = [sdepdnn_cov,sdepdnn_cov,sdepdnn_cov,sdepdnn_cov,sdepdnn_cov]\n",
    "hmmmean = [sdephmm_mn,sdephmm_mn,sdephmm_mn,sdephmm_mn,sdephmm_mn]\n",
    "hmmcova = [sdephmm_cov,sdephmm_cov,sdephmm_cov,sdephmm_cov,sdephmm_cov]\n",
    "ttsmean = [sdeptts_mn,sdeptts_mn,sdeptts_mn,sdeptts_mn,sdeptts_mn]\n",
    "ttscova = [sdeptts_cov,sdeptts_cov,sdeptts_cov,sdeptts_cov,sdeptts_cov]\n",
    "A_asr,mn_asr,cov_asr = SDEPTRAIN(initialTransition,initialProbabilities,asrmean,asrcova,0,25,14)\n",
    "A_cnn,mn_cnn,cov_cnn = SDEPTRAIN(initialTransition,initialProbabilities,cnnmean,cnncova,1,25,14)\n",
    "A_dnn,mn_dnn,cov_dnn = SDEPTRAIN(initialTransition,initialProbabilities,dnnmean,dnncova,2,25,14)\n",
    "A_hmm,mn_hmm,cov_hmm = SDEPTRAIN(initialTransition,initialProbabilities,hmmmean,hmmcova,3,25,14)\n",
    "A_tts,mn_tts,cov_tts = SDEPTRAIN(initialTransition,initialProbabilities,ttsmean,ttscova,4,25,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kalinuxmsi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:67: RuntimeWarning: overflow encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.   0.   0.   0.   0.  ]\n",
      " [0.   1.   0.   0.   0.  ]\n",
      " [0.   0.25 0.75 0.   0.  ]\n",
      " [0.   0.   0.   1.   0.  ]\n",
      " [0.   0.   0.   0.   1.  ]]\n",
      "0.95\n"
     ]
    }
   ],
   "source": [
    "#SPEAKER DEPENDENT TEST\n",
    "#sdeptest[speaker][word]\n",
    "classifications = np.zeros((5,5))\n",
    "#classifications[truth][prediction] where idx{0,1,2,3,4} => {'asr','cnn','dnn','hmm','tts'}\n",
    "for i in range(4):\n",
    "    for j in range(5):\n",
    "        file = sdeptest[i][j]\n",
    "        preds = [testlogProb(file,A_asr,mn_asr,cov_asr,initialProbabilities),\n",
    "                 testlogProb(file,A_cnn,mn_cnn,cov_cnn,initialProbabilities),\n",
    "                 testlogProb(file,A_dnn,mn_dnn,cov_dnn,initialProbabilities),\n",
    "                 testlogProb(file,A_hmm,mn_hmm,cov_hmm,initialProbabilities),\n",
    "                 testlogProb(file,A_tts,mn_tts,cov_tts,initialProbabilities)]\n",
    "        classifications[j][np.argmax(preds)] += 1\n",
    "\n",
    "print(classifications/4)\n",
    "print(np.mean(np.diag(classifications/4)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.87918047 0.12081953 0.         0.         0.        ]\n",
      " [0.         0.90683834 0.09316166 0.         0.        ]\n",
      " [0.         0.         0.59904418 0.40095582 0.        ]\n",
      " [0.         0.         0.         0.94039935 0.05960065]\n",
      " [0.         0.         0.         0.         1.        ]]\n",
      "indep hmm completed for word 0\n",
      "[[0.94716842 0.05283158 0.         0.         0.        ]\n",
      " [0.         0.4392236  0.5607764  0.         0.        ]\n",
      " [0.         0.         0.911113   0.088887   0.        ]\n",
      " [0.         0.         0.         0.9346353  0.0653647 ]\n",
      " [0.         0.         0.         0.         1.        ]]\n",
      "indep hmm completed for word 1\n",
      "[[0.84408361 0.15591639 0.         0.         0.        ]\n",
      " [0.         0.9095073  0.0904927  0.         0.        ]\n",
      " [0.         0.         0.88727656 0.11272344 0.        ]\n",
      " [0.         0.         0.         0.97147538 0.02852462]\n",
      " [0.         0.         0.         0.         1.        ]]\n",
      "indep hmm completed for word 2\n",
      "[[0.80265743 0.19734257 0.         0.         0.        ]\n",
      " [0.         0.8887434  0.1112566  0.         0.        ]\n",
      " [0.         0.         0.84142455 0.15857545 0.        ]\n",
      " [0.         0.         0.         0.8930553  0.1069447 ]\n",
      " [0.         0.         0.         0.         1.        ]]\n",
      "indep hmm completed for word 3\n",
      "[[0.89271014 0.10728986 0.         0.         0.        ]\n",
      " [0.         0.89167764 0.10832236 0.         0.        ]\n",
      " [0.         0.         0.79806634 0.20193366 0.        ]\n",
      " [0.         0.         0.         0.96796242 0.03203758]\n",
      " [0.         0.         0.         0.         1.        ]]\n",
      "indep hmm completed for word 4\n"
     ]
    }
   ],
   "source": [
    "#SPEAKER INDEPENDENT TRAIN\n",
    "asrmeani = [sdepasri_mn,sdepasri_mn,sdepasri_mn,sdepasri_mn,sdepasri_mn]\n",
    "asrcovai = [sdepasri_cov,sdepasri_cov,sdepasri_cov,sdepasri_cov,sdepasri_cov]\n",
    "cnnmeani = [sdepcnni_mn,sdepcnni_mn,sdepcnni_mn,sdepcnni_mn,sdepcnni_mn]\n",
    "cnncovai = [sdepcnni_cov,sdepcnni_cov,sdepcnni_cov,sdepcnni_cov,sdepcnni_cov]\n",
    "dnnmeani = [sdepdnni_mn,sdepdnni_mn,sdepdnni_mn,sdepdnni_mn,sdepdnni_mn]\n",
    "dnncovai = [sdepdnni_cov,sdepdnni_cov,sdepdnni_cov,sdepdnni_cov,sdepdnni_cov]\n",
    "hmmmeani = [sdephmmi_mn,sdephmmi_mn,sdephmmi_mn,sdephmmi_mn,sdephmmi_mn]\n",
    "hmmcovai = [sdephmmi_cov,sdephmmi_cov,sdephmmi_cov,sdephmmi_cov,sdephmmi_cov]\n",
    "ttsmeani = [sdepttsi_mn,sdepttsi_mn,sdepttsi_mn,sdepttsi_mn,sdepttsi_mn]\n",
    "ttscovai = [sdepttsi_cov,sdepttsi_cov,sdepttsi_cov,sdepttsi_cov,sdepttsi_cov]\n",
    "A_asri,mn_asri,cov_asri = SINDEPTRAIN(initialTransition,initialProbabilities,asrmeani,asrcovai,0,25,14)\n",
    "A_cnni,mn_cnni,cov_cnni = SINDEPTRAIN(initialTransition,initialProbabilities,cnnmeani,cnncovai,1,25,14)\n",
    "A_dnni,mn_dnni,cov_dnni = SINDEPTRAIN(initialTransition,initialProbabilities,dnnmeani,dnncovai,2,25,14)\n",
    "A_hmmi,mn_hmmi,cov_hmmi = SINDEPTRAIN(initialTransition,initialProbabilities,hmmmeani,hmmcovai,3,25,14)\n",
    "A_ttsi,mn_ttsi,cov_ttsi = SINDEPTRAIN(initialTransition,initialProbabilities,ttsmeani,ttscovai,4,25,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kalinuxmsi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:67: RuntimeWarning: overflow encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.  0.  0.  0.  0. ]\n",
      " [0.6 0.2 0.2 0.  0. ]\n",
      " [0.2 0.  0.8 0.  0. ]\n",
      " [0.  1.  0.  0.  0. ]\n",
      " [0.2 0.  0.  0.  0.8]]\n",
      "0.5599999999999999\n"
     ]
    }
   ],
   "source": [
    "#SPEAKER INDEPENDENT TEST\n",
    "#test[word][utt]\n",
    "classificationsi = np.zeros((5,5))\n",
    "#classifications[truth][prediction] where idx{0,1,2,3,4} => {'asr','cnn','dnn','hmm','tts'}\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        filei = sindeptest[i][j]\n",
    "        predsi = [testlogProb(filei,A_asri,mn_asri,cov_asri,initialProbabilities),\n",
    "                 testlogProb(filei,A_cnni,mn_cnni,cov_cnni,initialProbabilities),\n",
    "                 testlogProb(filei,A_dnni,mn_dnni,cov_dnni,initialProbabilities),\n",
    "                 testlogProb(filei,A_hmmi,mn_hmmi,cov_hmmi,initialProbabilities),\n",
    "                 testlogProb(filei,A_ttsi,mn_ttsi,cov_ttsi,initialProbabilities)]\n",
    "        classificationsi[i][np.argmax(predsi)] += 1\n",
    "\n",
    "print(classificationsi/5)\n",
    "print(np.mean(np.diag(classificationsi/5)))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on Recorded Voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kalinuxmsi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:54: RuntimeWarning: divide by zero encountered in log\n",
      "/home/kalinuxmsi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:55: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan, nan, nan, nan, nan]\n",
      "[nan, nan, nan, nan, nan]\n",
      "[nan, nan, nan, nan, nan]\n",
      "[nan, nan, nan, nan, nan]\n",
      "[nan, nan, nan, nan, nan]\n",
      "[nan, nan, nan, nan, nan]\n",
      "[nan, nan, nan, nan, nan]\n",
      "[nan, nan, nan, nan, nan]\n",
      "[nan, nan, nan, nan, nan]\n",
      "[nan, nan, nan, nan, nan]\n",
      "[nan, nan, nan, nan, nan]\n",
      "[nan, nan, nan, nan, nan]\n",
      "[nan, nan, nan, nan, nan]\n",
      "[nan, nan, nan, nan, nan]\n",
      "[nan, nan, nan, nan, nan]\n",
      "[nan, nan, nan, nan, nan]\n",
      "[nan, nan, nan, nan, nan]\n",
      "[nan, nan, nan, nan, nan]\n",
      "[nan, nan, nan, nan, nan]\n",
      "[nan, nan, nan, nan, nan]\n",
      "[nan, nan, nan, nan, nan]\n",
      "[nan, nan, nan, nan, nan]\n",
      "[nan, nan, nan, nan, nan]\n",
      "[nan, nan, nan, nan, nan]\n",
      "[nan, nan, nan, nan, nan]\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#access voicetest[word][utt]\n",
    "voicetest = [[[] for i in range(5)] for j in range(5)]\n",
    "p = './feature/self_recf'\n",
    "wordList = ['asr','cnn','dnn','hmm','tts']\n",
    "\n",
    "speakertestpath = p\n",
    "wnum = 0\n",
    "for w in wordList:\n",
    "    wordtestpath = speakertestpath +'/'+w\n",
    "    for utt in range(1,6):\n",
    "        ftestpath = wordtestpath + str(utt)+'.fea'\n",
    "        fitest = open(ftestpath,'r')\n",
    "        dataitest = fitest.readlines()\n",
    "        dataitest = [i.split(',') for i in dataitest]\n",
    "        for sample in range(len(dataitest)):\n",
    "            dataitest[sample] = [float(i) for i in dataitest[sample]]\n",
    "        for d in dataitest:\n",
    "            voicetest[wnum][utt-1].append(d)\n",
    "    wnum += 1\n",
    "\n",
    "voicetest = np.array(voicetest)\n",
    "#SPEAKER INDEPENDENT TEST\n",
    "#test[word][utt]\n",
    "classificationsvoice = np.zeros((5,5))\n",
    "#classifications[truth][prediction] where idx{0,1,2,3,4} => {'asr','cnn','dnn','hmm','tts'}\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        filei = voicetest[i][j]\n",
    "        predsi = [testlogProb(filei,A_asri,mn_asri,cov_asri,initialProbabilities),\n",
    "                 testlogProb(filei,A_cnni,mn_cnni,cov_cnni,initialProbabilities),\n",
    "                 testlogProb(filei,A_dnni,mn_dnni,cov_dnni,initialProbabilities),\n",
    "                 testlogProb(filei,A_hmmi,mn_hmmi,cov_hmmi,initialProbabilities),\n",
    "                 testlogProb(filei,A_ttsi,mn_ttsi,cov_ttsi,initialProbabilities)]\n",
    "        classificationsvoice[i][np.argmax(predsi)] += 1\n",
    "        print(predsi)\n",
    "\n",
    "print(classificationsvoice/5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction for EC\n",
    "For the extra credit we have opted to use the Mel Frequency Spectral Coefficients. Since the MFCC comes from taking the IDFT of the of the MFSC, we obtained the MFSC by taking the fft of the MFCC features given to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.23225914e+01  5.29382403e+01  3.75032229e+01  2.89647140e+01\n",
      "   1.62596657e+01  1.20291698e+00 -3.31961531e+00 -7.55295367e-01\n",
      "  -3.31961531e+00  1.20291698e+00  1.62596657e+01  2.89647140e+01\n",
      "   3.75032229e+01  5.29382403e+01]\n",
      " [ 5.29382403e+01  5.76293407e+01  4.07127936e+01  3.11544386e+01\n",
      "   1.67555639e+01  1.67746326e-02 -5.51450544e+00 -3.59749565e+00\n",
      "  -5.51450544e+00  1.67746326e-02  1.67555639e+01  3.11544386e+01\n",
      "   4.07127936e+01  5.73293407e+01]\n",
      " [ 3.75032229e+01  4.07127936e+01  3.15972367e+01  2.35589200e+01\n",
      "   1.17420024e+01  2.51295450e-01 -4.82895211e+00 -4.62417816e+00\n",
      "  -4.82895211e+00  2.51295450e-01  1.17420024e+01  2.35589200e+01\n",
      "   3.12972367e+01  4.07127936e+01]\n",
      " [ 2.89647140e+01  3.11544386e+01  2.35589200e+01  2.11084869e+01\n",
      "   1.10135660e+01  1.34635807e+00 -4.64283287e+00 -4.55673067e+00\n",
      "  -4.64283287e+00  1.34635807e+00  1.10135660e+01  2.08084869e+01\n",
      "   2.35589200e+01  3.11544386e+01]\n",
      " [ 1.62596657e+01  1.67555639e+01  1.17420024e+01  1.10135660e+01\n",
      "   1.15233016e+01  2.03257326e+00 -3.45869748e+00 -3.98063531e+00\n",
      "  -3.45869748e+00  2.03257326e+00  1.12233016e+01  1.10135660e+01\n",
      "   1.17420024e+01  1.67555639e+01]\n",
      " [ 1.20291698e+00  1.67746326e-02  2.51295450e-01  1.34635807e+00\n",
      "   2.03257326e+00  4.48881302e+00  1.09929654e+00 -6.62602778e-01\n",
      "   1.09929654e+00  4.18881302e+00  2.03257326e+00  1.34635807e+00\n",
      "   2.51295450e-01  1.67746326e-02]\n",
      " [-3.31961531e+00 -5.51450544e+00 -4.82895211e+00 -4.64283287e+00\n",
      "  -3.45869748e+00  1.09929654e+00  5.82325379e+00  5.51003117e+00\n",
      "   5.52325379e+00  1.09929654e+00 -3.45869748e+00 -4.64283287e+00\n",
      "  -4.82895211e+00 -5.51450544e+00]\n",
      " [-7.55295367e-01 -3.59749565e+00 -4.62417816e+00 -4.55673067e+00\n",
      "  -3.98063531e+00 -6.62602778e-01  5.51003117e+00  1.11948528e+01\n",
      "   5.51003117e+00 -6.62602778e-01 -3.98063531e+00 -4.55673067e+00\n",
      "  -4.62417816e+00 -3.59749565e+00]\n",
      " [-3.31961531e+00 -5.51450544e+00 -4.82895211e+00 -4.64283287e+00\n",
      "  -3.45869748e+00  1.09929654e+00  5.52325379e+00  5.51003117e+00\n",
      "   5.82325379e+00  1.09929654e+00 -3.45869748e+00 -4.64283287e+00\n",
      "  -4.82895211e+00 -5.51450544e+00]\n",
      " [ 1.20291698e+00  1.67746326e-02  2.51295450e-01  1.34635807e+00\n",
      "   2.03257326e+00  4.18881302e+00  1.09929654e+00 -6.62602778e-01\n",
      "   1.09929654e+00  4.48881302e+00  2.03257326e+00  1.34635807e+00\n",
      "   2.51295450e-01  1.67746326e-02]\n",
      " [ 1.62596657e+01  1.67555639e+01  1.17420024e+01  1.10135660e+01\n",
      "   1.12233016e+01  2.03257326e+00 -3.45869748e+00 -3.98063531e+00\n",
      "  -3.45869748e+00  2.03257326e+00  1.15233016e+01  1.10135660e+01\n",
      "   1.17420024e+01  1.67555639e+01]\n",
      " [ 2.89647140e+01  3.11544386e+01  2.35589200e+01  2.08084869e+01\n",
      "   1.10135660e+01  1.34635807e+00 -4.64283287e+00 -4.55673067e+00\n",
      "  -4.64283287e+00  1.34635807e+00  1.10135660e+01  2.11084869e+01\n",
      "   2.35589200e+01  3.11544386e+01]\n",
      " [ 3.75032229e+01  4.07127936e+01  3.12972367e+01  2.35589200e+01\n",
      "   1.17420024e+01  2.51295450e-01 -4.82895211e+00 -4.62417816e+00\n",
      "  -4.82895211e+00  2.51295450e-01  1.17420024e+01  2.35589200e+01\n",
      "   3.15972367e+01  4.07127936e+01]\n",
      " [ 5.29382403e+01  5.73293407e+01  4.07127936e+01  3.11544386e+01\n",
      "   1.67555639e+01  1.67746326e-02 -5.51450544e+00 -3.59749565e+00\n",
      "  -5.51450544e+00  1.67746326e-02  1.67555639e+01  3.11544386e+01\n",
      "   4.07127936e+01  5.76293407e+01]]\n"
     ]
    }
   ],
   "source": [
    "#speaker dependent partitions\n",
    "#sdeptrain dimensions sdeptrain[speaker][word][utterance idx] with idxs given alphabetically\n",
    "ec_sdeptrain = [[[[] for i in range(4)] for j in range(5)] for k in range(4)]\n",
    "#ec_sdeptest dimensions sdeptrain[speaker][word] with idxs given alphabetically\n",
    "ec_sdeptest = [[[] for i in range(5)] for j in range(4)]\n",
    "p = './feature'\n",
    "speakerListec = ['dg', 'ls', 'mh', 'yx']\n",
    "wordListec = ['asr','cnn','dnn','hmm','tts']\n",
    "snum = 0\n",
    "for s in speakerListec:\n",
    "    speakerpathec = p+'/'+s\n",
    "    wnum = 0\n",
    "    for w in wordListec:\n",
    "        wordpathec = speakerpathec +'/'+s+'_'+w\n",
    "        #set up train\n",
    "        for utt in range(1,5):\n",
    "            fpathec = wordpathec + str(utt)+'.fea'\n",
    "            fec = open(fpathec,'r')\n",
    "            data = fec.readlines()\n",
    "            data = [i.split(',') for i in data]\n",
    "            for sample in range(len(data)):\n",
    "                data[sample] = [float(i) for i in data[sample]]\n",
    "            for d in data:\n",
    "                ec_sdeptrain[snum][wnum][utt-1].append(np.real(np.fft.fft(d)))\n",
    "        #set up test\n",
    "        fpath2ec = wordpathec + str(5)+'.fea'\n",
    "        f2ec = open(fpath2ec,'r')\n",
    "        dataec = f2ec.readlines()\n",
    "        dataec = [i.split(',') for i in dataec]\n",
    "        for d in dataec:\n",
    "            ec_sdeptest[snum][wnum].append(np.real(np.fft.fft(np.array(d).astype(np.float))))\n",
    "        wnum += 1\n",
    "    snum += 1\n",
    "ec_sdeptrain = np.array(ec_sdeptrain)\n",
    "\n",
    "\n",
    "ec_bigasr = []\n",
    "ec_bigcnn = []\n",
    "ec_bigdnn = []\n",
    "ec_bighmm = []\n",
    "ec_bigtts = []\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        ec_bigasr.extend(ec_sdeptrain[i][0][j])\n",
    "        ec_bigcnn.extend(ec_sdeptrain[i][1][j])\n",
    "        ec_bigdnn.extend(ec_sdeptrain[i][2][j])\n",
    "        ec_bighmm.extend(ec_sdeptrain[i][3][j])\n",
    "        ec_bigtts.extend(ec_sdeptrain[i][4][j])\n",
    "ec_sdepasr_cov = (np.cov(np.array(ec_bigasr).T)+0.3*np.identity(14))\n",
    "ec_sdepcnn_cov = (np.cov(np.array(ec_bigcnn).T)+0.3*np.identity(14))\n",
    "ec_sdepdnn_cov = (np.cov(np.array(ec_bigdnn).T)+0.3*np.identity(14))\n",
    "ec_sdephmm_cov = (np.cov(np.array(ec_bighmm).T)+0.3*np.identity(14))\n",
    "ec_sdeptts_cov = (np.cov(np.array(ec_bigtts).T)+0.3*np.identity(14))\n",
    "ec_sdepasr_mn = np.mean(ec_bigasr,axis=0)\n",
    "ec_sdepcnn_mn = np.mean(ec_bigcnn,axis=0)\n",
    "ec_sdepdnn_mn = np.mean(ec_bigdnn,axis=0)\n",
    "ec_sdephmm_mn = np.mean(ec_bighmm,axis=0)\n",
    "ec_sdeptts_mn = np.mean(ec_bigtts,axis=0)\n",
    "print(ec_sdepasr_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.95199724 0.04800276 0.         0.         0.        ]\n",
      " [0.         0.66372964 0.33627036 0.         0.        ]\n",
      " [0.         0.         0.89571994 0.10428006 0.        ]\n",
      " [0.         0.         0.         0.92853525 0.07146475]\n",
      " [0.         0.         0.         0.         1.        ]]\n",
      "dep hmm completed for word 0\n",
      "[[0.87241152 0.12758848 0.         0.         0.        ]\n",
      " [0.         0.93802442 0.06197558 0.         0.        ]\n",
      " [0.         0.         0.89885313 0.10114687 0.        ]\n",
      " [0.         0.         0.         0.94933469 0.05066531]\n",
      " [0.         0.         0.         0.         1.        ]]\n",
      "dep hmm completed for word 1\n",
      "[[0.84454446 0.15545554 0.         0.         0.        ]\n",
      " [0.         0.94623538 0.05376462 0.         0.        ]\n",
      " [0.         0.         0.88627527 0.11372473 0.        ]\n",
      " [0.         0.         0.         0.96392437 0.03607563]\n",
      " [0.         0.         0.         0.         1.        ]]\n",
      "dep hmm completed for word 2\n",
      "[[0.91130775 0.08869225 0.         0.         0.        ]\n",
      " [0.         0.83570507 0.16429493 0.         0.        ]\n",
      " [0.         0.         0.88025493 0.11974507 0.        ]\n",
      " [0.         0.         0.         0.84784838 0.15215162]\n",
      " [0.         0.         0.         0.         1.        ]]\n",
      "dep hmm completed for word 3\n",
      "[[0.96381955 0.03618045 0.         0.         0.        ]\n",
      " [0.         0.92668673 0.07331327 0.         0.        ]\n",
      " [0.         0.         0.6832752  0.3167248  0.        ]\n",
      " [0.         0.         0.         0.96582085 0.03417915]\n",
      " [0.         0.         0.         0.         1.        ]]\n",
      "dep hmm completed for word 4\n"
     ]
    }
   ],
   "source": [
    "#TRAINING FOR SPEAKER DEPENDENT FOR EXTRA CREDIT\n",
    "ec_asrmean = [ec_sdepasr_mn,ec_sdepasr_mn,ec_sdepasr_mn,ec_sdepasr_mn,ec_sdepasr_mn]\n",
    "ec_asrcova = [ec_sdepasr_cov,ec_sdepasr_cov,ec_sdepasr_cov,ec_sdepasr_cov,ec_sdepasr_cov]\n",
    "ec_cnnmean = [ec_sdepcnn_mn,ec_sdepcnn_mn,ec_sdepcnn_mn,ec_sdepcnn_mn,ec_sdepcnn_mn]\n",
    "ec_cnncova = [ec_sdepcnn_cov,ec_sdepcnn_cov,ec_sdepcnn_cov,ec_sdepcnn_cov,ec_sdepcnn_cov]\n",
    "ec_dnnmean = [ec_sdepdnn_mn,ec_sdepdnn_mn,ec_sdepdnn_mn,ec_sdepdnn_mn,ec_sdepdnn_mn]\n",
    "ec_dnncova = [ec_sdepdnn_cov,ec_sdepdnn_cov,ec_sdepdnn_cov,ec_sdepdnn_cov,ec_sdepdnn_cov]\n",
    "ec_hmmmean = [ec_sdephmm_mn,ec_sdephmm_mn,ec_sdephmm_mn,ec_sdephmm_mn,ec_sdephmm_mn]\n",
    "ec_hmmcova = [ec_sdephmm_cov,ec_sdephmm_cov,ec_sdephmm_cov,ec_sdephmm_cov,ec_sdephmm_cov]\n",
    "ec_ttsmean = [ec_sdeptts_mn,ec_sdeptts_mn,ec_sdeptts_mn,ec_sdeptts_mn,ec_sdeptts_mn]\n",
    "ec_ttscova = [ec_sdeptts_cov,ec_sdeptts_cov,ec_sdeptts_cov,ec_sdeptts_cov,ec_sdeptts_cov]\n",
    "ec_A_asr,ec_mn_asr,ec_cov_asr = SDEPTRAIN(initialTransition,initialProbabilities,ec_asrmean,ec_asrcova,0,25,14)\n",
    "ec_A_cnn,ec_mn_cnn,ec_cov_cnn = SDEPTRAIN(initialTransition,initialProbabilities,ec_cnnmean,ec_cnncova,1,25,14)\n",
    "ec_A_dnn,ec_mn_dnn,ec_cov_dnn = SDEPTRAIN(initialTransition,initialProbabilities,ec_dnnmean,ec_dnncova,2,25,14)\n",
    "ec_A_hmm,ec_mn_hmm,ec_cov_hmm = SDEPTRAIN(initialTransition,initialProbabilities,ec_hmmmean,ec_hmmcova,3,25,14)\n",
    "ec_A_tts,ec_mn_tts,ec_cov_tts = SDEPTRAIN(initialTransition,initialProbabilities,ec_ttsmean,ec_ttscova,4,25,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kalinuxmsi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:54: RuntimeWarning: divide by zero encountered in log\n",
      "/home/kalinuxmsi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:55: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan, nan, nan, nan, nan]\n",
      "[nan, nan, nan, nan, nan]\n",
      "[nan, nan, nan, nan, nan]\n",
      "[nan, nan, nan, nan, nan]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of bounds for axis 0 with size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-9a040e1361e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mec_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mec_sdeptrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         ec_preds = [testlogProb(ec_file,ec_A_asr,ec_mn_asr,ec_cov_asr,initialProbabilities),\n\u001b[1;32m      8\u001b[0m                  \u001b[0mtestlogProb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mec_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mec_A_cnn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mec_mn_cnn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mec_cov_cnn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitialProbabilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 4 is out of bounds for axis 0 with size 4"
     ]
    }
   ],
   "source": [
    "#SPEAKER DEPENDENT TEST FOR EXTRA CREDIT\n",
    "ec_classifications = np.zeros((5,5))\n",
    "#classifications[truth][prediction] where idx{0,1,2,3,4} => {'asr','cnn','dnn','hmm','tts'}\n",
    "for i in range(4):\n",
    "    for j in range(5):\n",
    "        ec_file = ec_sdeptrain[0][i][j]\n",
    "        ec_preds = [testlogProb(ec_file,ec_A_asr,ec_mn_asr,ec_cov_asr,initialProbabilities),\n",
    "                 testlogProb(ec_file,ec_A_cnn,ec_mn_cnn,ec_cov_cnn,initialProbabilities),\n",
    "                 testlogProb(ec_file,ec_A_dnn,ec_mn_dnn,ec_cov_dnn,initialProbabilities),\n",
    "                 testlogProb(ec_file,ec_A_hmm,ec_mn_hmm,ec_cov_hmm,initialProbabilities),\n",
    "                 testlogProb(ec_file,ec_A_tts,ec_mn_tts,ec_cov_tts,initialProbabilities)]\n",
    "        print(ec_preds)\n",
    "        ec_classifications[j][np.argmax(ec_preds)] += 1\n",
    "\n",
    "print(ec_classifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
