{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction\n",
    "train will have the training samples and test will have the test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.04048341e+01  1.27636270e+01  2.43505882e-02 -9.68879574e-02\n",
      "   6.62948182e-01 -4.48165841e-01 -5.90144298e-01 -4.66678538e-01\n",
      "  -8.21344077e-02 -1.66746807e-01  1.62683654e-01  2.05282523e-02\n",
      "   2.98959372e-01  2.53225667e-01]\n",
      " [ 1.27636270e+01  1.87229059e+01 -7.55863487e-01 -7.56105813e-01\n",
      "   8.98603059e-01 -7.48330443e-01 -7.91945381e-01 -6.17434456e-01\n",
      "  -3.50967201e-01 -3.93730449e-01 -7.84939754e-02  6.66048585e-02\n",
      "   3.80946887e-01  3.67717591e-01]\n",
      " [ 2.43505882e-02 -7.55863487e-01  2.43649242e+00 -8.18914427e-01\n",
      "   8.31471700e-02 -2.26633800e-03 -5.55522437e-02 -1.32992328e-02\n",
      "   9.70252900e-02  1.23088858e-01  1.04846005e-01  2.44834159e-02\n",
      "   8.68224494e-02 -5.60005611e-02]\n",
      " [-9.68879574e-02 -7.56105813e-01 -8.18914427e-01  1.00768454e+00\n",
      "  -5.12910447e-02 -4.05244669e-02  3.36360552e-02  8.31173113e-02\n",
      "   1.97579194e-02 -7.15650203e-02  6.20710968e-02 -8.21601356e-03\n",
      "  -1.73139381e-02  1.65536484e-03]\n",
      " [ 6.62948182e-01  8.98603059e-01  8.31471700e-02 -5.12910447e-02\n",
      "   8.10812017e-01 -2.15046643e-01 -1.19589678e-01 -9.68667317e-03\n",
      "  -7.96023612e-03 -1.18897545e-01 -7.87605540e-02  5.24871812e-02\n",
      "  -3.10348989e-02  2.75848044e-02]\n",
      " [-4.48165841e-01 -7.48330443e-01 -2.26633800e-03 -4.05244669e-02\n",
      "  -2.15046643e-01  2.78982712e-01  4.05350812e-02 -3.39545787e-02\n",
      "   2.70736086e-02  9.17557045e-02 -2.20736657e-02 -3.73203821e-02\n",
      "  -2.50442392e-02 -2.45513609e-02]\n",
      " [-5.90144298e-01 -7.91945381e-01 -5.55522437e-02  3.36360552e-02\n",
      "  -1.19589678e-01  4.05350812e-02  2.20852756e-01 -2.63896859e-02\n",
      "  -1.58672368e-02  4.42143750e-02  2.55947663e-02 -1.68867057e-02\n",
      "  -7.16288098e-02  1.53074656e-02]\n",
      " [-4.66678538e-01 -6.17434456e-01 -1.32992328e-02  8.31173113e-02\n",
      "  -9.68667317e-03 -3.39545787e-02 -2.63896859e-02  1.93944982e-01\n",
      "   2.82789556e-02 -3.40025932e-02  3.26698769e-03  1.02593629e-02\n",
      "   2.90309098e-02 -4.26891181e-02]\n",
      " [-8.21344077e-02 -3.50967201e-01  9.70252900e-02  1.97579194e-02\n",
      "  -7.96023612e-03  2.70736086e-02 -1.58672368e-02  2.82789556e-02\n",
      "   1.36703919e-01  2.48972935e-02  2.82720586e-02 -1.61039383e-02\n",
      "   1.43605451e-02 -2.45233469e-02]\n",
      " [-1.66746807e-01 -3.93730449e-01  1.23088858e-01 -7.15650203e-02\n",
      "  -1.18897545e-01  9.17557045e-02  4.42143750e-02 -3.40025932e-02\n",
      "   2.48972935e-02  1.37492712e-01  2.12154960e-02 -1.21591147e-02\n",
      "  -1.16572697e-02  5.60209344e-03]\n",
      " [ 1.62683654e-01 -7.84939754e-02  1.04846005e-01  6.20710968e-02\n",
      "  -7.87605540e-02 -2.20736657e-02  2.55947663e-02  3.26698769e-03\n",
      "   2.82720586e-02  2.12154960e-02  1.44832485e-01 -2.17568890e-02\n",
      "   7.37554007e-03 -5.65937023e-04]\n",
      " [ 2.05282523e-02  6.66048585e-02  2.44834159e-02 -8.21601356e-03\n",
      "   5.24871812e-02 -3.73203821e-02 -1.68867057e-02  1.02593629e-02\n",
      "  -1.61039383e-02 -1.21591147e-02 -2.17568890e-02  7.57073365e-02\n",
      "   2.00962020e-02  3.91007687e-03]\n",
      " [ 2.98959372e-01  3.80946887e-01  8.68224494e-02 -1.73139381e-02\n",
      "  -3.10348989e-02 -2.50442392e-02 -7.16288098e-02  2.90309098e-02\n",
      "   1.43605451e-02 -1.16572697e-02  7.37554007e-03  2.00962020e-02\n",
      "   1.09759211e-01 -8.55470142e-03]\n",
      " [ 2.53225667e-01  3.67717591e-01 -5.60005611e-02  1.65536484e-03\n",
      "   2.75848044e-02 -2.45513609e-02  1.53074656e-02 -4.26891181e-02\n",
      "  -2.45233469e-02  5.60209344e-03 -5.65937023e-04  3.91007687e-03\n",
      "  -8.55470142e-03  7.02134753e-02]]\n"
     ]
    }
   ],
   "source": [
    "#speaker dependent partitions\n",
    "#sdeptrain dimensions sdeptrain[speaker][word][utterance idx] with idxs given alphabetically\n",
    "sdeptrain = [[[[] for i in range(4)] for j in range(5)] for k in range(4)]\n",
    "#sdeptest dimensions sdeptrain[speaker][word] with idxs given alphabetically\n",
    "sdeptest = [[[] for i in range(5)] for j in range(4)]\n",
    "p = './feature'\n",
    "speakerList = ['dg', 'ls', 'mh', 'yx']\n",
    "wordList = ['asr','cnn','dnn','hmm','tts']\n",
    "snum = 0\n",
    "for s in speakerList:\n",
    "    speakerpath = p+'/'+s\n",
    "    wnum = 0\n",
    "    for w in wordList:\n",
    "        wordpath = speakerpath +'/'+s+'_'+w\n",
    "        #set up train\n",
    "        for utt in range(1,5):\n",
    "            fpath = wordpath + str(utt)+'.fea'\n",
    "            f = open(fpath,'r')\n",
    "            data = f.readlines()\n",
    "            data = [i.split(',') for i in data]\n",
    "            for sample in range(len(data)):\n",
    "                data[sample] = [float(i) for i in data[sample]]\n",
    "            for d in data:\n",
    "                sdeptrain[snum][wnum][utt-1].append(d)\n",
    "        #set up test\n",
    "        fpath2 = wordpath + str(5)+'.fea'\n",
    "        f2 = open(fpath2,'r')\n",
    "        data = f2.readlines()\n",
    "        data = [i.split(',') for i in data]\n",
    "        for d in data:\n",
    "            sdeptest[snum][wnum].append(d)\n",
    "        wnum += 1\n",
    "    snum += 1\n",
    "sdeptrain = np.array(sdeptrain)\n",
    "\n",
    "sdepasr_mn = np.mean([[np.mean(np.array(sdeptrain[i][0][j]),axis=0)] for i in range(4) for j in range(4)],axis=0)[0]\n",
    "sdepcnn_mn = np.mean([[np.mean(np.array(sdeptrain[i][1][j]),axis=0)] for i in range(4) for j in range(4)],axis=0)[0]\n",
    "sdepdnn_mn = np.mean([[np.mean(np.array(sdeptrain[i][2][j]),axis=0)] for i in range(4) for j in range(4)],axis=0)[0]\n",
    "sdephmm_mn = np.mean([[np.mean(np.array(sdeptrain[i][3][j]),axis=0)] for i in range(4) for j in range(4)],axis=0)[0]\n",
    "sdeptts_mn = np.mean([[np.mean(np.array(sdeptrain[i][4][j]),axis=0)] for i in range(4) for j in range(4)],axis=0)[0]\n",
    "bigasr = []\n",
    "bigcnn = []\n",
    "bigdnn = []\n",
    "bighmm = []\n",
    "bigtts = []\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        bigasr.extend(sdeptrain[i][0][j])\n",
    "        bigcnn.extend(sdeptrain[i][1][j])\n",
    "        bigdnn.extend(sdeptrain[i][2][j])\n",
    "        bighmm.extend(sdeptrain[i][3][j])\n",
    "        bigtts.extend(sdeptrain[i][4][j])\n",
    "sdepasr_cov = np.cov(np.array(bigasr).T)\n",
    "sdepcnn_cov = np.cov(np.array(bigcnn).T)\n",
    "sdepdnn_cov = np.cov(np.array(bigdnn).T)\n",
    "sdephmm_cov = np.cov(np.array(bighmm).T)\n",
    "sdeptts_cov = np.cov(np.array(bigtts).T)\n",
    "print(sdepasr_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sdeptrain[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "#speaker independent partitions\n",
    "#Extract feature vectors\n",
    "p = './feature'\n",
    "#test separetes each utterance of each word of each speaker\n",
    "test = []\n",
    "train = []\n",
    "#test_big chunks all utterances of each word for each speaker\n",
    "test_big = []\n",
    "train_big = []\n",
    "dirList = ['dg', 'ls', 'mh', 'yx']\n",
    "for d in os.listdir(p):\n",
    "    #put mh in the test set\n",
    "    if d == 'mh':\n",
    "        dirpath = p + '/' + d\n",
    "        filelist = os.listdir(dirpath)\n",
    "        word = [[] for i in range(5)]\n",
    "        word_big = [[] for i in range(5)]\n",
    "        for i in range(len(filelist)):\n",
    "            f = open(dirpath + '/' + filelist[i])\n",
    "            data_ = f.readlines() \n",
    "            data = [i.split(',') for i in data_]\n",
    "            for sample in range(len(data)):\n",
    "                data[sample] = [float(s) for s in data[sample] ]\n",
    "            word[i//5].append(data)\n",
    "            word_big[i//5].extend(data)\n",
    "        test.append(word)\n",
    "        test_big.append(word_big)\n",
    "    else:\n",
    "        if d in dirList:\n",
    "            #put all others in train\n",
    "            dirpath = p + '/' + d\n",
    "            filelist = os.listdir(dirpath)\n",
    "            word = [[] for i in range(5)]\n",
    "            word_big = [[] for i in range(5)]\n",
    "            for i in range(len(filelist)):\n",
    "                f = open(dirpath + '/' + filelist[i])\n",
    "                data_ = f.readlines() \n",
    "                data = [i.split(',') for i in data_]\n",
    "                for sample in range(len(data)):\n",
    "                    data[sample] = [float(s) for s in data[sample] ]\n",
    "                word[i//5].append(data)\n",
    "                word_big[i//5].extend(data)\n",
    "            train.append(word)\n",
    "            train_big.append(word_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 5\n"
     ]
    }
   ],
   "source": [
    "print(len(train_big),len(train_big[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.718281828459045\n"
     ]
    }
   ],
   "source": [
    "#speaker dependent hmm\n",
    "\n",
    "print(np.exp(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 3, 3], [2, 2, 2], [1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "a = [[1,1,1],\n",
    "     [2,2,2],\n",
    "     [3,3,3]]\n",
    "\n",
    "a_ = [a[-i] for i in range(1,len(a)+1)]\n",
    "print(a_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### HELPER FUNCTIONSSSSS\n",
    "#implemented from https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm\n",
    "\n",
    "#Forward Pass\n",
    "\n",
    "def calculateBMatrix(X, mu, sigma,N):\n",
    "    #X is the data in a file\n",
    "    #mu is the average across all states \n",
    "    #sigma is the covariance matrix across all states \n",
    "    #N is the number of states\n",
    "    #BMatrix = [T,N]\n",
    "\n",
    "    T = len(X)\n",
    "    bMatrix = np.zeros((N,T))\n",
    "    for frame in range(T):\n",
    "        temp_frame = []\n",
    "        for state in range(N):\n",
    "            bMatrix[state,frame] = stats.multivariate_normal(mean=(mu[state]),cov=sigma[state]).pdf(X[frame])\n",
    "        \n",
    "    return bMatrix\n",
    "\n",
    "def calculateAlphaMatrix(transMatrix, BMatrix,priors):\n",
    "    #transMatrix is A \n",
    "    #BMatrix is B\n",
    "    #priors is pi\n",
    "    #populate our alpha matrix\n",
    "    T = len(BMatrix[0])\n",
    "    N = len(BMatrix)\n",
    "    Amat = np.array(transMatrix)\n",
    "    alpha = np.zeros((N,T))\n",
    "    #alphaMatrix = [T,N]\n",
    "    for i in range(N):\n",
    "        alpha[i,0]=priors[i]*BMatrix[i,0]\n",
    "    for t in range(1,T):\n",
    "        for i in range(0,N):\n",
    "            alpha[i,t]=BMatrix[i,t]*np.inner(alpha[:,t-1],Amat[:,i])\n",
    "        \n",
    "#     alphaMatrix = []\n",
    "\n",
    "#     for frame in range(T):\n",
    "#         temp_frame = []\n",
    "#         for state in range(N):\n",
    "#             if frame == 0:\n",
    "#                 #if we are in the first frame\n",
    "#                 temp_frame.append(priors[state] * BMatrix[frame][state])\n",
    "#             else:\n",
    "#                 #any other frame\n",
    "#                 s = sum([ alphaMatrix[frame-1][n] * transMatrix[n][state] for n in range(N)])\n",
    "#                 x = BMatrix[frame][state] * s\n",
    "#                 temp_frame.append(x)\n",
    "#         alphaMatrix.append(temp_frame)\n",
    "        \n",
    "    return alpha\n",
    "       \n",
    "#Backward Pass\n",
    "\n",
    "def calculateBetaMatrix(transMatrix, bMatrix):\n",
    "    #populate the Beta matrix\n",
    "\n",
    "    Amat = np.array(transMatrix)\n",
    "    Bmat = np.array(bMatrix)\n",
    "    T = len(bMatrix[0])\n",
    "    N = len(bMatrix)\n",
    "    beta = np.zeros((N,T))\n",
    "    for i in range(N):\n",
    "        beta[i,T-1]=1\n",
    "    for t in range(T-2,-1,-1):\n",
    "        for i in range(0,N):\n",
    "            beta[i,t]=np.inner(Amat[i,0:N],beta[:,t+1]*Bmat[:,t+1])\n",
    "    return beta\n",
    "#Update step helper functions\n",
    "\n",
    "def calculateGammaMatrix(alphaMatrix, betaMatrix):\n",
    "    T = len(betaMatrix[0])\n",
    "    N = len(betaMatrix)\n",
    "    gamma = np.zeros((N,T))\n",
    "    for t in range(T):\n",
    "        gamma[:,t]=alphaMatrix[:,t]*betaMatrix[:,t]\n",
    "        gamma[:,t]=gamma[:,t]/np.sum(gamma[:,t])\n",
    "#     sumList = np.zeros((T))\n",
    "#     for f in range(T):\n",
    "#         for n in range(N):\n",
    "#             sumList[f] += alphaMatrix[f][n]*betaMatrix[f][n]\n",
    "#     #sumList = [sum([alphaMatrix[f][n] * betaMatrix[f][n] for n in range(N)]) for f in range(T)]\n",
    "#     gammaMatrix = []\n",
    "#     for frame in range(T):\n",
    "#         temp_frame = []\n",
    "#         for state in range(N):\n",
    "#             temp_frame.append(alphaMatrix[frame][state] / sumList[frame])\n",
    "#         gammaMatrix.append(temp_frame)\n",
    "    return gamma\n",
    "\n",
    "def calculateXiMatrix( transMatrix, alphaMatrix, betaMatrix, bMatrix):\n",
    "    #caluculate xi \n",
    "    #probability of going from state n to state j at times t and t+1 respectively\n",
    "    T = len(bMatrix)\n",
    "    N = len(bMatrix[0])\n",
    "    \n",
    "    xiMatrix = [[0 for i in range(2*N)] for j in range(T)]\n",
    "    \n",
    "    for frame in range(T):\n",
    "        for state in range(N):\n",
    "            for state_ in range(state, state + 2):\n",
    "                xiMatrix[frame, state + state_] = alphaMatrix[frame, state] * transMatrix[state][state_]\n",
    "                if frame < (T-1):\n",
    "                    if state_ == N:\n",
    "                        xiMatrix[frame, state + state_] = 0\n",
    "                    else:\n",
    "                        xiMatrix[frame, state + state_] = xiMatrix[frame, state + state_] * bMatrix[frame + 1, state_] * betaMatrix[frame+1, state_]\n",
    "        xi[:,t]=xi[:,t]/np.sum(xi[:,t])\n",
    "        xiMatrix[frame,:] = xiMatrix[frame,:]/np.sum(xiMatrix[frame, :]) \n",
    "    \n",
    "    return xiMatrix \n",
    "\n",
    "#Update steps\n",
    "def updatePriors(gammaMatrix):\n",
    "    #we want the expected frequency spent in state i at time step t = 1\n",
    "    new_priors = []\n",
    "    N = len(gammaMatrix[0])\n",
    "    for n in range(N):\n",
    "        new_priors.append(gammaMatrix[0][n])\n",
    "    return new_priors\n",
    "\n",
    "def updateA(priors, transMatrix, X, mu, sigma, n, j, t, N, T):\n",
    "    num = 0\n",
    "    for i in range(T-1):\n",
    "        #across all t\n",
    "        temp = calculateXi(transMatrix, X, mu, sigma, n, j, i, N, T)\n",
    "        num += temp\n",
    "        \n",
    "    denom = 0\n",
    "    for i in range(T-1):\n",
    "        #across all t\n",
    "        temp = calculateGamma(priors, transMatrix, X, mu, sigma, n, i, N, T)\n",
    "        denom += temp\n",
    "    \n",
    "    alpha_update = num / denom\n",
    "    \n",
    "    return alpha_update\n",
    "\n",
    "def updateB(vk, priors, transMatrix, X, mu, sigma, n, N, T):\n",
    "    #vk is a specific observation\n",
    "    #how to compare that against all the y_t?\n",
    "    num = 0\n",
    "    for i in range(T):\n",
    "        #if yt == vk:\n",
    "        temp = calculateGamma(priors, transMatrix, X, mu, sigma, n, i, N, T)\n",
    "        num += temp\n",
    "    denom = 0\n",
    "    for i in range(T):\n",
    "        temp = calculateGamma(priors, transMatrix, X, mu, sigma, n, i, N, T)\n",
    "        denom += temp\n",
    "    b_update = num / denom\n",
    "    \n",
    "    return b_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "#speaker independent hmm\n",
    "initialProbabilities = [0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "\n",
    "initialTransition = [[0.8,0.2,  0,  0,   0],\n",
    "                     [  0,0.8,0.2,  0,   0],\n",
    "                     [  0,  0,0.8,0.2,   0],\n",
    "                     [  0,  0,  0,0.8, 0.2],\n",
    "                     [  0,  0,  0,  0,   1]]\n",
    "\n",
    "#create an HMM for each word\n",
    "word0 = []\n",
    "word1 = []\n",
    "word2 = []\n",
    "word3 = []\n",
    "word4 = []\n",
    "\n",
    "for speaker in range(len(train_big)):\n",
    "    for word in range(len(train_big[speaker])):\n",
    "        if word == 0:\n",
    "            word0.extend(train_big[speaker][word])\n",
    "        if word == 1:\n",
    "            word1.extend(train_big[speaker][word])\n",
    "        if word == 2:\n",
    "            word2.extend(train_big[speaker][word])\n",
    "        if word == 3:\n",
    "            word3.extend(train_big[speaker][word])\n",
    "        if word == 4:\n",
    "            word4.extend(train_big[speaker][word])\n",
    "\n",
    "#initialize mean of hmm for the first word\n",
    "word0 = np.array(word0)\n",
    "#mu should be (1x14) for each state\n",
    "# mu_matrix\n",
    "mu0 = np.mean(word0, axis=0)\n",
    "#initialize the covariance matrix, sigma, of the hmm for the first word\n",
    "word0_np = np.array(word0)\n",
    "#covmat should be (14x14) for each state\n",
    "covmat0 = np.cov(word0_np.T)\n",
    "\n",
    "# Baum Welch this bitchhhhhh\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments\n",
    "\n",
    "There are N = 5 possible states and the number of observations is equal to the number of rows or time steps for each utterance. At each time step or observation, we are in one of the states, so a number 1-5 i think. So T = how many ever rows there are present. Still not sure exactly how we should chunk the data. Not sure if we should separate it by utterance or if we should group all the utterances of the same word together. Update parameters using baum welch forward/backward propogation equations. According to Piazza, update alpha and beta after each individual utterance and update the transition matrix after all the files for that word? Not how to implement this though. \n",
    "\n",
    "We train one hmm per word, so 5 in total. To predict a test sample, we run the utterance through each of the 5 models and assign it the label of the hmm that gives the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kalinuxmsi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:79: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "#SDEP for ASR\n",
    "A = initialTransition\n",
    "mn = [sdepasr_mn,sdepasr_mn,sdepasr_mn,sdepasr_mn,sdepasr_mn]\n",
    "cov = [sdepasr_cov,sdepasr_cov,sdepasr_cov,sdepasr_cov,sdepasr_cov]\n",
    "B = calculateBMatrix(sdeptrain[0][0][0],mn,cov,5)\n",
    "Alpha = calculateAlphaMatrix(A,B,initialProbabilities)\n",
    "Beta = calculateBetaMatrix(A,B)\n",
    "Gamma = calculateGammaMatrix(Alpha,Beta)\n",
    "print(Gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
