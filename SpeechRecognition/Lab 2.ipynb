{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction\n",
    "train will have the training samples and test will have the test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#speaker dependent partitions\n",
    "#sdeptrain dimensions sdeptrain[speaker][word][utterance idx] with idxs given alphabetically\n",
    "sdeptrain = [[[[] for i in range(4)] for j in range(5)] for k in range(4)]\n",
    "#sdeptest dimensions sdeptrain[speaker][word] with idxs given alphabetically\n",
    "sdeptest = [[[] for i in range(5)] for j in range(4)]\n",
    "p = './feature'\n",
    "speakerList = ['dg', 'ls', 'mh', 'yx']\n",
    "wordList = ['asr','cnn','dnn','hmm','tts']\n",
    "snum = 0\n",
    "for s in speakerList:\n",
    "    speakerpath = p+'/'+s\n",
    "    wnum = 0\n",
    "    for w in wordList:\n",
    "        wordpath = speakerpath +'/'+s+'_'+w\n",
    "        #set up train\n",
    "        for utt in range(1,5):\n",
    "            fpath = wordpath + str(utt)+'.fea'\n",
    "            f = open(fpath,'r')\n",
    "            data = f.readlines()\n",
    "            data = [i.split(',') for i in data]\n",
    "            for d in data:\n",
    "                sdeptrain[snum][wnum][utt-1].append(d)\n",
    "        #set up test\n",
    "        fpath2 = wordpath + str(5)+'.fea'\n",
    "        f2 = open(fpath2,'r')\n",
    "        data = f2.readlines()\n",
    "        data = [i.split(',') for i in data]\n",
    "        for d in data:\n",
    "            sdeptest[snum][wnum].append(d)\n",
    "        wnum += 1\n",
    "    snum += 1\n",
    "sdeptrain = np.array(sdeptrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#speaker independent partitions\n",
    "#Extract feature vectors\n",
    "p = './feature'\n",
    "test = []\n",
    "test_big = []\n",
    "train = []\n",
    "train_big = []\n",
    "dirList = ['dg', 'ls', 'mh', 'yx']\n",
    "for d in os.listdir(p):\n",
    "    #put mh in the test set\n",
    "    if d == 'mh':\n",
    "        dirpath = p + '/' + d\n",
    "        filelist = os.listdir(dirpath)\n",
    "        word = [[] for i in range(5)]\n",
    "        word_big = [[] for i in range(5)]\n",
    "        for i in range(len(filelist)):\n",
    "            f = open(dirpath + '/' + filelist[i])\n",
    "            data_ = f.readlines() \n",
    "            data = [i.split(',') for i in data_]\n",
    "            for sample in range(len(data)):\n",
    "                data[sample] = [float(s) for s in data[sample] ]\n",
    "            word[i//5].append(data)\n",
    "            word_big[i//5].extend(data)\n",
    "        test.append(word)\n",
    "        test_big.append(word_big)\n",
    "    else:\n",
    "        if d in dirList:\n",
    "            #put all others in train\n",
    "            dirpath = p + '/' + d\n",
    "            filelist = os.listdir(dirpath)\n",
    "            word = [[] for i in range(5)]\n",
    "            word_big = [[] for i in range(5)]\n",
    "            for i in range(len(filelist)):\n",
    "                f = open(dirpath + '/' + filelist[i])\n",
    "                data_ = f.readlines() \n",
    "                data = [i.split(',') for i in data_]\n",
    "                for sample in range(len(data)):\n",
    "                    data[sample] = [float(s) for s in data[sample] ]\n",
    "                word[i//5].append(data)\n",
    "                word_big[i//5].extend(data)\n",
    "            train.append(word)\n",
    "            train_big.append(word_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 5\n"
     ]
    }
   ],
   "source": [
    "print(len(train_big),len(train_big[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#speaker dependent hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2310, 14)\n"
     ]
    }
   ],
   "source": [
    "#speaker independent hmm\n",
    "initialProbabilities = [0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "\n",
    "initialTransition = [[0.8,0.2,  0,  0,   0],\n",
    "                     [  0,0.8,0.2,  0,   0],\n",
    "                     [  0,  0,0.8,0.2,   0],\n",
    "                     [  0,  0,  0,0.8, 0.2],\n",
    "                     [  0,  0,  0,  0,   1]]\n",
    "\n",
    "#create an HMM for each word\n",
    "word0 = []\n",
    "word1 = []\n",
    "word2 = []\n",
    "word3 = []\n",
    "word4 = []\n",
    "\n",
    "for speaker in range(len(train_big)):\n",
    "    for word in range(len(train_big[speaker])):\n",
    "        if word == 0:\n",
    "            word0.extend(train_big[speaker][word])\n",
    "        if word == 1:\n",
    "            word1.extend(train_big[speaker][word])\n",
    "        if word == 2:\n",
    "            word2.extend(train_big[speaker][word])\n",
    "        if word == 3:\n",
    "            word3.extend(train_big[speaker][word])\n",
    "        if word == 4:\n",
    "            word4.extend(train_big[speaker][word])\n",
    "\n",
    "#initialize mean of hmm for the first word\n",
    "word0 = np.array(word0)\n",
    "print(word0.shape)\n",
    "#mu should be (1x14)\n",
    "mu0 = np.mean(word0, axis=0)\n",
    "#initialize the covariance matrix, sigma, of the hmm for the first word\n",
    "word0_np = np.array(word0)\n",
    "#covmat should be (14x14)\n",
    "covmat0 = np.cov(word0_np.T)\n",
    "\n",
    "# Baum Welch this bitchhhhhh\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments\n",
    "\n",
    "There are N = 5 possible states and the number of observations is equal to the number of rows or time steps for each utterance. At each time step or observation, we are in one of the states, so a number 1-5 i think. So T = how many ever rows there are present. Still not sure exactly how we should chunk the data. Not sure if we should separate it by utterance or if we should group all the utterances of the same word together. Update parameters using baum welch forward/backward propogation equations. According to Piazza, update alpha and beta after each individual utterance and update the transition matrix after all the files for that word? Not how to implement this though. \n",
    "\n",
    "We train one hmm per word, so 5 in total. To predict a test sample, we run the utterance through each of the 5 models and assign it the label of the hmm that gives the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.8224, -14.252, 1.6086, -0.41555, 0.055411, 0.20185, 0.41501, 0.10763, -0.033127, -0.26792, 0.0079, -0.023143, 0.30929, 0.25359]\n"
     ]
    }
   ],
   "source": [
    "# def covmat(data):\n",
    "#     #create the covariance matrix\n",
    "#     #each row is a variable\n",
    "#     #each column is an observation of each of those variables\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
