{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction\n",
    "train will have the training samples and test will have the test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract feature vectors\n",
    "p = './feature'\n",
    "test = []\n",
    "test_big = []\n",
    "test_2 = []\n",
    "train = []\n",
    "train_big = []\n",
    "train_2 = []\n",
    "dirList = ['dg', 'ls', 'mh', 'yx']\n",
    "for d in os.listdir(p):\n",
    "    #put mh in the test set\n",
    "    if d == 'mh':\n",
    "        dirpath = p + '/' + d\n",
    "        filelist = os.listdir(dirpath)\n",
    "        word = [[] for i in range(5)]\n",
    "        word_big = [[] for i in range(5)]\n",
    "        rn1 = np.random.randint(0,5)\n",
    "        rn2 = np.random.randint(5,10)\n",
    "        rn3 = np.random.randint(10,15)\n",
    "        rn4 = np.random.randint(15,20)\n",
    "        rn5 = np.random.randint(20,25)\n",
    "        for i in range(len(filelist)):\n",
    "            f = open(dirpath + '/' + filelist[i])\n",
    "            data_ = f.readlines() \n",
    "            data = [i.split(',') for i in data_]\n",
    "            for sample in range(len(data)):\n",
    "                data[sample] = [float(s) for s in data[sample] ]\n",
    "            word[i//5].append(data)\n",
    "            word_big[i//5].extend(data)\n",
    "        test.append(word)\n",
    "        test_big.append(word_big)\n",
    "    else:\n",
    "        if d in dirList:\n",
    "            #put all others in train\n",
    "            dirpath = p + '/' + d\n",
    "            filelist = os.listdir(dirpath)\n",
    "            word = [[] for i in range(5)]\n",
    "            word_big = [[] for i in range(5)]\n",
    "            for i in range(len(filelist)):\n",
    "                f = open(dirpath + '/' + filelist[i])\n",
    "                data_ = f.readlines() \n",
    "                data = [i.split(',') for i in data_]\n",
    "                for sample in range(len(data)):\n",
    "                    data[sample] = [float(s) for s in data[sample] ]\n",
    "                word[i//5].append(data)\n",
    "                word_big[i//5].extend(data)\n",
    "            train.append(word)\n",
    "            train_big.append(word_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(train_big[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2224, 14)\n",
      "(14,)\n"
     ]
    }
   ],
   "source": [
    "initialProbabilities = [0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "\n",
    "initialTransition = [[0.8, 0.2, 0, 0, 0],\n",
    "                     [0, 0.8, 0.2, 0, 0],\n",
    "                     [0, 0, 0.8, 0.2, 0],\n",
    "                     [0, 0, 0, 0.8, 0.2],\n",
    "                     [0, 0, 0, 0, 1]]\n",
    "\n",
    "#create an HMM for each word\n",
    "word0 = []\n",
    "word1 = []\n",
    "word2 = []\n",
    "word3 = []\n",
    "word4 = []\n",
    "\n",
    "for speaker in range(len(train_big)):\n",
    "    for word in range(len(train_big[speaker])):\n",
    "        if word == 0:\n",
    "            word0.extend(train_big[speaker][word])\n",
    "        if word == 1:\n",
    "            word1.extend(train_big[speaker][word])\n",
    "        if word == 2:\n",
    "            word2.extend(train_big[speaker][word])\n",
    "        if word == 3:\n",
    "            word3.extend(train_big[speaker][word])\n",
    "        if word == 4:\n",
    "            word4.extend(train_big[speaker][word])\n",
    "\n",
    "#initialize mean of hmm for the first word\n",
    "word0 = np.array(word0)\n",
    "print(word0.shape)\n",
    "#mu should be (1x14)\n",
    "mu0 = np.mean(word0, axis=0)\n",
    "#initialize the covariance matrix, sigma, of the hmm for the first word\n",
    "word0_np = np.array(word0)\n",
    "#covmat should be (14x14)\n",
    "covmat0 = np.cov(word0_np.T)\n",
    "\n",
    "# Baum Welch this bitchhhhhh\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.8224, -14.252, 1.6086, -0.41555, 0.055411, 0.20185, 0.41501, 0.10763, -0.033127, -0.26792, 0.0079, -0.023143, 0.30929, 0.25359]\n"
     ]
    }
   ],
   "source": [
    "# def covmat(data):\n",
    "#     #create the covariance matrix\n",
    "#     #each row is a variable\n",
    "#     #each column is an observation of each of those variables\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
