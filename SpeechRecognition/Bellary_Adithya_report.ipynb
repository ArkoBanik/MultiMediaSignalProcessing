{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adithya Bellary\n",
    "Kourosh Arasteh\n",
    "Arko Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speaker dependent feature extraction\n",
    "We assign the 5th utterance of each word of each speaker to the test set. All else will be used to train our HMM on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#speaker dependent partitions\n",
    "#sdeptrain dimensions sdeptrain[speaker][word][utterance idx] with idxs given alphabetically\n",
    "sdeptrain = [[[[] for i in range(4)] for j in range(5)] for k in range(4)]\n",
    "#sdeptest dimensions sdeptrain[speaker][word] with idxs given alphabetically\n",
    "sdeptest = [[[] for i in range(5)] for j in range(4)]\n",
    "p = './feature'\n",
    "speakerList = ['dg', 'ls', 'mh', 'yx']\n",
    "wordList = ['asr','cnn','dnn','hmm','tts']\n",
    "snum = 0\n",
    "for s in speakerList:\n",
    "    speakerpath = p+'/'+s\n",
    "    wnum = 0\n",
    "    for w in wordList:\n",
    "        wordpath = speakerpath +'/'+s+'_'+w\n",
    "        #set up train\n",
    "        for utt in range(1,5):\n",
    "            fpath = wordpath + str(utt)+'.fea'\n",
    "            f = open(fpath,'r')\n",
    "            data = f.readlines()\n",
    "            data = [i.split(',') for i in data]\n",
    "            for sample in range(len(data)):\n",
    "                data[sample] = [float(i) for i in data[sample]]\n",
    "            for d in data:\n",
    "                sdeptrain[snum][wnum][utt-1].append(d)\n",
    "        #set up test\n",
    "        fpath2 = wordpath + str(5)+'.fea'\n",
    "        f2 = open(fpath2,'r')\n",
    "        data = f2.readlines()\n",
    "        data = [i.split(',') for i in data]\n",
    "        for d in data:\n",
    "            sdeptest[snum][wnum].append(d)\n",
    "        wnum += 1\n",
    "    snum += 1\n",
    "sdeptest = np.array(sdeptest)\n",
    "sdeptrain = np.array(sdeptrain)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the mean and covariance matrices\n",
    "\n",
    "For this specific lab, we are modeling each of our 5 states as Gaussians. Therefore, each state will be characterized by a mean matrix and a covariance matrix. Below, we initialize these matrices across all utterances of that word in the training set. Each state will start out with the same mean and covariance matrix. Throughout the training process, these values will be updated and tuned to give our HMM the optimal set of parameters.\n",
    "\n",
    "We choose to use the diagonal covariance matrix instead of the full covariance matrix. We make this design choice because when we tried to use the full matrix, our covariance matrix did not stay positive semidefinite after a number of iterations. Only taking the values across the diagonal also greatly helped our numerical stability of the HMM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigasr = []\n",
    "bigcnn = []\n",
    "bigdnn = []\n",
    "bighmm = []\n",
    "bigtts = []\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        bigasr.extend(sdeptrain[i][0][j])\n",
    "        bigcnn.extend(sdeptrain[i][1][j])\n",
    "        bigdnn.extend(sdeptrain[i][2][j])\n",
    "        bighmm.extend(sdeptrain[i][3][j])\n",
    "        bigtts.extend(sdeptrain[i][4][j])\n",
    "sdepasr_cov = np.diag(np.diag(np.cov(np.array(bigasr).T)))\n",
    "sdepcnn_cov = np.diag(np.diag(np.cov(np.array(bigcnn).T)))\n",
    "sdepdnn_cov = np.diag(np.diag(np.cov(np.array(bigdnn).T)))\n",
    "sdephmm_cov = np.diag(np.diag(np.cov(np.array(bighmm).T)))\n",
    "sdeptts_cov = np.diag(np.diag(np.cov(np.array(bigtts).T)))\n",
    "sdepasr_mn = np.mean(bigasr,axis=0)\n",
    "sdepcnn_mn = np.mean(bigcnn,axis=0)\n",
    "sdepdnn_mn = np.mean(bigdnn,axis=0)\n",
    "sdephmm_mn = np.mean(bighmm,axis=0)\n",
    "sdeptts_mn = np.mean(bigtts,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speaker Independent Feature Extraction\n",
    "All utterances of speaker mh will be used as test data. The rest will be assigned as training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#speaker independent partitions\n",
    "#sindeptrain dimensions sdeptrain[speaker][word][utterance idx] with idxs given alphabetically\n",
    "sindeptrain = [[[[] for i in range(5)] for j in range(5)] for k in range(3)]\n",
    "#sindeptest dimensions sdeptest[word][utterance idx] with idxs given alphabetically\n",
    "sindeptest = [[[] for i in range(5)] for j in range(5)]\n",
    "p = './feature'\n",
    "speakerList2 = ['dg', 'ls', 'mh']#'mh'\n",
    "wordList = ['asr','cnn','dnn','hmm','tts']\n",
    "snum = 0\n",
    "for s in speakerList2:\n",
    "    speakerpathi = p+'/'+s\n",
    "    wnum = 0\n",
    "    for w in wordList:\n",
    "        wordpathi = speakerpathi +'/'+s+'_'+w\n",
    "        #set up train\n",
    "        for utt in range(1,6):\n",
    "            fpathi = wordpathi + str(utt)+'.fea'\n",
    "            fi = open(fpathi,'r')\n",
    "            datai = fi.readlines()\n",
    "            datai = [i.split(',') for i in datai]\n",
    "            for sample in range(len(datai)):\n",
    "                datai[sample] = [float(i) for i in datai[sample]]\n",
    "            for d in datai:\n",
    "                sindeptrain[snum][wnum][utt-1].append(d)\n",
    "        wnum += 1\n",
    "    snum += 1\n",
    "    \n",
    "#set up test\n",
    "s2 = 'yx'\n",
    "speakertestpath = p+'/'+s2\n",
    "wnum = 0\n",
    "for w in wordList:\n",
    "    wordtestpath = speakertestpath +'/'+s2+'_'+w\n",
    "    for utt in range(1,6):\n",
    "        ftestpath = wordtestpath + str(utt)+'.fea'\n",
    "        fitest = open(ftestpath,'r')\n",
    "        dataitest = fitest.readlines()\n",
    "        dataitest = [i.split(',') for i in dataitest]\n",
    "        for sample in range(len(dataitest)):\n",
    "            dataitest[sample] = [float(i) for i in dataitest[sample]]\n",
    "        for d in dataitest:\n",
    "            sindeptest[wnum][utt-1].append(d)\n",
    "    wnum += 1\n",
    "\n",
    "sindeptrain = np.array(sindeptrain)\n",
    "sindeptest = np.array(sindeptest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the mean and covariance matrices in the same manner as we did for the speaker dependent method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigasri = []\n",
    "bigcnni = []\n",
    "bigdnni = []\n",
    "bighmmi = []\n",
    "bigttsi = []\n",
    "for i in range(3):\n",
    "    for j in range(5):\n",
    "        bigasri.extend(sindeptrain[i][0][j])\n",
    "        bigcnni.extend(sindeptrain[i][1][j])\n",
    "        bigdnni.extend(sindeptrain[i][2][j])\n",
    "        bighmmi.extend(sindeptrain[i][3][j])\n",
    "        bigttsi.extend(sindeptrain[i][4][j])\n",
    "sdepasri_cov = np.diag(np.diag(np.cov(np.array(bigasri).T)))\n",
    "sdepcnni_cov = np.diag(np.diag(np.cov(np.array(bigcnni).T)))\n",
    "sdepdnni_cov = np.diag(np.diag(np.cov(np.array(bigdnni).T)))\n",
    "sdephmmi_cov = np.diag(np.diag(np.cov(np.array(bighmmi).T)))\n",
    "sdepttsi_cov = np.diag(np.diag(np.cov(np.array(bigttsi).T)))\n",
    "sdepasri_mn = np.mean(bigasri,axis=0)\n",
    "sdepcnni_mn = np.mean(bigcnni,axis=0)\n",
    "sdepdnni_mn = np.mean(bigdnni,axis=0)\n",
    "sdephmmi_mn = np.mean(bighmmi,axis=0)\n",
    "sdepttsi_mn = np.mean(bigttsi,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Probability and Transition Probability DIstribution\n",
    "\n",
    "The initial probability matrix defines the probability of which state the process starts in. Since we have 5 states per HMM and we want there to be an equal chance that each state is the first state, each state will have 20%.\n",
    "\n",
    "The transition probability will define how we move from state to state. Since we are training a left-to-right nonskip HMM, then the only option for a next state is just the next one. It will move on with probability 20% and stay in the same state 80%. These are the only two options. The transition probability matrix will be updated as we move through the training process.\n",
    "\n",
    "These two matrices as well as the mean and covariance matrices we just computed will be the 4 parameters that define an HMM at a given time. These parameters are often denoted as $\\Theta = (\\pi, A, \\mu, \\sigma)$, with $\\pi$ being the initial state probability distribution and A being the transition probability matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialProbabilities = [0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "\n",
    "initialTransition = [[0.8,0.2,  0,  0,   0],\n",
    "                     [  0,0.8,0.2,  0,   0],\n",
    "                     [  0,  0,0.8,0.2,   0],\n",
    "                     [  0,  0,  0,0.8, 0.2],\n",
    "                     [  0,  0,  0,  0,   1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM Training Helper Functions\n",
    "\n",
    "Now that we have the parameters that define the HMM at a particular time, we need to define how these parameters are going to be updated. We will train each HMM according to the Baum-Welch algorithm at this link here: https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm. \n",
    "\n",
    "The helper functions below generate the $\\alpha, \\beta, \\gamma,$ and $\\xi$ values that we need to update our initial probabilities $\\pi$ and transition matrix A. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### HELPER FUNCTIONSSSSS\n",
    "#implemented from https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm\n",
    "\n",
    "#Forward Pass\n",
    "def calculateBMatrix(X, mu, sigma,N):\n",
    "    #X is the data in a file\n",
    "    #mu is the average across all states \n",
    "    #sigma is the covariance matrix across all states \n",
    "    #N is the number of states\n",
    "    #BMatrix = [T,N]\n",
    "    #print(sigma[0])\n",
    "    T = len(X)\n",
    "    bMatrix = np.zeros((N,T))\n",
    "    for frame in range(T):\n",
    "        temp_frame = []\n",
    "        for state in range(N):\n",
    "            bMatrix[state,frame] = stats.multivariate_normal(mean=(mu[state]),cov=sigma[state]).pdf(X[frame])\n",
    "    return bMatrix\n",
    "\n",
    "def calculateAlphaMatrix(transMatrix, BMatrix,priors):\n",
    "    #transMatrix is A \n",
    "    #BMatrix is B\n",
    "    #priors is pi\n",
    "    #populate our alpha matrix\n",
    "    T = len(BMatrix[0])\n",
    "    N = len(BMatrix)\n",
    "    Amat = np.array(transMatrix)\n",
    "    alpha = np.zeros((N,T))\n",
    "    #alphaMatrix = [T,N]\n",
    "    for i in range(N):\n",
    "        alpha[i,0]=priors[i]*BMatrix[i,0]\n",
    "    for t in range(1,T):\n",
    "        for i in range(0,N):\n",
    "            alpha[i,t]=BMatrix[i,t]*np.inner(alpha[:,t-1],Amat[:,i])       \n",
    "    return alpha\n",
    "\n",
    "def calculateAlphaBetaTildeG(transMatrix,BMatrix,priors):\n",
    "    T = len(BMatrix[0])\n",
    "    N = len(BMatrix)\n",
    "    baralpha = np.zeros((N,T))\n",
    "    tildealpha = np.zeros((N,T))\n",
    "    tildebeta = np.zeros((N,T))\n",
    "    log_g = np.zeros((T))\n",
    "    for i in range(0,N):\n",
    "        baralpha[i,0]=priors[i]*BMatrix[i,0]\n",
    "    log_g[0] = np.log(np.sum(baralpha[:,0]))\n",
    "    tildealpha[:,0]=baralpha[:,0]/np.exp(log_g[0])\n",
    "\n",
    "    for t in range(1,T):\n",
    "        for i in range(0,N):\n",
    "            baralpha[i,t]=BMatrix[i,t]*np.inner(tildealpha[:,t-1],transMatrix[:,i])\n",
    "        log_g[t] = np.log(np.sum(baralpha[:,t]))\n",
    "        tildealpha[:,t]=baralpha[:,t]/np.exp(log_g[t])\n",
    "    for i in range(0,N):\n",
    "        tildebeta[i,T-1] = 1/np.exp(log_g[T-1])\n",
    "\n",
    "    for t in range(T-2,-1,-1):\n",
    "        for i in range(0,N):\n",
    "            tildebeta[i,t]=np.inner(transMatrix[i,0:N],tildebeta[:,t+1]*BMatrix[:,t+1])/np.exp(log_g[t+1])\n",
    "    \n",
    "    return tildealpha,tildebeta,log_g\n",
    "#Backward Pass\n",
    "    \n",
    "def calculateBetaMatrix(transMatrix, bMatrix):\n",
    "    #populate the Beta matrix\n",
    "    T = len(bMatrix[0])\n",
    "    N = len(bMatrix)\n",
    "    beta = np.zeros((N,T))\n",
    "    for i in range(N):\n",
    "        beta[i,T-1]=1\n",
    "    for t in range(T-2,-1,-1):\n",
    "        for i in range(0,N):\n",
    "            beta[i,t]=np.inner(transMatrix[i,0:N],beta[:,t+1]*bMatrix[:,t+1])\n",
    "    return beta\n",
    "#Update step helper functions\n",
    "\n",
    "def calculateGammaMatrix(alphaMatrix, betaMatrix):\n",
    "    T = len(betaMatrix[0])\n",
    "    N = len(betaMatrix)\n",
    "    gamma = np.zeros((N,T))\n",
    "    for t in range(T):\n",
    "        gamma[:,t]=alphaMatrix[:,t]*betaMatrix[:,t]\n",
    "        gamma[:,t]=gamma[:,t]/np.sum(gamma[:,t])\n",
    "    return gamma\n",
    "\n",
    "def calculateXiMatrix( transMatrix, alphaMatrix, betaMatrix, bMatrix):\n",
    "    #caluculate xi \n",
    "    #probability of going from state n to state j at times t and t+1 respectively\n",
    "    T = len(bMatrix[0])\n",
    "    N = len(bMatrix)\n",
    "    xi = np.zeros((2*N,T))\n",
    "    for t in range(0,T):\n",
    "        for i in range(0,N):\n",
    "            for j in range(i,i+2):\n",
    "                if j>= N:\n",
    "                    xi[i+j,t] = 0\n",
    "                else:\n",
    "                    xi[i+j,t]=alphaMatrix[i,t]*transMatrix[i,j]\n",
    "                if (t<T-1):\n",
    "                    if j==N:\n",
    "                        xi[i+j,t]=0\n",
    "                    else:\n",
    "                        xi[i+j,t] = xi[i+j,t]*bMatrix[j,t+1]*betaMatrix[j,t+1]\n",
    "        xi[:,t]=xi[:,t]/np.sum(xi[:,t])\n",
    "    return xi \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the HMM Independent of speaker\n",
    "\n",
    "We train each HMM until the parameters reach convergence. To find this optimal number of iterations, we plot the maximum probability of the HMM and stop once it does change anymore. We found that 25 iterations is sufficient for the model parameters to reach convergence.\n",
    "\n",
    "The general method of how we train the HMM for the speaker independent and speaker dependent version is the same. The difference stems from how we make the test and train split above. In the speaker independent version, there are only 3 speakers, and the speaker dependent, we train on utterances from all 4 speakers. These differences will change the code structure slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#SDEP general\n",
    "def SINDEPTRAIN(initialTransition,initialProbabilities,initialmeans,initialcov,wordidx,numiterations,numcoeff,Sindeptrain):\n",
    "    A = np.array(initialTransition)\n",
    "    mn = initialmeans\n",
    "    cov = initialcov\n",
    "    logprobs = np.zeros((15,numiterations))\n",
    "    for iteration in range(numiterations):\n",
    "        alphas = []\n",
    "        betas = []\n",
    "        gammas = []\n",
    "        xis = []\n",
    "        for s in range(3):#for each speaker\n",
    "            for u in range(5): # for each utterance\n",
    "                B = calculateBMatrix(Sindeptrain[s][wordidx][u],mn,cov,5)\n",
    "                Alpha,Beta,logg = calculateAlphaBetaTildeG(A,B,initialProbabilities)\n",
    "                Gamma = calculateGammaMatrix(Alpha,Beta)\n",
    "                xi = calculateXiMatrix(A,Alpha,Beta,B)\n",
    "                alphas.append(Alpha)\n",
    "                betas.append(Beta)\n",
    "                gammas.append(Gamma)\n",
    "                xis.append(xi)\n",
    "                logprobs[:,iteration] = np.sum(logg)\n",
    "\n",
    "        #upd8 \n",
    "        num_files = len(alphas)\n",
    "        N = len(gammas[0])\n",
    "        A_new = np.zeros((N,N))\n",
    "        mu_new = np.zeros((N,numcoeff))\n",
    "        sigma_new = np.zeros((N,numcoeff, numcoeff))\n",
    "        # find aij\n",
    "        denomsum = np.zeros((N))\n",
    "        for i in range(0,N):\n",
    "\n",
    "            for f in range(num_files):\n",
    "                for t in range(len(gammas[f][i])):\n",
    "                    denomsum[i] += gammas[f][i][t]\n",
    "\n",
    "            for j in range(i,i+2):\n",
    "                if j<N:\n",
    "                    numsum = 0\n",
    "                    for f in range(num_files):\n",
    "                        for t in range(len(gammas[f][i])):\n",
    "                            numsum += xis[f][i+j][t]\n",
    "                    A_new[i][j] = numsum/denomsum[i]\n",
    "        #find mui\n",
    "        for i in range(0,N):\n",
    "            numsum = 0\n",
    "            for f in range(num_files):\n",
    "                for t in range(len(Sindeptrain[f//5][wordidx][f%5])):\n",
    "                    numsum += (np.array(gammas[f][i][t]) * np.array(Sindeptrain[f//5][wordidx][f%5][t]))\n",
    "            mu_new[i] = numsum/denomsum[i]\n",
    "\n",
    "        #find covdi\n",
    "        for i in range(0,N):\n",
    "            numsum = 0\n",
    "            for f in range(num_files):\n",
    "                for t in range(len(Sindeptrain[f//5][wordidx][f%5])):\n",
    "                    numsum += (np.array(gammas[f][i][t]) * np.array(np.outer((Sindeptrain[f//5][wordidx][f%5][t]-mn[i]),(Sindeptrain[f//5][wordidx][f%5][t]-mn[i]))))\n",
    "            sigma_new[i] = numsum/denomsum[i]\n",
    "        #save the new parameters\n",
    "        A = A_new\n",
    "        mn = mu_new\n",
    "        cov = sigma_new\n",
    "    print(\"indep hmm completed for word\",wordidx)\n",
    "    return A,mn,cov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SDEP general\n",
    "def SDEPTRAIN(initialTransition,initialProbabilities,initialmeans,initialcov,wordidx,numiterations,numcoeff,Sdeptrain):\n",
    "    A = np.array(initialTransition)\n",
    "    mn = initialmeans\n",
    "    cov = initialcov\n",
    "    logprobs = np.zeros((16,numiterations))\n",
    "    for iteration in range(numiterations):\n",
    "        # calc params\n",
    "        alphas = []\n",
    "        betas = []\n",
    "        gammas = []\n",
    "        xis = []\n",
    "        for s in range(4):#for each speaker\n",
    "            for u in range(4): # for each utterance\n",
    "                B = calculateBMatrix(Sdeptrain[s][wordidx][u],mn,cov,5)\n",
    "                Alpha,Beta,logg = calculateAlphaBetaTildeG(A,B,initialProbabilities)\n",
    "                Gamma = calculateGammaMatrix(Alpha,Beta)\n",
    "                xi = calculateXiMatrix(A,Alpha,Beta,B)\n",
    "                alphas.append(Alpha)\n",
    "                betas.append(Beta)\n",
    "                gammas.append(Gamma)\n",
    "                xis.append(xi)\n",
    "                logprobs[:,iteration] = np.sum(logg)\n",
    "\n",
    "        #upd8 \n",
    "        num_files = len(alphas)\n",
    "        N = len(gammas[0])\n",
    "        A_new = np.zeros((N,N))\n",
    "        mu_new = np.zeros((N,numcoeff))\n",
    "        sigma_new = np.zeros((N,numcoeff, numcoeff))\n",
    "        # find aij\n",
    "        denomsum = np.zeros((N))\n",
    "        for i in range(0,N):\n",
    "\n",
    "            for f in range(num_files):\n",
    "                for t in range(len(gammas[f][i])):\n",
    "                    denomsum[i] += gammas[f][i][t]\n",
    "\n",
    "            for j in range(i,i+2):\n",
    "                if j<N:\n",
    "                    numsum = 0\n",
    "                    for f in range(num_files):\n",
    "                        for t in range(len(gammas[f][i])):\n",
    "                            numsum += xis[f][i+j][t]\n",
    "                    A_new[i][j] = numsum/denomsum[i]\n",
    "        #find mui\n",
    "        for i in range(0,N):\n",
    "\n",
    "            numsum = 0\n",
    "            for f in range(num_files):\n",
    "                for t in range(len(gammas[f][i])):\n",
    "                    numsum += (np.array(gammas[f][i][t]) * np.array(Sdeptrain[f//4][wordidx][f%4][t]))\n",
    "            mu_new[i] = numsum/denomsum[i]\n",
    "\n",
    "        #find covdi\n",
    "        for i in range(0,N):\n",
    "\n",
    "            numsum = 0\n",
    "            for f in range(num_files):\n",
    "                for t in range(len(gammas[f][i])):\n",
    "                    numsum += (np.array(gammas[f][i][t]) * np.array(np.outer((Sdeptrain[f//4][wordidx][f%4][t]-mn[i]),(Sdeptrain[f//4][wordidx][f%4][t]-mn[i]))))\n",
    "            sigma_new[i] = numsum/denomsum[i]\n",
    "\n",
    "        A = A_new\n",
    "        mn = mu_new\n",
    "        cov = sigma_new\n",
    "    print(A_new)\n",
    "    print(\"dep hmm completed for word\",wordidx)\n",
    "    return A,mn,cov\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get the optimal parameters for the speaker dependent HMM of each word by training on that portion of the train dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.88004837  0.11995163  0.          0.          0.        ]\n",
      " [ 0.          0.91842459  0.08157541  0.          0.        ]\n",
      " [ 0.          0.          0.89646572  0.10353428  0.        ]\n",
      " [ 0.          0.          0.          0.92693646  0.07306354]\n",
      " [ 0.          0.          0.          0.          1.        ]]\n",
      "dep hmm completed for word 0\n",
      "[[ 0.93941349  0.06058651  0.          0.          0.        ]\n",
      " [ 0.          0.93207714  0.06792286  0.          0.        ]\n",
      " [ 0.          0.          0.85274211  0.14725789  0.        ]\n",
      " [ 0.          0.          0.          0.94678115  0.05321885]\n",
      " [ 0.          0.          0.          0.          1.        ]]\n",
      "dep hmm completed for word 1\n",
      "[[ 0.85035388  0.14964612  0.          0.          0.        ]\n",
      " [ 0.          0.94331581  0.05668419  0.          0.        ]\n",
      " [ 0.          0.          0.89479096  0.10520904  0.        ]\n",
      " [ 0.          0.          0.          0.96392057  0.03607943]\n",
      " [ 0.          0.          0.          0.          1.        ]]\n",
      "dep hmm completed for word 2\n",
      "[[ 0.82018617  0.17981383  0.          0.          0.        ]\n",
      " [ 0.          0.94225915  0.05774085  0.          0.        ]\n",
      " [ 0.          0.          0.77478542  0.22521458  0.        ]\n",
      " [ 0.          0.          0.          0.84477418  0.15522582]\n",
      " [ 0.          0.          0.          0.          1.        ]]\n",
      "dep hmm completed for word 3\n",
      "[[ 0.90084898  0.09915102  0.          0.          0.        ]\n",
      " [ 0.          0.92296079  0.07703921  0.          0.        ]\n",
      " [ 0.          0.          0.91058641  0.08941359  0.        ]\n",
      " [ 0.          0.          0.          0.96567364  0.03432636]\n",
      " [ 0.          0.          0.          0.          1.        ]]\n",
      "dep hmm completed for word 4\n"
     ]
    }
   ],
   "source": [
    "#TRAINING FOR SPEAKER DEPENDENT\n",
    "asrmean = [sdepasr_mn,sdepasr_mn,sdepasr_mn,sdepasr_mn,sdepasr_mn]\n",
    "asrcova = [sdepasr_cov,sdepasr_cov,sdepasr_cov,sdepasr_cov,sdepasr_cov]\n",
    "cnnmean = [sdepcnn_mn,sdepcnn_mn,sdepcnn_mn,sdepcnn_mn,sdepcnn_mn]\n",
    "cnncova = [sdepcnn_cov,sdepcnn_cov,sdepcnn_cov,sdepcnn_cov,sdepcnn_cov]\n",
    "dnnmean = [sdepdnn_mn,sdepdnn_mn,sdepdnn_mn,sdepdnn_mn,sdepdnn_mn]\n",
    "dnncova = [sdepdnn_cov,sdepdnn_cov,sdepdnn_cov,sdepdnn_cov,sdepdnn_cov]\n",
    "hmmmean = [sdephmm_mn,sdephmm_mn,sdephmm_mn,sdephmm_mn,sdephmm_mn]\n",
    "hmmcova = [sdephmm_cov,sdephmm_cov,sdephmm_cov,sdephmm_cov,sdephmm_cov]\n",
    "ttsmean = [sdeptts_mn,sdeptts_mn,sdeptts_mn,sdeptts_mn,sdeptts_mn]\n",
    "ttscova = [sdeptts_cov,sdeptts_cov,sdeptts_cov,sdeptts_cov,sdeptts_cov]\n",
    "A_asr,mn_asr,cov_asr = SDEPTRAIN(initialTransition,initialProbabilities,asrmean,asrcova,0,25,14,sdeptrain)\n",
    "A_cnn,mn_cnn,cov_cnn = SDEPTRAIN(initialTransition,initialProbabilities,cnnmean,cnncova,1,25,14,sdeptrain)\n",
    "A_dnn,mn_dnn,cov_dnn = SDEPTRAIN(initialTransition,initialProbabilities,dnnmean,dnncova,2,25,14,sdeptrain)\n",
    "A_hmm,mn_hmm,cov_hmm = SDEPTRAIN(initialTransition,initialProbabilities,hmmmean,hmmcova,3,25,14,sdeptrain)\n",
    "A_tts,mn_tts,cov_tts = SDEPTRAIN(initialTransition,initialProbabilities,ttsmean,ttscova,4,25,14,sdeptrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the HMM\n",
    "\n",
    "We test our test utterances by first making a forward pass and getting the $\\alpha$ and $\\beta$ values. Then, we can get the maximum probability from that HMM by summing the log of the $g$ vector. Then, we assign the utterance to be the HMM that gave the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testlogProb(testfile,Atest,meanstest,covtest,initProbs):\n",
    "    At = np.array(Atest)\n",
    "    Btest = calculateBMatrix(testfile,meanstest,covtest,5)\n",
    "    logg = np.array(calculateAlphaBetaTildeG(At,Btest,initProbs)[2])\n",
    "    return np.sum(logg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\adithya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:59: RuntimeWarning: overflow encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      " [[ 1.    0.    0.    0.    0.  ]\n",
      " [ 0.    1.    0.    0.    0.  ]\n",
      " [ 0.    0.25  0.75  0.    0.  ]\n",
      " [ 0.    0.    0.    1.    0.  ]\n",
      " [ 0.    0.    0.    0.    1.  ]]\n",
      "Accuracy\n",
      " 0.95\n"
     ]
    }
   ],
   "source": [
    "#SPEAKER DEPENDENT TEST\n",
    "#sdeptest[speaker][word]\n",
    "classifications = np.zeros((5,5))\n",
    "#classifications[truth][prediction] where idx{0,1,2,3,4} => {'asr','cnn','dnn','hmm','tts'}\n",
    "for i in range(4):\n",
    "    for j in range(5):\n",
    "        file = sdeptest[i][j]\n",
    "        preds = [testlogProb(file,A_asr,mn_asr,cov_asr,initialProbabilities),\n",
    "                 testlogProb(file,A_cnn,mn_cnn,cov_cnn,initialProbabilities),\n",
    "                 testlogProb(file,A_dnn,mn_dnn,cov_dnn,initialProbabilities),\n",
    "                 testlogProb(file,A_hmm,mn_hmm,cov_hmm,initialProbabilities),\n",
    "                 testlogProb(file,A_tts,mn_tts,cov_tts,initialProbabilities)]\n",
    "        classifications[j][np.argmax(preds)] += 1\n",
    "\n",
    "print('Confusion Matrix\\n',classifications/4)\n",
    "print('Accuracy\\n',np.mean(np.diag(classifications/4)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indep hmm completed for word 0\n",
      "indep hmm completed for word 1\n",
      "indep hmm completed for word 2\n",
      "indep hmm completed for word 3\n",
      "indep hmm completed for word 4\n"
     ]
    }
   ],
   "source": [
    "#SPEAKER INDEPENDENT TRAIN\n",
    "asrmeani = [sdepasri_mn,sdepasri_mn,sdepasri_mn,sdepasri_mn,sdepasri_mn]\n",
    "asrcovai = [sdepasri_cov,sdepasri_cov,sdepasri_cov,sdepasri_cov,sdepasri_cov]\n",
    "cnnmeani = [sdepcnni_mn,sdepcnni_mn,sdepcnni_mn,sdepcnni_mn,sdepcnni_mn]\n",
    "cnncovai = [sdepcnni_cov,sdepcnni_cov,sdepcnni_cov,sdepcnni_cov,sdepcnni_cov]\n",
    "dnnmeani = [sdepdnni_mn,sdepdnni_mn,sdepdnni_mn,sdepdnni_mn,sdepdnni_mn]\n",
    "dnncovai = [sdepdnni_cov,sdepdnni_cov,sdepdnni_cov,sdepdnni_cov,sdepdnni_cov]\n",
    "hmmmeani = [sdephmmi_mn,sdephmmi_mn,sdephmmi_mn,sdephmmi_mn,sdephmmi_mn]\n",
    "hmmcovai = [sdephmmi_cov,sdephmmi_cov,sdephmmi_cov,sdephmmi_cov,sdephmmi_cov]\n",
    "ttsmeani = [sdepttsi_mn,sdepttsi_mn,sdepttsi_mn,sdepttsi_mn,sdepttsi_mn]\n",
    "ttscovai = [sdepttsi_cov,sdepttsi_cov,sdepttsi_cov,sdepttsi_cov,sdepttsi_cov]\n",
    "A_asri,mn_asri,cov_asri = SINDEPTRAIN(initialTransition,initialProbabilities,asrmeani,asrcovai,0,25,14, sindeptrain)\n",
    "A_cnni,mn_cnni,cov_cnni = SINDEPTRAIN(initialTransition,initialProbabilities,cnnmeani,cnncovai,1,25,14, sindeptrain)\n",
    "A_dnni,mn_dnni,cov_dnni = SINDEPTRAIN(initialTransition,initialProbabilities,dnnmeani,dnncovai,2,25,14, sindeptrain)\n",
    "A_hmmi,mn_hmmi,cov_hmmi = SINDEPTRAIN(initialTransition,initialProbabilities,hmmmeani,hmmcovai,3,25,14, sindeptrain)\n",
    "A_ttsi,mn_ttsi,cov_ttsi = SINDEPTRAIN(initialTransition,initialProbabilities,ttsmeani,ttscovai,4,25,14, sindeptrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\adithya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:59: RuntimeWarning: overflow encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification\n",
      " [[ 1.   0.   0.   0.   0. ]\n",
      " [ 0.6  0.2  0.2  0.   0. ]\n",
      " [ 0.2  0.   0.8  0.   0. ]\n",
      " [ 0.   1.   0.   0.   0. ]\n",
      " [ 0.2  0.   0.   0.   0.8]]\n",
      "Accuracy\n",
      " 0.56\n"
     ]
    }
   ],
   "source": [
    "#SPEAKER INDEPENDENT TEST\n",
    "#test[word][utt]\n",
    "classificationsi = np.zeros((5,5))\n",
    "#classifications[truth][prediction] where idx{0,1,2,3,4} => {'asr','cnn','dnn','hmm','tts'}\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        filei = sindeptest[i][j]\n",
    "        predsi = [testlogProb(filei,A_asri,mn_asri,cov_asri,initialProbabilities),\n",
    "                 testlogProb(filei,A_cnni,mn_cnni,cov_cnni,initialProbabilities),\n",
    "                 testlogProb(filei,A_dnni,mn_dnni,cov_dnni,initialProbabilities),\n",
    "                 testlogProb(filei,A_hmmi,mn_hmmi,cov_hmmi,initialProbabilities),\n",
    "                 testlogProb(filei,A_ttsi,mn_ttsi,cov_ttsi,initialProbabilities)]\n",
    "        classificationsi[i][np.argmax(predsi)] += 1\n",
    "\n",
    "print('Classification\\n',classificationsi/5)\n",
    "print('Accuracy\\n',np.mean(np.diag(classificationsi/5)))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on Recorded Voice\n",
    "## Feature Extraction for Python generated MFCC features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ FEATURE EXTRACTION FOR PYTHON GENERATED MFCC FEATURES ###########\n",
    "#speaker independent partitions\n",
    "#sindeptrain dimensions sdeptrain[speaker][word][utterance idx] with idxs given alphabetically\n",
    "py_sindeptrain = [[[[] for i in range(5)] for j in range(5)] for k in range(3)]\n",
    "#sindeptest dimensions sdeptest[word][utterance idx] with idxs given alphabetically\n",
    "py_sindeptest = [[[] for i in range(5)] for j in range(5)]\n",
    "py_p = './feature'\n",
    "py_speakerList2 = ['dg', 'ls', 'mh']#'yx'\n",
    "py_wordList = ['asr','cnn','dnn','hmm','tts']\n",
    "py_snum = 0\n",
    "for s in py_speakerList2:\n",
    "    py_speakerpathi = py_p+'/py_'+s\n",
    "    py_wnum = 0\n",
    "    for w in py_wordList:\n",
    "        py_wordpathi = py_speakerpathi +'/'+s+'_'+w\n",
    "        #set up train\n",
    "        for utt in range(1,6):\n",
    "            py_fpathi = py_wordpathi + str(utt)+'.fea'\n",
    "            py_fi = open(py_fpathi,'r')\n",
    "            py_datai = py_fi.readlines()\n",
    "            py_datai = [i.split(',') for i in py_datai]\n",
    "            for sample in range(len(py_datai)):\n",
    "                py_datai[sample] = [float(i) for i in py_datai[sample]]\n",
    "                #normalizing training data\n",
    "                py_datai[sample] = py_datai[sample]/np.linalg.norm(py_datai[sample])\n",
    "            for d in py_datai:\n",
    "                py_sindeptrain[py_snum][py_wnum][utt-1].append(d)\n",
    "        py_wnum += 1\n",
    "    py_snum += 1\n",
    "    \n",
    "        #set up test\n",
    "py_s2 = 'yx'\n",
    "py_speakertestpath = py_p+'/py_'+py_s2\n",
    "py_wnum = 0\n",
    "for w in py_wordList:\n",
    "    py_wordtestpath = py_speakertestpath +'/'+py_s2+'_'+w\n",
    "    for utt in range(1,6):\n",
    "        py_ftestpath = py_wordtestpath + str(utt)+'.fea'\n",
    "        py_fitest = open(py_ftestpath,'r')\n",
    "        py_dataitest = py_fitest.readlines()\n",
    "        py_dataitest = [i.split(',') for i in py_dataitest]\n",
    "        for sample in range(len(py_dataitest)):\n",
    "            py_dataitest[sample] = [float(i) for i in py_dataitest[sample]]\n",
    "            py_dataitest[sample] = py_dataitest[sample]/np.linalg.norm(py_dataitest[sample])\n",
    "        for d in py_dataitest:\n",
    "            py_sindeptest[py_wnum][utt-1].append(d)\n",
    "    py_wnum += 1\n",
    "\n",
    "py_sindeptrain = np.array(py_sindeptrain)\n",
    "py_sindeptest = np.array(py_sindeptest)\n",
    "\n",
    "py_bigasri = []\n",
    "py_bigcnni = []\n",
    "py_bigdnni = []\n",
    "py_bighmmi = []\n",
    "py_bigttsi = []\n",
    "for i in range(3):\n",
    "    for j in range(5):\n",
    "        py_bigasri.extend(py_sindeptrain[i][0][j])\n",
    "        py_bigcnni.extend(py_sindeptrain[i][1][j])\n",
    "        py_bigdnni.extend(py_sindeptrain[i][2][j])\n",
    "        py_bighmmi.extend(py_sindeptrain[i][3][j])\n",
    "        py_bigttsi.extend(py_sindeptrain[i][4][j])\n",
    "py_sdepasri_cov = np.diag(np.diag(np.cov(np.array(py_bigasri).T)))\n",
    "py_sdepcnni_cov = np.diag(np.diag(np.cov(np.array(py_bigcnni).T)))\n",
    "py_sdepdnni_cov = np.diag(np.diag(np.cov(np.array(py_bigdnni).T)))\n",
    "py_sdephmmi_cov = np.diag(np.diag(np.cov(np.array(py_bighmmi).T)))\n",
    "py_sdepttsi_cov = np.diag(np.diag(np.cov(np.array(py_bigttsi).T)))\n",
    "py_sdepasri_mn = np.mean(py_bigasri,axis=0)\n",
    "py_sdepcnni_mn = np.mean(py_bigcnni,axis=0)\n",
    "py_sdepdnni_mn = np.mean(py_bigdnni,axis=0)\n",
    "py_sdephmmi_mn = np.mean(py_bighmmi,axis=0)\n",
    "py_sdepttsi_mn = np.mean(py_bigttsi,axis=0)\n",
    "# print(len(py_sindeptrain[0][0][0][0]))\n",
    "# print(py_sdepasri_cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Speaker Independent on python genereated MFCC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indep hmm completed for word 0\n",
      "indep hmm completed for word 1\n",
      "indep hmm completed for word 2\n",
      "indep hmm completed for word 3\n",
      "indep hmm completed for word 4\n"
     ]
    }
   ],
   "source": [
    "#SPEAKER INDEPENDENT TRAIN\n",
    "py_asrmeani = [py_sdepasri_mn,py_sdepasri_mn,py_sdepasri_mn,py_sdepasri_mn,py_sdepasri_mn]\n",
    "py_asrcovai = [py_sdepasri_cov,py_sdepasri_cov,py_sdepasri_cov,py_sdepasri_cov,py_sdepasri_cov]\n",
    "py_cnnmeani = [py_sdepcnni_mn,py_sdepcnni_mn,py_sdepcnni_mn,py_sdepcnni_mn,py_sdepcnni_mn]\n",
    "py_cnncovai = [py_sdepcnni_cov,py_sdepcnni_cov,py_sdepcnni_cov,py_sdepcnni_cov,py_sdepcnni_cov]\n",
    "py_dnnmeani = [py_sdepdnni_mn,py_sdepdnni_mn,py_sdepdnni_mn,py_sdepdnni_mn,py_sdepdnni_mn]\n",
    "py_dnncovai = [py_sdepdnni_cov,py_sdepdnni_cov,py_sdepdnni_cov,py_sdepdnni_cov,py_sdepdnni_cov]\n",
    "py_hmmmeani = [py_sdephmmi_mn,py_sdephmmi_mn,py_sdephmmi_mn,py_sdephmmi_mn,py_sdephmmi_mn]\n",
    "py_hmmcovai = [py_sdephmmi_cov,py_sdephmmi_cov,py_sdephmmi_cov,py_sdephmmi_cov,py_sdephmmi_cov]\n",
    "py_ttsmeani = [py_sdepttsi_mn,py_sdepttsi_mn,py_sdepttsi_mn,py_sdepttsi_mn,py_sdepttsi_mn]\n",
    "py_ttscovai = [py_sdepttsi_cov,py_sdepttsi_cov,py_sdepttsi_cov,py_sdepttsi_cov,py_sdepttsi_cov]\n",
    "py_A_asri,py_mn_asri,py_cov_asri = SINDEPTRAIN(initialTransition,initialProbabilities,py_asrmeani,py_asrcovai,0,25,14,py_sindeptrain)\n",
    "py_A_cnni,py_mn_cnni,py_cov_cnni = SINDEPTRAIN(initialTransition,initialProbabilities,py_cnnmeani,py_cnncovai,1,25,14,py_sindeptrain)\n",
    "py_A_dnni,py_mn_dnni,py_cov_dnni = SINDEPTRAIN(initialTransition,initialProbabilities,py_dnnmeani,py_dnncovai,2,25,14,py_sindeptrain)\n",
    "py_A_hmmi,py_mn_hmmi,py_cov_hmmi = SINDEPTRAIN(initialTransition,initialProbabilities,py_hmmmeani,py_hmmcovai,3,25,14,py_sindeptrain)\n",
    "py_A_ttsi,py_mn_ttsi,py_cov_ttsi = SINDEPTRAIN(initialTransition,initialProbabilities,py_ttsmeani,py_ttscovai,4,25,14,py_sindeptrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\adithya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:59: RuntimeWarning: overflow encountered in double_scalars\n",
      "c:\\users\\adithya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:59: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-23187.940744276118, -23264.339042853164, -20325.786658585221, -15604.562403920221, -18667.387289781167]\n",
      "[-28575.038009401538, -28281.825071188057, -25638.959716546709, -19225.415901204269, -22814.788792711639]\n",
      "[-27312.365223374931, -26502.464761314477, -23994.241010773985, -17959.646806842913, -21755.114404550881]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\adithya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:52: RuntimeWarning: divide by zero encountered in log\n",
      "c:\\users\\adithya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:53: RuntimeWarning: invalid value encountered in true_divide\n",
      "c:\\users\\adithya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:55: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-26648.102430315932, -26103.16494999432, -22819.513087620453, -17191.653344745013, -20792.990596113719]\n",
      "[-38224.962738793984, -36901.22909767172, -31298.69141470499, -23828.226556382848, -30217.026489409163]\n",
      "[[ 0.   0.   0.   1.   0. ]\n",
      " [ 0.   0.   0.   1.   0. ]\n",
      " [ 0.   0.   0.   1.   0. ]\n",
      " [ 0.4  0.   0.   0.6  0. ]\n",
      " [ 0.2  0.   0.   0.8  0. ]]\n",
      "0.12\n"
     ]
    }
   ],
   "source": [
    "#SPEAKER INDEPENDENT TEST\n",
    "#test[word][utt]\n",
    "py_classificationsi = np.zeros((5,5))\n",
    "#classifications[truth][prediction] where idx{0,1,2,3,4} => {'asr','cnn','dnn','hmm','tts'}\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        py_filei = py_sindeptest[i][j]\n",
    "        py_predsi = [testlogProb(py_filei,py_A_asri,py_mn_asri,py_cov_asri,initialProbabilities),\n",
    "                 testlogProb(py_filei,py_A_cnni,py_mn_cnni,py_cov_cnni,initialProbabilities),\n",
    "                 testlogProb(py_filei,py_A_dnni,py_mn_dnni,py_cov_dnni,initialProbabilities),\n",
    "                 testlogProb(py_filei,py_A_hmmi,py_mn_hmmi,py_cov_hmmi,initialProbabilities),\n",
    "                 testlogProb(py_filei,py_A_ttsi,py_mn_ttsi,py_cov_ttsi,initialProbabilities)]\n",
    "        py_classificationsi[i][np.argmax(py_predsi)] += 1\n",
    "    print(py_predsi)\n",
    "\n",
    "print(py_classificationsi/5)\n",
    "print(np.mean(np.diag(py_classificationsi/5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\adithya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:52: RuntimeWarning: divide by zero encountered in log\n",
      "c:\\users\\adithya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:53: RuntimeWarning: invalid value encountered in true_divide\n",
      "c:\\users\\adithya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:46: RuntimeWarning: divide by zero encountered in log\n",
      "c:\\users\\adithya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:47: RuntimeWarning: invalid value encountered in true_divide\n",
      "c:\\users\\adithya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:55: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "c:\\users\\adithya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:59: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      " [[ 0.   0.2  0.6  0.   0.2]\n",
      " [ 0.2  0.   0.4  0.   0.4]\n",
      " [ 0.2  0.2  0.6  0.   0. ]\n",
      " [ 0.   0.6  0.2  0.   0.2]\n",
      " [ 0.   0.6  0.4  0.   0. ]]\n",
      "Accuracy\n",
      " 0.12\n"
     ]
    }
   ],
   "source": [
    "#access voicetest[word][utt]\n",
    "voicetest = [[[] for i in range(5)] for j in range(5)]\n",
    "p = './feature/self_recf'\n",
    "wordList = ['asr','cnn','dnn','hmm','tts']\n",
    "\n",
    "speakertestpath = p\n",
    "wnum = 0\n",
    "for w in wordList:\n",
    "    wordtestpath = speakertestpath +'/'+w\n",
    "    for utt in range(1,6):\n",
    "        ftestpath = wordtestpath + str(utt)+'.fea'\n",
    "        fitest = open(ftestpath,'r')\n",
    "        dataitest = fitest.readlines()\n",
    "        dataitest = [i.split(',') for i in dataitest]\n",
    "        for sample in range(len(dataitest)):\n",
    "            dataitest[sample] = [float(i) for i in dataitest[sample]]\n",
    "            dataitest[sample] = dataitest[sample]/np.linalg.norm(dataitest[sample])\n",
    "#             print(dataitest)\n",
    "\n",
    "        for d in dataitest:\n",
    "            voicetest[wnum][utt-1].append(d)\n",
    "    wnum += 1\n",
    "\n",
    "voicetest = np.array(voicetest)\n",
    "#SPEAKER INDEPENDENT TEST\n",
    "#test[word][utt]\n",
    "py_classificationsvoice = np.zeros((5,5))\n",
    "#classifications[truth][prediction] where idx{0,1,2,3,4} => {'asr','cnn','dnn','hmm','tts'}\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        filei = voicetest[i][j]\n",
    "        py_predsi = [testlogProb(filei,py_A_asri,py_mn_asri,py_cov_asri,initialProbabilities),\n",
    "                 testlogProb(filei,py_A_cnni,py_mn_cnni,py_cov_cnni,initialProbabilities),\n",
    "                 testlogProb(filei,py_A_dnni,py_mn_dnni,py_cov_dnni,initialProbabilities),\n",
    "                 testlogProb(filei,py_A_hmmi,py_mn_hmmi,py_cov_hmmi,initialProbabilities),\n",
    "                 testlogProb(filei,py_A_ttsi,py_mn_ttsi,py_cov_ttsi,initialProbabilities)]\n",
    "        py_classificationsvoice[i][np.argmax(py_predsi)] += 1\n",
    "#         print(predsi)\n",
    "\n",
    "print('Confusion Matrix\\n',py_classificationsvoice/5)\n",
    "print('Accuracy\\n',np.mean(np.diag(py_classificationsi/5)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Comparing the speaker independent and dependent results, we found that the speaker dependent accuracy did better. This makes sense, because with this way, the HMM was able to train on utterances from all the speakers. Each speaker has their own personal sound and way they pronounce certain words. Some people have accents we well, and that can impact how a certain word sounds when they say it. Therefore, it is a good idea to train each HMM on each speaker as well. That way, the model parameters that are produced can represent a variety voices. Speaker dependent training trains the HMM in a more robust fashion.\n",
    "\n",
    "Upon further inspection, it makes sense why the speaker independent HMM performed worse than the speaker dependent HMMs.\n",
    "\n",
    "One reason why the accuracy for our our voice testing is a bit lower is because we did not have access to the Matlab mfcc fucntion. Therefore, we had to use a python library that generated the mfcc coefficients. However, when we tried to recreate the Matlab mfcc values using the python library, we were unable to. There must be something different with the python library and it is possible it is not able to capture the same information from the wav file we pass into it. However, to be consistent, we train the independent HMM parameters on the mfcc values from the python library. We run that library on the wav files given to us and use those valued instead of the given mfcc values. If we had access to the Matlab mfcc function we would be able to recreate the right values. Another reason why the accuracy for this specific test is low is because training the HMM independent of the speaker is not a good idea. It matters very much who the speaker is because as mentioned before, everyone has their own way of saying the same words. Their intonation and the time they spend on each syllable is very different from other people. So, if we want our HMM to have the best performance, we should train it on as many voices as possible. The speaker does matter a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction for EC\n",
    "For the extra credit we have opted to use the Mel Frequency Spectral Coefficients. Since the MFCC comes from taking the IDFT of the of the MFSC, we obtained the MFSC by taking the fft of the MFCC features given to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#speaker dependent partitions\n",
    "#sdeptrain dimensions sdeptrain[speaker][word][utterance idx] with idxs given alphabetically\n",
    "ec_sdeptrain = [[[[] for i in range(4)] for j in range(5)] for k in range(4)]\n",
    "#ec_sdeptest dimensions sdeptrain[speaker][word] with idxs given alphabetically\n",
    "ec_sdeptest = [[[] for i in range(5)] for j in range(4)]\n",
    "p = './feature'\n",
    "speakerListec = ['dg', 'ls', 'mh', 'yx']\n",
    "wordListec = ['asr','cnn','dnn','hmm','tts']\n",
    "snum = 0\n",
    "for s in speakerListec:\n",
    "    speakerpathec = p+'/'+s\n",
    "    wnum = 0\n",
    "    for w in wordListec:\n",
    "        wordpathec = speakerpathec +'/'+s+'_'+w\n",
    "        #set up train\n",
    "        for utt in range(1,5):\n",
    "            fpathec = wordpathec + str(utt)+'.fea'\n",
    "            fec = open(fpathec,'r')\n",
    "            data = fec.readlines()\n",
    "            data = [i.split(',') for i in data]\n",
    "            for sample in range(len(data)):\n",
    "                data[sample] = [float(i) for i in data[sample]]\n",
    "                #normalize\n",
    "                data[sample] = data[sample]/np.linalg.norm(data[sample])\n",
    "                \n",
    "            for d in data:\n",
    "                ec_sdeptrain[snum][wnum][utt-1].append(np.real(np.fft.fft(d)))\n",
    "        #set up test\n",
    "        fpath2ec = wordpathec + str(5)+'.fea'\n",
    "        f2ec = open(fpath2ec,'r')\n",
    "        dataec = f2ec.readlines()\n",
    "        dataec = [i.split(',') for i in dataec]\n",
    "        for d in dataec:\n",
    "            ec_sdeptest[snum][wnum].append(np.real(np.fft.fft(np.array(d).astype(np.float))))\n",
    "        wnum += 1\n",
    "    snum += 1\n",
    "ec_sdeptrain = np.array(ec_sdeptrain)\n",
    "\n",
    "\n",
    "ec_bigasr = []\n",
    "ec_bigcnn = []\n",
    "ec_bigdnn = []\n",
    "ec_bighmm = []\n",
    "ec_bigtts = []\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        ec_bigasr.extend(ec_sdeptrain[i][0][j])\n",
    "        ec_bigcnn.extend(ec_sdeptrain[i][1][j])\n",
    "        ec_bigdnn.extend(ec_sdeptrain[i][2][j])\n",
    "        ec_bighmm.extend(ec_sdeptrain[i][3][j])\n",
    "        ec_bigtts.extend(ec_sdeptrain[i][4][j])\n",
    "# ec_sdepasr_cov = (np.cov(np.array(ec_bigasr).T)+0.3*np.identity(14))\n",
    "# ec_sdepcnn_cov = (np.cov(np.array(ec_bigcnn).T)+0.3*np.identity(14))\n",
    "# ec_sdepdnn_cov = (np.cov(np.array(ec_bigdnn).T)+0.3*np.identity(14))\n",
    "# ec_sdephmm_cov = (np.cov(np.array(ec_bighmm).T)+0.3*np.identity(14))\n",
    "# ec_sdeptts_cov = (np.cov(np.array(ec_bigtts).T)+0.3*np.identity(14))\n",
    "\n",
    "ec_sdepasr_cov = np.diag(np.diag(np.cov(np.array(ec_bigasr).T)))\n",
    "ec_sdepcnn_cov = np.diag(np.diag(np.cov(np.array(ec_bigcnn).T)))\n",
    "ec_sdepdnn_cov = np.diag(np.diag(np.cov(np.array(ec_bigdnn).T)))\n",
    "ec_sdephmm_cov = np.diag(np.diag(np.cov(np.array(ec_bighmm).T)))\n",
    "ec_sdeptts_cov = np.diag(np.diag(np.cov(np.array(ec_bigtts).T)))\n",
    "\n",
    "\n",
    "ec_sdepasr_mn = np.mean(ec_bigasr,axis=0)\n",
    "ec_sdepcnn_mn = np.mean(ec_bigcnn,axis=0)\n",
    "ec_sdepdnn_mn = np.mean(ec_bigdnn,axis=0)\n",
    "ec_sdephmm_mn = np.mean(ec_bighmm,axis=0)\n",
    "ec_sdeptts_mn = np.mean(ec_bigtts,axis=0)\n",
    "# print(ec_sdepasr_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-3fddd0c158ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mec_ttscova\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mec_sdeptts_cov\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mec_sdeptts_cov\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mec_sdeptts_cov\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mec_sdeptts_cov\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mec_sdeptts_cov\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# print(ec_asrcova)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mec_A_asr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mec_mn_asr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mec_cov_asr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSDEPTRAIN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitialTransition\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minitialProbabilities\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mec_asrmean\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mec_asrcova\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mec_sdeptrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'did 1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mec_A_cnn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mec_mn_cnn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mec_cov_cnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSDEPTRAIN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitialTransition\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minitialProbabilities\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mec_cnnmean\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mec_cnncova\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mec_sdeptrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-4e1c13b0be7a>\u001b[0m in \u001b[0;36mSDEPTRAIN\u001b[1;34m(initialTransition, initialProbabilities, initialmeans, initialcov, wordidx, numiterations, numcoeff, Sdeptrain)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;31m#for each speaker\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mu\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# for each utterance\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                 \u001b[0mB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculateBMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSdeptrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwordidx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcov\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                 \u001b[0mAlpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mBeta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlogg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculateAlphaBetaTildeG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minitialProbabilities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[0mGamma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculateGammaMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAlpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mBeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-afccbb000942>\u001b[0m in \u001b[0;36mcalculateBMatrix\u001b[1;34m(X, mu, sigma, N)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mtemp_frame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mbMatrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultivariate_normal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcov\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msigma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mbMatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\adithya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\scipy\\stats\\_multivariate.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, mean, cov, allow_singular, seed)\u001b[0m\n\u001b[0;32m    355\u001b[0m         return multivariate_normal_frozen(mean, cov,\n\u001b[0;32m    356\u001b[0m                                           \u001b[0mallow_singular\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallow_singular\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m                                           seed=seed)\n\u001b[0m\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_process_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\adithya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\scipy\\stats\\_multivariate.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, mean, cov, allow_singular, seed, maxpts, abseps, releps)\u001b[0m\n\u001b[0;32m    725\u001b[0m         self.dim, self.mean, self.cov = self._dist._process_parameters(\n\u001b[0;32m    726\u001b[0m                                                             None, mean, cov)\n\u001b[1;32m--> 727\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcov_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_PSD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcov\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_singular\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallow_singular\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmaxpts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m             \u001b[0mmaxpts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000000\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\adithya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\scipy\\stats\\_multivariate.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, M, cond, rcond, lower, check_finite, allow_singular)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_singular\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinAlgError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'singular matrix'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m         \u001b[0ms_pinv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_pinv_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[0mU\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms_pinv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLinAlgError\u001b[0m: singular matrix"
     ]
    }
   ],
   "source": [
    "#TRAINING FOR SPEAKER DEPENDENT FOR EXTRA CREDIT\n",
    "ec_asrmean = [ec_sdepasr_mn,ec_sdepasr_mn,ec_sdepasr_mn,ec_sdepasr_mn,ec_sdepasr_mn]\n",
    "ec_asrcova = [ec_sdepasr_cov,ec_sdepasr_cov,ec_sdepasr_cov,ec_sdepasr_cov,ec_sdepasr_cov]\n",
    "ec_cnnmean = [ec_sdepcnn_mn,ec_sdepcnn_mn,ec_sdepcnn_mn,ec_sdepcnn_mn,ec_sdepcnn_mn]\n",
    "ec_cnncova = [ec_sdepcnn_cov,ec_sdepcnn_cov,ec_sdepcnn_cov,ec_sdepcnn_cov,ec_sdepcnn_cov]\n",
    "ec_dnnmean = [ec_sdepdnn_mn,ec_sdepdnn_mn,ec_sdepdnn_mn,ec_sdepdnn_mn,ec_sdepdnn_mn]\n",
    "ec_dnncova = [ec_sdepdnn_cov,ec_sdepdnn_cov,ec_sdepdnn_cov,ec_sdepdnn_cov,ec_sdepdnn_cov]\n",
    "ec_hmmmean = [ec_sdephmm_mn,ec_sdephmm_mn,ec_sdephmm_mn,ec_sdephmm_mn,ec_sdephmm_mn]\n",
    "ec_hmmcova = [ec_sdephmm_cov,ec_sdephmm_cov,ec_sdephmm_cov,ec_sdephmm_cov,ec_sdephmm_cov]\n",
    "ec_ttsmean = [ec_sdeptts_mn,ec_sdeptts_mn,ec_sdeptts_mn,ec_sdeptts_mn,ec_sdeptts_mn]\n",
    "ec_ttscova = [ec_sdeptts_cov,ec_sdeptts_cov,ec_sdeptts_cov,ec_sdeptts_cov,ec_sdeptts_cov]\n",
    "# print(ec_asrcova)\n",
    "ec_A_asr,ec_mn_asr,ec_cov_asr = SDEPTRAIN(initialTransition,initialProbabilities,ec_asrmean,ec_asrcova,0,25,14,ec_sdeptrain)\n",
    "print('did 1')\n",
    "ec_A_cnn,ec_mn_cnn,ec_cov_cnn = SDEPTRAIN(initialTransition,initialProbabilities,ec_cnnmean,ec_cnncova,1,25,14,ec_sdeptrain)\n",
    "print('did 1')\n",
    "ec_A_dnn,ec_mn_dnn,ec_cov_dnn = SDEPTRAIN(initialTransition,initialProbabilities,ec_dnnmean,ec_dnncova,2,25,14,ec_sdeptrain)\n",
    "print('did 1')\n",
    "ec_A_hmm,ec_mn_hmm,ec_cov_hmm = SDEPTRAIN(initialTransition,initialProbabilities,ec_hmmmean,ec_hmmcova,3,25,14,ec_sdeptrain)\n",
    "print('did 1')\n",
    "ec_A_tts,ec_mn_tts,ec_cov_tts = SDEPTRAIN(initialTransition,initialProbabilities,ec_ttsmean,ec_ttscova,4,25,14,ec_sdeptrain)\n",
    "print('did 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPEAKER DEPENDENT TEST FOR EXTRA CREDIT\n",
    "ec_classifications = np.zeros((5,5))\n",
    "#classifications[truth][prediction] where idx{0,1,2,3,4} => {'asr','cnn','dnn','hmm','tts'}\n",
    "for i in range(4):\n",
    "    for j in range(5):\n",
    "        ec_file = ec_sdeptrain[0][i][j]\n",
    "        ec_preds = [testlogProb(ec_file,ec_A_asr,ec_mn_asr,ec_cov_asr,initialProbabilities),\n",
    "                 testlogProb(ec_file,ec_A_cnn,ec_mn_cnn,ec_cov_cnn,initialProbabilities),\n",
    "                 testlogProb(ec_file,ec_A_dnn,ec_mn_dnn,ec_cov_dnn,initialProbabilities),\n",
    "                 testlogProb(ec_file,ec_A_hmm,ec_mn_hmm,ec_cov_hmm,initialProbabilities),\n",
    "                 testlogProb(ec_file,ec_A_tts,ec_mn_tts,ec_cov_tts,initialProbabilities)]\n",
    "        print(ec_preds)\n",
    "        ec_classifications[j][np.argmax(ec_preds)] += 1\n",
    "\n",
    "print('Confusion Matrix\\n',ec_classifications)\n",
    "print('Accuracy\\n',np.mean(np.diag(ec_classifications/5)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
