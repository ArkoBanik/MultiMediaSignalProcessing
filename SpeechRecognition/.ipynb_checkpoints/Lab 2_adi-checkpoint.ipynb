{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speaker dependent feature extraction\n",
    "We assign the 5th utterance of each word of each speaker to the test set. All else will be used to train our HMM on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#speaker dependent partitions\n",
    "#sdeptrain dimensions sdeptrain[speaker][word][utterance idx] with idxs given alphabetically\n",
    "sdeptrain = [[[[] for i in range(4)] for j in range(5)] for k in range(4)]\n",
    "#sdeptest dimensions sdeptrain[speaker][word] with idxs given alphabetically\n",
    "sdeptest = [[[] for i in range(5)] for j in range(4)]\n",
    "p = './feature'\n",
    "speakerList = ['dg', 'ls', 'mh', 'yx']\n",
    "wordList = ['asr','cnn','dnn','hmm','tts']\n",
    "snum = 0\n",
    "for s in speakerList:\n",
    "    speakerpath = p+'/'+s\n",
    "    wnum = 0\n",
    "    for w in wordList:\n",
    "        wordpath = speakerpath +'/'+s+'_'+w\n",
    "        #set up train\n",
    "        for utt in range(1,5):\n",
    "            fpath = wordpath + str(utt)+'.fea'\n",
    "            f = open(fpath,'r')\n",
    "            data = f.readlines()\n",
    "            data = [i.split(',') for i in data]\n",
    "            for sample in range(len(data)):\n",
    "                data[sample] = [float(i) for i in data[sample]]\n",
    "            for d in data:\n",
    "                sdeptrain[snum][wnum][utt-1].append(d)\n",
    "        #set up test\n",
    "        fpath2 = wordpath + str(5)+'.fea'\n",
    "        f2 = open(fpath2,'r')\n",
    "        data = f2.readlines()\n",
    "        data = [i.split(',') for i in data]\n",
    "        for d in data:\n",
    "            sdeptest[snum][wnum].append(d)\n",
    "        wnum += 1\n",
    "    snum += 1\n",
    "sdeptest = np.array(sdeptest)\n",
    "sdeptrain = np.array(sdeptrain)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the mean and covariance matrices\n",
    "\n",
    "For this specific lab, we are modeling each of our 5 states as Gaussians. Therefore, each state will be characterized by a mean matrix and a covariance matrix. Below, we initialize these matrices across all utterances of that word in the training set. Each state will start out with the same mean and covariance matrix. Throughout the training process, these values will be updated and tuned to give our HMM the optimal set of parameters.\n",
    "\n",
    "We choose to use the diagonal covariance matrix instead of the full covariance matrix. We make this design choice because when we tried to use the full matrix, our covariance matrix did not stay positive semidefinite after a number of iterations. Only taking the values across the diagonal also greatly helped our numerical stability of the HMM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigasr = []\n",
    "bigcnn = []\n",
    "bigdnn = []\n",
    "bighmm = []\n",
    "bigtts = []\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        bigasr.extend(sdeptrain[i][0][j])\n",
    "        bigcnn.extend(sdeptrain[i][1][j])\n",
    "        bigdnn.extend(sdeptrain[i][2][j])\n",
    "        bighmm.extend(sdeptrain[i][3][j])\n",
    "        bigtts.extend(sdeptrain[i][4][j])\n",
    "sdepasr_cov = np.diag(np.diag(np.cov(np.array(bigasr).T)))\n",
    "sdepcnn_cov = np.diag(np.diag(np.cov(np.array(bigcnn).T)))\n",
    "sdepdnn_cov = np.diag(np.diag(np.cov(np.array(bigdnn).T)))\n",
    "sdephmm_cov = np.diag(np.diag(np.cov(np.array(bighmm).T)))\n",
    "sdeptts_cov = np.diag(np.diag(np.cov(np.array(bigtts).T)))\n",
    "sdepasr_mn = np.mean(bigasr,axis=0)\n",
    "sdepcnn_mn = np.mean(bigcnn,axis=0)\n",
    "sdepdnn_mn = np.mean(bigdnn,axis=0)\n",
    "sdephmm_mn = np.mean(bighmm,axis=0)\n",
    "sdeptts_mn = np.mean(bigtts,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speaker Independent Feature Extraction\n",
    "All utterances of speaker mh will be used as test data. The rest will be assigned as training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#speaker independent partitions\n",
    "#sindeptrain dimensions sdeptrain[speaker][word][utterance idx] with idxs given alphabetically\n",
    "sindeptrain = [[[[] for i in range(5)] for j in range(5)] for k in range(3)]\n",
    "#sindeptest dimensions sdeptest[word][utterance idx] with idxs given alphabetically\n",
    "sindeptest = [[[] for i in range(5)] for j in range(5)]\n",
    "p = './feature'\n",
    "speakerList2 = ['dg', 'ls', 'mh']#'mh'\n",
    "wordList = ['asr','cnn','dnn','hmm','tts']\n",
    "snum = 0\n",
    "for s in speakerList2:\n",
    "    speakerpathi = p+'/'+s\n",
    "    wnum = 0\n",
    "    for w in wordList:\n",
    "        wordpathi = speakerpathi +'/'+s+'_'+w\n",
    "        #set up train\n",
    "        for utt in range(1,6):\n",
    "            fpathi = wordpathi + str(utt)+'.fea'\n",
    "            fi = open(fpathi,'r')\n",
    "            datai = fi.readlines()\n",
    "            datai = [i.split(',') for i in datai]\n",
    "            for sample in range(len(datai)):\n",
    "                datai[sample] = [float(i) for i in datai[sample]]\n",
    "            for d in datai:\n",
    "                sindeptrain[snum][wnum][utt-1].append(d)\n",
    "        wnum += 1\n",
    "    snum += 1\n",
    "    \n",
    "#set up test\n",
    "s2 = 'yx'\n",
    "speakertestpath = p+'/'+s2\n",
    "wnum = 0\n",
    "for w in wordList:\n",
    "    wordtestpath = speakertestpath +'/'+s2+'_'+w\n",
    "    for utt in range(1,6):\n",
    "        ftestpath = wordtestpath + str(utt)+'.fea'\n",
    "        fitest = open(ftestpath,'r')\n",
    "        dataitest = fitest.readlines()\n",
    "        dataitest = [i.split(',') for i in dataitest]\n",
    "        for sample in range(len(dataitest)):\n",
    "            dataitest[sample] = [float(i) for i in dataitest[sample]]\n",
    "        for d in dataitest:\n",
    "            sindeptest[wnum][utt-1].append(d)\n",
    "    wnum += 1\n",
    "\n",
    "sindeptrain = np.array(sindeptrain)\n",
    "sindeptest = np.array(sindeptest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the mean and covariance matrices in the same manner as we did for the speaker dependent method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigasri = []\n",
    "bigcnni = []\n",
    "bigdnni = []\n",
    "bighmmi = []\n",
    "bigttsi = []\n",
    "for i in range(3):\n",
    "    for j in range(5):\n",
    "        bigasri.extend(sindeptrain[i][0][j])\n",
    "        bigcnni.extend(sindeptrain[i][1][j])\n",
    "        bigdnni.extend(sindeptrain[i][2][j])\n",
    "        bighmmi.extend(sindeptrain[i][3][j])\n",
    "        bigttsi.extend(sindeptrain[i][4][j])\n",
    "sdepasri_cov = np.diag(np.diag(np.cov(np.array(bigasri).T)))\n",
    "sdepcnni_cov = np.diag(np.diag(np.cov(np.array(bigcnni).T)))\n",
    "sdepdnni_cov = np.diag(np.diag(np.cov(np.array(bigdnni).T)))\n",
    "sdephmmi_cov = np.diag(np.diag(np.cov(np.array(bighmmi).T)))\n",
    "sdepttsi_cov = np.diag(np.diag(np.cov(np.array(bigttsi).T)))\n",
    "sdepasri_mn = np.mean(bigasri,axis=0)\n",
    "sdepcnni_mn = np.mean(bigcnni,axis=0)\n",
    "sdepdnni_mn = np.mean(bigdnni,axis=0)\n",
    "sdephmmi_mn = np.mean(bighmmi,axis=0)\n",
    "sdepttsi_mn = np.mean(bigttsi,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Probability and Transition Probability DIstribution\n",
    "\n",
    "The initial probability matrix defines the probability of which state the process starts in. Since we have 5 states per HMM and we want there to be an equal chance that each state is the first state, each state will have 20%.\n",
    "\n",
    "The transition probability will define how we move from state to state. Since we are training a left-to-right nonskip HMM, then the only option for a next state is just the next one. It will move on with probability 20% and stay in the same state 80%. These are the only two options. The transition probability matrix will be updated as we move through the training process.\n",
    "\n",
    "These two matrices as well as the mean and covariance matrices we just computed will be the 4 parameters that define an HMM at a given time. These parameters are often denoted as $\\Theta = (\\pi, A, \\mu, \\sigma)$, with $\\pi$ being the initial state probability distribution and A being the transition probability matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialProbabilities = [0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "\n",
    "initialTransition = [[0.8,0.2,  0,  0,   0],\n",
    "                     [  0,0.8,0.2,  0,   0],\n",
    "                     [  0,  0,0.8,0.2,   0],\n",
    "                     [  0,  0,  0,0.8, 0.2],\n",
    "                     [  0,  0,  0,  0,   1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM Training Helper Functions\n",
    "\n",
    "Now that we have the parameters that define the HMM at a particular time, we need to define how these parameters are going to be updated. We will train each HMM according to the Baum-Welch algorithm at this link here: https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm. \n",
    "\n",
    "The helper functions below generate the $\\alpha, \\beta, \\gamma,$ and $\\xi$ values that we need to update our initial probabilities $\\pi$ and transition matrix A. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### HELPER FUNCTIONSSSSS\n",
    "#implemented from https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm\n",
    "\n",
    "#Forward Pass\n",
    "def calculateBMatrix(X, mu, sigma,N):\n",
    "    #X is the data in a file\n",
    "    #mu is the average across all states \n",
    "    #sigma is the covariance matrix across all states \n",
    "    #N is the number of states\n",
    "    #BMatrix = [T,N]\n",
    "    #print(sigma[0])\n",
    "    T = len(X)\n",
    "    bMatrix = np.zeros((N,T))\n",
    "    for frame in range(T):\n",
    "        temp_frame = []\n",
    "        for state in range(N):\n",
    "            bMatrix[state,frame] = stats.multivariate_normal(mean=(mu[state]),cov=sigma[state]).pdf(X[frame])\n",
    "    return bMatrix\n",
    "\n",
    "def calculateAlphaMatrix(transMatrix, BMatrix,priors):\n",
    "    #transMatrix is A \n",
    "    #BMatrix is B\n",
    "    #priors is pi\n",
    "    #populate our alpha matrix\n",
    "    T = len(BMatrix[0])\n",
    "    N = len(BMatrix)\n",
    "    Amat = np.array(transMatrix)\n",
    "    alpha = np.zeros((N,T))\n",
    "    #alphaMatrix = [T,N]\n",
    "    for i in range(N):\n",
    "        alpha[i,0]=priors[i]*BMatrix[i,0]\n",
    "    for t in range(1,T):\n",
    "        for i in range(0,N):\n",
    "            alpha[i,t]=BMatrix[i,t]*np.inner(alpha[:,t-1],Amat[:,i])       \n",
    "    return alpha\n",
    "\n",
    "def calculateAlphaBetaTildeG(transMatrix,BMatrix,priors):\n",
    "    T = len(BMatrix[0])\n",
    "    N = len(BMatrix)\n",
    "    baralpha = np.zeros((N,T))\n",
    "    tildealpha = np.zeros((N,T))\n",
    "    tildebeta = np.zeros((N,T))\n",
    "    log_g = np.zeros((T))\n",
    "    for i in range(0,N):\n",
    "        baralpha[i,0]=priors[i]*BMatrix[i,0]\n",
    "    log_g[0] = np.log(np.sum(baralpha[:,0]))\n",
    "    tildealpha[:,0]=baralpha[:,0]/np.exp(log_g[0])\n",
    "\n",
    "    for t in range(1,T):\n",
    "        for i in range(0,N):\n",
    "            baralpha[i,t]=BMatrix[i,t]*np.inner(tildealpha[:,t-1],transMatrix[:,i])\n",
    "        log_g[t] = np.log(np.sum(baralpha[:,t]))\n",
    "        tildealpha[:,t]=baralpha[:,t]/np.exp(log_g[t])\n",
    "    for i in range(0,N):\n",
    "        tildebeta[i,T-1] = 1/np.exp(log_g[T-1])\n",
    "\n",
    "    for t in range(T-2,-1,-1):\n",
    "        for i in range(0,N):\n",
    "            tildebeta[i,t]=np.inner(transMatrix[i,0:N],tildebeta[:,t+1]*BMatrix[:,t+1])/np.exp(log_g[t+1])\n",
    "    \n",
    "    return tildealpha,tildebeta,log_g\n",
    "#Backward Pass\n",
    "    \n",
    "def calculateBetaMatrix(transMatrix, bMatrix):\n",
    "    #populate the Beta matrix\n",
    "    T = len(bMatrix[0])\n",
    "    N = len(bMatrix)\n",
    "    beta = np.zeros((N,T))\n",
    "    for i in range(N):\n",
    "        beta[i,T-1]=1\n",
    "    for t in range(T-2,-1,-1):\n",
    "        for i in range(0,N):\n",
    "            beta[i,t]=np.inner(transMatrix[i,0:N],beta[:,t+1]*bMatrix[:,t+1])\n",
    "    return beta\n",
    "#Update step helper functions\n",
    "\n",
    "def calculateGammaMatrix(alphaMatrix, betaMatrix):\n",
    "    T = len(betaMatrix[0])\n",
    "    N = len(betaMatrix)\n",
    "    gamma = np.zeros((N,T))\n",
    "    for t in range(T):\n",
    "        gamma[:,t]=alphaMatrix[:,t]*betaMatrix[:,t]\n",
    "        gamma[:,t]=gamma[:,t]/np.sum(gamma[:,t])\n",
    "    return gamma\n",
    "\n",
    "def calculateXiMatrix( transMatrix, alphaMatrix, betaMatrix, bMatrix):\n",
    "    #caluculate xi \n",
    "    #probability of going from state n to state j at times t and t+1 respectively\n",
    "    T = len(bMatrix[0])\n",
    "    N = len(bMatrix)\n",
    "    xi = np.zeros((2*N,T))\n",
    "    for t in range(0,T):\n",
    "        for i in range(0,N):\n",
    "            for j in range(i,i+2):\n",
    "                if j>= N:\n",
    "                    xi[i+j,t] = 0\n",
    "                else:\n",
    "                    xi[i+j,t]=alphaMatrix[i,t]*transMatrix[i,j]\n",
    "                if (t<T-1):\n",
    "                    if j==N:\n",
    "                        xi[i+j,t]=0\n",
    "                    else:\n",
    "                        xi[i+j,t] = xi[i+j,t]*bMatrix[j,t+1]*betaMatrix[j,t+1]\n",
    "        xi[:,t]=xi[:,t]/np.sum(xi[:,t])\n",
    "    return xi \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the HMM Independent of speaker\n",
    "\n",
    "We train each HMM until the parameters reach convergence. To find this optimal number of iterations, we plot the maximum probability of the HMM and stop once it does change anymore. We found that 25 iterations is sufficient for the model parameters to reach convergence.\n",
    "\n",
    "The general method of how we train the HMM for the speaker independent and speaker dependent version is the same. The difference stems from how we make the test and train split above. In the speaker independent version, there are only 3 speakers, and the speaker dependent, we train on utterances from all 4 speakers. These differences will change the code structure slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#SDEP general\n",
    "def SINDEPTRAIN(initialTransition,initialProbabilities,initialmeans,initialcov,wordidx,numiterations,numcoeff):\n",
    "    A = np.array(initialTransition)\n",
    "    mn = initialmeans\n",
    "    cov = initialcov\n",
    "    logprobs = np.zeros((15,numiterations))\n",
    "    for iteration in range(numiterations):\n",
    "        alphas = []\n",
    "        betas = []\n",
    "        gammas = []\n",
    "        xis = []\n",
    "        for s in range(3):#for each speaker\n",
    "            for u in range(5): # for each utterance\n",
    "                B = calculateBMatrix(sindeptrain[s][wordidx][u],mn,cov,5)\n",
    "                Alpha,Beta,logg = calculateAlphaBetaTildeG(A,B,initialProbabilities)\n",
    "                Gamma = calculateGammaMatrix(Alpha,Beta)\n",
    "                xi = calculateXiMatrix(A,Alpha,Beta,B)\n",
    "                alphas.append(Alpha)\n",
    "                betas.append(Beta)\n",
    "                gammas.append(Gamma)\n",
    "                xis.append(xi)\n",
    "                logprobs[:,iteration] = np.sum(logg)\n",
    "\n",
    "        #upd8 \n",
    "        num_files = len(alphas)\n",
    "        N = len(gammas[0])\n",
    "        A_new = np.zeros((N,N))\n",
    "        mu_new = np.zeros((N,numcoeff))\n",
    "        sigma_new = np.zeros((N,numcoeff, numcoeff))\n",
    "        # find aij\n",
    "        denomsum = np.zeros((N))\n",
    "        for i in range(0,N):\n",
    "\n",
    "            for f in range(num_files):\n",
    "                for t in range(len(gammas[f][i])):\n",
    "                    denomsum[i] += gammas[f][i][t]\n",
    "\n",
    "            for j in range(i,i+2):\n",
    "                if j<N:\n",
    "                    numsum = 0\n",
    "                    for f in range(num_files):\n",
    "                        for t in range(len(gammas[f][i])):\n",
    "                            numsum += xis[f][i+j][t]\n",
    "                    A_new[i][j] = numsum/denomsum[i]\n",
    "        #find mui\n",
    "        for i in range(0,N):\n",
    "            numsum = 0\n",
    "            for f in range(num_files):\n",
    "                for t in range(len(sindeptrain[f//5][wordidx][f%5])):\n",
    "                    numsum += (np.array(gammas[f][i][t]) * np.array(sindeptrain[f//5][wordidx][f%5][t]))\n",
    "            mu_new[i] = numsum/denomsum[i]\n",
    "\n",
    "        #find covdi\n",
    "        for i in range(0,N):\n",
    "            numsum = 0\n",
    "            for f in range(num_files):\n",
    "                for t in range(len(sindeptrain[f//5][wordidx][f%5])):\n",
    "                    numsum += (np.array(gammas[f][i][t]) * np.array(np.outer((sindeptrain[f//5][wordidx][f%5][t]-mn[i]),(sindeptrain[f//5][wordidx][f%5][t]-mn[i]))))\n",
    "            sigma_new[i] = numsum/denomsum[i]\n",
    "        #save the new parameters\n",
    "        A = A_new\n",
    "        mn = mu_new\n",
    "        cov = sigma_new\n",
    "    print(\"indep hmm completed for word\",wordidx)\n",
    "    return A,mn,cov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SDEP general\n",
    "def SDEPTRAIN(initialTransition,initialProbabilities,initialmeans,initialcov,wordidx,numiterations,numcoeff):\n",
    "    A = np.array(initialTransition)\n",
    "    mn = initialmeans\n",
    "    cov = initialcov\n",
    "    logprobs = np.zeros((16,numiterations))\n",
    "    for iteration in range(numiterations):\n",
    "        # calc params\n",
    "        alphas = []\n",
    "        betas = []\n",
    "        gammas = []\n",
    "        xis = []\n",
    "        for s in range(4):#for each speaker\n",
    "            for u in range(4): # for each utterance\n",
    "                B = calculateBMatrix(sdeptrain[s][wordidx][u],mn,cov,5)\n",
    "                Alpha,Beta,logg = calculateAlphaBetaTildeG(A,B,initialProbabilities)\n",
    "                Gamma = calculateGammaMatrix(Alpha,Beta)\n",
    "                xi = calculateXiMatrix(A,Alpha,Beta,B)\n",
    "                alphas.append(Alpha)\n",
    "                betas.append(Beta)\n",
    "                gammas.append(Gamma)\n",
    "                xis.append(xi)\n",
    "                logprobs[:,iteration] = np.sum(logg)\n",
    "\n",
    "        #upd8 \n",
    "        num_files = len(alphas)\n",
    "        N = len(gammas[0])\n",
    "        A_new = np.zeros((N,N))\n",
    "        mu_new = np.zeros((N,numcoeff))\n",
    "        sigma_new = np.zeros((N,numcoeff, numcoeff))\n",
    "        # find aij\n",
    "        denomsum = np.zeros((N))\n",
    "        for i in range(0,N):\n",
    "\n",
    "            for f in range(num_files):\n",
    "                for t in range(len(gammas[f][i])):\n",
    "                    denomsum[i] += gammas[f][i][t]\n",
    "\n",
    "            for j in range(i,i+2):\n",
    "                if j<N:\n",
    "                    numsum = 0\n",
    "                    for f in range(num_files):\n",
    "                        for t in range(len(gammas[f][i])):\n",
    "                            numsum += xis[f][i+j][t]\n",
    "                    A_new[i][j] = numsum/denomsum[i]\n",
    "        #find mui\n",
    "        for i in range(0,N):\n",
    "\n",
    "            numsum = 0\n",
    "            for f in range(num_files):\n",
    "                for t in range(len(gammas[f][i])):\n",
    "                    numsum += (np.array(gammas[f][i][t]) * np.array(sdeptrain[f//4][wordidx][f%4][t]))\n",
    "            mu_new[i] = numsum/denomsum[i]\n",
    "\n",
    "        #find covdi\n",
    "        for i in range(0,N):\n",
    "\n",
    "            numsum = 0\n",
    "            for f in range(num_files):\n",
    "                for t in range(len(gammas[f][i])):\n",
    "                    numsum += (np.array(gammas[f][i][t]) * np.array(np.outer((sdeptrain[f//4][wordidx][f%4][t]-mn[i]),(sdeptrain[f//4][wordidx][f%4][t]-mn[i]))))\n",
    "            sigma_new[i] = numsum/denomsum[i]\n",
    "\n",
    "        A = A_new\n",
    "        mn = mu_new\n",
    "        cov = sigma_new\n",
    "    print(A_new)\n",
    "    print(\"dep hmm completed for word\",wordidx)\n",
    "    return A,mn,cov\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get the optimal parameters for the speaker dependent HMM of each word by training on that portion of the train dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING FOR SPEAKER DEPENDENT\n",
    "asrmean = [sdepasr_mn,sdepasr_mn,sdepasr_mn,sdepasr_mn,sdepasr_mn]\n",
    "asrcova = [sdepasr_cov,sdepasr_cov,sdepasr_cov,sdepasr_cov,sdepasr_cov]\n",
    "cnnmean = [sdepcnn_mn,sdepcnn_mn,sdepcnn_mn,sdepcnn_mn,sdepcnn_mn]\n",
    "cnncova = [sdepcnn_cov,sdepcnn_cov,sdepcnn_cov,sdepcnn_cov,sdepcnn_cov]\n",
    "dnnmean = [sdepdnn_mn,sdepdnn_mn,sdepdnn_mn,sdepdnn_mn,sdepdnn_mn]\n",
    "dnncova = [sdepdnn_cov,sdepdnn_cov,sdepdnn_cov,sdepdnn_cov,sdepdnn_cov]\n",
    "hmmmean = [sdephmm_mn,sdephmm_mn,sdephmm_mn,sdephmm_mn,sdephmm_mn]\n",
    "hmmcova = [sdephmm_cov,sdephmm_cov,sdephmm_cov,sdephmm_cov,sdephmm_cov]\n",
    "ttsmean = [sdeptts_mn,sdeptts_mn,sdeptts_mn,sdeptts_mn,sdeptts_mn]\n",
    "ttscova = [sdeptts_cov,sdeptts_cov,sdeptts_cov,sdeptts_cov,sdeptts_cov]\n",
    "A_asr,mn_asr,cov_asr = SDEPTRAIN(initialTransition,initialProbabilities,asrmean,asrcova,0,25,14)\n",
    "A_cnn,mn_cnn,cov_cnn = SDEPTRAIN(initialTransition,initialProbabilities,cnnmean,cnncova,1,25,14)\n",
    "A_dnn,mn_dnn,cov_dnn = SDEPTRAIN(initialTransition,initialProbabilities,dnnmean,dnncova,2,25,14)\n",
    "A_hmm,mn_hmm,cov_hmm = SDEPTRAIN(initialTransition,initialProbabilities,hmmmean,hmmcova,3,25,14)\n",
    "A_tts,mn_tts,cov_tts = SDEPTRAIN(initialTransition,initialProbabilities,ttsmean,ttscova,4,25,14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the HMM\n",
    "\n",
    "We test our test utterances by first making a forward pass and getting the $\\alpha$ and $\\beta$ values. Then, we can get the maximum probability from that HMM by summing the log of the $g$ vector. Then, we assign the utterance to be the HMM that gave the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testlogProb(testfile,Atest,meanstest,covtest,initProbs):\n",
    "    At = np.array(Atest)\n",
    "    Btest = calculateBMatrix(testfile,meanstest,covtest,5)\n",
    "    logg = np.array(calculateAlphaBetaTildeG(At,Btest,initProbs)[2])\n",
    "    return np.sum(logg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPEAKER DEPENDENT TEST\n",
    "#sdeptest[speaker][word]\n",
    "classifications = np.zeros((5,5))\n",
    "#classifications[truth][prediction] where idx{0,1,2,3,4} => {'asr','cnn','dnn','hmm','tts'}\n",
    "for i in range(4):\n",
    "    for j in range(5):\n",
    "        file = sdeptest[i][j]\n",
    "        preds = [testlogProb(file,A_asr,mn_asr,cov_asr,initialProbabilities),\n",
    "                 testlogProb(file,A_cnn,mn_cnn,cov_cnn,initialProbabilities),\n",
    "                 testlogProb(file,A_dnn,mn_dnn,cov_dnn,initialProbabilities),\n",
    "                 testlogProb(file,A_hmm,mn_hmm,cov_hmm,initialProbabilities),\n",
    "                 testlogProb(file,A_tts,mn_tts,cov_tts,initialProbabilities)]\n",
    "        classifications[j][np.argmax(preds)] += 1\n",
    "\n",
    "print('Confusion Matrix\\n',classifications/4)\n",
    "print('Accuracy\\n',np.mean(np.diag(classifications/4)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPEAKER INDEPENDENT TRAIN\n",
    "asrmeani = [sdepasri_mn,sdepasri_mn,sdepasri_mn,sdepasri_mn,sdepasri_mn]\n",
    "asrcovai = [sdepasri_cov,sdepasri_cov,sdepasri_cov,sdepasri_cov,sdepasri_cov]\n",
    "cnnmeani = [sdepcnni_mn,sdepcnni_mn,sdepcnni_mn,sdepcnni_mn,sdepcnni_mn]\n",
    "cnncovai = [sdepcnni_cov,sdepcnni_cov,sdepcnni_cov,sdepcnni_cov,sdepcnni_cov]\n",
    "dnnmeani = [sdepdnni_mn,sdepdnni_mn,sdepdnni_mn,sdepdnni_mn,sdepdnni_mn]\n",
    "dnncovai = [sdepdnni_cov,sdepdnni_cov,sdepdnni_cov,sdepdnni_cov,sdepdnni_cov]\n",
    "hmmmeani = [sdephmmi_mn,sdephmmi_mn,sdephmmi_mn,sdephmmi_mn,sdephmmi_mn]\n",
    "hmmcovai = [sdephmmi_cov,sdephmmi_cov,sdephmmi_cov,sdephmmi_cov,sdephmmi_cov]\n",
    "ttsmeani = [sdepttsi_mn,sdepttsi_mn,sdepttsi_mn,sdepttsi_mn,sdepttsi_mn]\n",
    "ttscovai = [sdepttsi_cov,sdepttsi_cov,sdepttsi_cov,sdepttsi_cov,sdepttsi_cov]\n",
    "A_asri,mn_asri,cov_asri = SINDEPTRAIN(initialTransition,initialProbabilities,asrmeani,asrcovai,0,25,14)\n",
    "A_cnni,mn_cnni,cov_cnni = SINDEPTRAIN(initialTransition,initialProbabilities,cnnmeani,cnncovai,1,25,14)\n",
    "A_dnni,mn_dnni,cov_dnni = SINDEPTRAIN(initialTransition,initialProbabilities,dnnmeani,dnncovai,2,25,14)\n",
    "A_hmmi,mn_hmmi,cov_hmmi = SINDEPTRAIN(initialTransition,initialProbabilities,hmmmeani,hmmcovai,3,25,14)\n",
    "A_ttsi,mn_ttsi,cov_ttsi = SINDEPTRAIN(initialTransition,initialProbabilities,ttsmeani,ttscovai,4,25,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPEAKER INDEPENDENT TEST\n",
    "#test[word][utt]\n",
    "classificationsi = np.zeros((5,5))\n",
    "#classifications[truth][prediction] where idx{0,1,2,3,4} => {'asr','cnn','dnn','hmm','tts'}\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        filei = sindeptest[i][j]\n",
    "        predsi = [testlogProb(filei,A_asri,mn_asri,cov_asri,initialProbabilities),\n",
    "                 testlogProb(filei,A_cnni,mn_cnni,cov_cnni,initialProbabilities),\n",
    "                 testlogProb(filei,A_dnni,mn_dnni,cov_dnni,initialProbabilities),\n",
    "                 testlogProb(filei,A_hmmi,mn_hmmi,cov_hmmi,initialProbabilities),\n",
    "                 testlogProb(filei,A_ttsi,mn_ttsi,cov_ttsi,initialProbabilities)]\n",
    "        classificationsi[i][np.argmax(predsi)] += 1\n",
    "\n",
    "print('Classification\\n',classificationsi/5)\n",
    "print('Accuracy\\n',np.mean(np.diag(classificationsi/5)))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Comparing the speaker independent and dependent results, we found that the speaker dependent accuracy did better. This makes sense, because with this way, the HMM was able to train on utterances from all the speakers. Each speaker has their own personal sound and way they pronounce certain words. Some people have accents as well, and that can impact how a certain word sounds when they say it. Therefore, it is a good idea to train each HMM on each speaker as well so it can learn . That way, the model parameters that are produced can represent a variety voices. Speaker dependent training trains the HMM in a more robust fashion.\n",
    "\n",
    "Upon further inspection, it makes sense why the speaker independent HMM performed worse than the speaker dependent HMMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on Recorded Voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#access voicetest[word][utt]\n",
    "voicetest = [[[] for i in range(5)] for j in range(5)]\n",
    "p = './feature/self_recf'\n",
    "wordList = ['asr','cnn','dnn','hmm','tts']\n",
    "\n",
    "speakertestpath = p\n",
    "wnum = 0\n",
    "for w in wordList:\n",
    "    wordtestpath = speakertestpath +'/'+w\n",
    "    for utt in range(1,6):\n",
    "        ftestpath = wordtestpath + str(utt)+'.fea'\n",
    "        fitest = open(ftestpath,'r')\n",
    "        dataitest = fitest.readlines()\n",
    "        dataitest = [i.split(',') for i in dataitest]\n",
    "        for sample in range(len(dataitest)):\n",
    "            dataitest[sample] = [float(i) for i in dataitest[sample]]\n",
    "            dataitest[sample] = dataitest[sample]/np.linalg.norm(dataitest[sample])\n",
    "#             print(dataitest)\n",
    "\n",
    "        for d in dataitest:\n",
    "            voicetest[wnum][utt-1].append(d)\n",
    "    wnum += 1\n",
    "\n",
    "voicetest = np.array(voicetest)\n",
    "#SPEAKER INDEPENDENT TEST\n",
    "#test[word][utt]\n",
    "classificationsvoice = np.zeros((5,5))\n",
    "#classifications[truth][prediction] where idx{0,1,2,3,4} => {'asr','cnn','dnn','hmm','tts'}\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        filei = voicetest[i][j]\n",
    "        predsi = [testlogProb(filei,A_asri,mn_asri,cov_asri,initialProbabilities),\n",
    "                 testlogProb(filei,A_cnni,mn_cnni,cov_cnni,initialProbabilities),\n",
    "                 testlogProb(filei,A_dnni,mn_dnni,cov_dnni,initialProbabilities),\n",
    "                 testlogProb(filei,A_hmmi,mn_hmmi,cov_hmmi,initialProbabilities),\n",
    "                 testlogProb(filei,A_ttsi,mn_ttsi,cov_ttsi,initialProbabilities)]\n",
    "        classificationsvoice[i][np.argmax(predsi)] += 1\n",
    "#         print(predsi)\n",
    "\n",
    "print('Confusion Matrix\\n',classificationsvoice/5)\n",
    "print('Accuracy\\n',np.mean(np.diag(classificationsi/5)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction for EC\n",
    "For the extra credit we have opted to use the Mel Frequency Spectral Coefficients. Since the MFCC comes from taking the IDFT of the of the MFSC, we obtained the MFSC by taking the fft of the MFCC features given to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#speaker dependent partitions\n",
    "#sdeptrain dimensions sdeptrain[speaker][word][utterance idx] with idxs given alphabetically\n",
    "ec_sdeptrain = [[[[] for i in range(4)] for j in range(5)] for k in range(4)]\n",
    "#ec_sdeptest dimensions sdeptrain[speaker][word] with idxs given alphabetically\n",
    "ec_sdeptest = [[[] for i in range(5)] for j in range(4)]\n",
    "p = './feature'\n",
    "speakerListec = ['dg', 'ls', 'mh', 'yx']\n",
    "wordListec = ['asr','cnn','dnn','hmm','tts']\n",
    "snum = 0\n",
    "for s in speakerListec:\n",
    "    speakerpathec = p+'/'+s\n",
    "    wnum = 0\n",
    "    for w in wordListec:\n",
    "        wordpathec = speakerpathec +'/'+s+'_'+w\n",
    "        #set up train\n",
    "        for utt in range(1,5):\n",
    "            fpathec = wordpathec + str(utt)+'.fea'\n",
    "            fec = open(fpathec,'r')\n",
    "            data = fec.readlines()\n",
    "            data = [i.split(',') for i in data]\n",
    "            for sample in range(len(data)):\n",
    "                data[sample] = [float(i) for i in data[sample]]\n",
    "                #normalize\n",
    "#                 data[sample] = data[sample]/np.linalg.norm(data[sample])\n",
    "                \n",
    "            for d in data:\n",
    "                ec_sdeptrain[snum][wnum][utt-1].append(np.real(np.fft.fft(d)))\n",
    "        #set up test\n",
    "        fpath2ec = wordpathec + str(5)+'.fea'\n",
    "        f2ec = open(fpath2ec,'r')\n",
    "        dataec = f2ec.readlines()\n",
    "        dataec = [i.split(',') for i in dataec]\n",
    "        for d in dataec:\n",
    "            ec_sdeptest[snum][wnum].append(np.real(np.fft.fft(np.array(d).astype(np.float))))\n",
    "        wnum += 1\n",
    "    snum += 1\n",
    "ec_sdeptrain = np.array(ec_sdeptrain)\n",
    "\n",
    "\n",
    "ec_bigasr = []\n",
    "ec_bigcnn = []\n",
    "ec_bigdnn = []\n",
    "ec_bighmm = []\n",
    "ec_bigtts = []\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        ec_bigasr.extend(ec_sdeptrain[i][0][j])\n",
    "        ec_bigcnn.extend(ec_sdeptrain[i][1][j])\n",
    "        ec_bigdnn.extend(ec_sdeptrain[i][2][j])\n",
    "        ec_bighmm.extend(ec_sdeptrain[i][3][j])\n",
    "        ec_bigtts.extend(ec_sdeptrain[i][4][j])\n",
    "# ec_sdepasr_cov = (np.cov(np.array(ec_bigasr).T)+0.3*np.identity(14))\n",
    "# ec_sdepcnn_cov = (np.cov(np.array(ec_bigcnn).T)+0.3*np.identity(14))\n",
    "# ec_sdepdnn_cov = (np.cov(np.array(ec_bigdnn).T)+0.3*np.identity(14))\n",
    "# ec_sdephmm_cov = (np.cov(np.array(ec_bighmm).T)+0.3*np.identity(14))\n",
    "# ec_sdeptts_cov = (np.cov(np.array(ec_bigtts).T)+0.3*np.identity(14))\n",
    "\n",
    "ec_sdepasr_cov = np.diag(np.diag(np.cov(np.array(ec_bigasr).T)))\n",
    "ec_sdepcnn_cov = np.diag(np.diag(np.cov(np.array(ec_bigcnn).T)))\n",
    "ec_sdepdnn_cov = np.diag(np.diag(np.cov(np.array(ec_bigdnn).T)))\n",
    "ec_sdephmm_cov = np.diag(np.diag(np.cov(np.array(ec_bighmm).T)))\n",
    "ec_sdeptts_cov = np.diag(np.diag(np.cov(np.array(ec_bigtts).T)))\n",
    "\n",
    "\n",
    "ec_sdepasr_mn = np.mean(ec_bigasr,axis=0)\n",
    "ec_sdepcnn_mn = np.mean(ec_bigcnn,axis=0)\n",
    "ec_sdepdnn_mn = np.mean(ec_bigdnn,axis=0)\n",
    "ec_sdephmm_mn = np.mean(ec_bighmm,axis=0)\n",
    "ec_sdeptts_mn = np.mean(ec_bigtts,axis=0)\n",
    "# print(ec_sdepasr_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING FOR SPEAKER DEPENDENT FOR EXTRA CREDIT\n",
    "ec_asrmean = [ec_sdepasr_mn,ec_sdepasr_mn,ec_sdepasr_mn,ec_sdepasr_mn,ec_sdepasr_mn]\n",
    "ec_asrcova = [ec_sdepasr_cov,ec_sdepasr_cov,ec_sdepasr_cov,ec_sdepasr_cov,ec_sdepasr_cov]\n",
    "ec_cnnmean = [ec_sdepcnn_mn,ec_sdepcnn_mn,ec_sdepcnn_mn,ec_sdepcnn_mn,ec_sdepcnn_mn]\n",
    "ec_cnncova = [ec_sdepcnn_cov,ec_sdepcnn_cov,ec_sdepcnn_cov,ec_sdepcnn_cov,ec_sdepcnn_cov]\n",
    "ec_dnnmean = [ec_sdepdnn_mn,ec_sdepdnn_mn,ec_sdepdnn_mn,ec_sdepdnn_mn,ec_sdepdnn_mn]\n",
    "ec_dnncova = [ec_sdepdnn_cov,ec_sdepdnn_cov,ec_sdepdnn_cov,ec_sdepdnn_cov,ec_sdepdnn_cov]\n",
    "ec_hmmmean = [ec_sdephmm_mn,ec_sdephmm_mn,ec_sdephmm_mn,ec_sdephmm_mn,ec_sdephmm_mn]\n",
    "ec_hmmcova = [ec_sdephmm_cov,ec_sdephmm_cov,ec_sdephmm_cov,ec_sdephmm_cov,ec_sdephmm_cov]\n",
    "ec_ttsmean = [ec_sdeptts_mn,ec_sdeptts_mn,ec_sdeptts_mn,ec_sdeptts_mn,ec_sdeptts_mn]\n",
    "ec_ttscova = [ec_sdeptts_cov,ec_sdeptts_cov,ec_sdeptts_cov,ec_sdeptts_cov,ec_sdeptts_cov]\n",
    "# print(ec_asrcova)\n",
    "ec_A_asr,ec_mn_asr,ec_cov_asr = SDEPTRAIN(initialTransition,initialProbabilities,ec_asrmean,ec_asrcova,0,25,14)\n",
    "print('did 1')\n",
    "ec_A_cnn,ec_mn_cnn,ec_cov_cnn = SDEPTRAIN(initialTransition,initialProbabilities,ec_cnnmean,ec_cnncova,1,25,14)\n",
    "print('did 1')\n",
    "ec_A_dnn,ec_mn_dnn,ec_cov_dnn = SDEPTRAIN(initialTransition,initialProbabilities,ec_dnnmean,ec_dnncova,2,25,14)\n",
    "print('did 1')\n",
    "ec_A_hmm,ec_mn_hmm,ec_cov_hmm = SDEPTRAIN(initialTransition,initialProbabilities,ec_hmmmean,ec_hmmcova,3,25,14)\n",
    "print('did 1')\n",
    "ec_A_tts,ec_mn_tts,ec_cov_tts = SDEPTRAIN(initialTransition,initialProbabilities,ec_ttsmean,ec_ttscova,4,25,14)\n",
    "print('did 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPEAKER DEPENDENT TEST FOR EXTRA CREDIT\n",
    "ec_classifications = np.zeros((5,5))\n",
    "#classifications[truth][prediction] where idx{0,1,2,3,4} => {'asr','cnn','dnn','hmm','tts'}\n",
    "for i in range(4):\n",
    "    for j in range(5):\n",
    "        ec_file = ec_sdeptrain[0][i][j]\n",
    "        ec_preds = [testlogProb(ec_file,ec_A_asr,ec_mn_asr,ec_cov_asr,initialProbabilities),\n",
    "                 testlogProb(ec_file,ec_A_cnn,ec_mn_cnn,ec_cov_cnn,initialProbabilities),\n",
    "                 testlogProb(ec_file,ec_A_dnn,ec_mn_dnn,ec_cov_dnn,initialProbabilities),\n",
    "                 testlogProb(ec_file,ec_A_hmm,ec_mn_hmm,ec_cov_hmm,initialProbabilities),\n",
    "                 testlogProb(ec_file,ec_A_tts,ec_mn_tts,ec_cov_tts,initialProbabilities)]\n",
    "        print(ec_preds)\n",
    "        ec_classifications[j][np.argmax(ec_preds)] += 1\n",
    "\n",
    "print('Confusion Matrix\\n',ec_classifications)\n",
    "print('Accuracy\\n',np.mean(np.diag(ec_classifications/5)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
